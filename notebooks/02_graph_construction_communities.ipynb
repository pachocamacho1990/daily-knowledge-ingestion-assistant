{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Step 2: Graph Construction & Community Detection\n",
    "\n",
    "This notebook continues the GraphRAG pipeline:\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Load extraction results** - From notebook 01\n",
    "2. **Build NetworkX graph** - Entities as nodes, relationships as edges\n",
    "3. **Compute graph metrics** - PageRank, centrality, degree\n",
    "4. **Community detection** - Louvain algorithm for topic clustering\n",
    "5. **Generate community summaries** - LLM-powered hierarchical summaries\n",
    "6. **Store in SQLite** - Persist graph structure and summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import httpx\n",
    "import networkx as nx\n",
    "import community as community_louvain  # python-louvain\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "MODEL = \"qwen2.5:3b\"\n",
    "DB_PATH = Path(\"graphrag.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ollama(prompt: str, system: str = \"\", temperature: float = 0.0) -> str:\n",
    "    \"\"\"Send a chat request to Ollama and return the response.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = httpx.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/chat\",\n",
    "        json={\n",
    "            \"model\": MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        },\n",
    "        timeout=120.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 14 entities, 7 relationships, 5 claims\n"
     ]
    }
   ],
   "source": [
    "# Load results from notebook 01\n",
    "with open(\"extraction_results.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "entities = data[\"entities\"]\n",
    "relationships = data[\"relationships\"]\n",
    "claims = data[\"claims\"]\n",
    "chunks = data[\"chunks\"]\n",
    "\n",
    "print(f\"Loaded: {len(entities)} entities, {len(relationships)} relationships, {len(claims)} claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENTITIES ===\n",
      "  [ORGANIZATION] OPENAI\n",
      "  [ORGANIZATION] MICROSOFT\n",
      "  [PERSON] SAM ALTMAN\n",
      "  [PRODUCT] GPT-5\n",
      "  [PERSON] SATYA NADELLA\n",
      "  ... and 9 more\n"
     ]
    }
   ],
   "source": [
    "# Preview entities\n",
    "print(\"=== ENTITIES ===\")\n",
    "for e in entities[:5]:\n",
    "    print(f\"  [{e['type']}] {e['name']}\")\n",
    "if len(entities) > 5:\n",
    "    print(f\"  ... and {len(entities) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build NetworkX Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created: 14 nodes, 6 edges\n"
     ]
    }
   ],
   "source": [
    "# Create directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add entity nodes with attributes\n",
    "for entity in entities:\n",
    "    G.add_node(\n",
    "        entity[\"name\"],\n",
    "        type=entity[\"type\"],\n",
    "        description=entity[\"description\"]\n",
    "    )\n",
    "\n",
    "# Add relationship edges with attributes\n",
    "for rel in relationships:\n",
    "    # Only add edge if both nodes exist\n",
    "    if rel[\"source\"] in G.nodes and rel[\"target\"] in G.nodes:\n",
    "        G.add_edge(\n",
    "            rel[\"source\"],\n",
    "            rel[\"target\"],\n",
    "            description=rel[\"description\"],\n",
    "            weight=rel[\"strength\"]\n",
    "        )\n",
    "\n",
    "print(f\"Graph created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GRAPH NODES ===\n",
      "  OPENAI (ORGANIZATION)\n",
      "  MICROSOFT (ORGANIZATION)\n",
      "  SAM ALTMAN (PERSON)\n",
      "  GPT-5 (PRODUCT)\n",
      "  SATYA NADELLA (PERSON)\n",
      "  GOLDMAN SACHS (ORGANIZATION)\n",
      "  GOOGLE (ORGANIZATION)\n",
      "  SUNDAR PICHAI (PERSON)\n",
      "  FEDERAL TRADE COMMISSION (ORGANIZATION)\n",
      "  FTC (ORGANIZATION)\n",
      "\n",
      "=== GRAPH EDGES ===\n",
      "  OPENAI --> GPT-5\n",
      "    OpenAI develops GPT-5...\n",
      "  MICROSOFT --> GPT-5\n",
      "    Microsoft integrates GPT-5 into its product suite...\n",
      "  MICROSOFT --> GOLDMAN SACHS\n",
      "    Goldman Sachs raised their price target for Microsoft stock...\n",
      "  MICROSOFT --> GOOGLE\n",
      "    Microsoft's primary competitor in cloud services is Google...\n",
      "  GOOGLE --> SUNDAR PICHAI\n",
      "    Sundar Pichai is the CEO of Google...\n",
      "  FEDERAL TRADE COMMISSION --> LINA KAHN\n",
      "    FEDERAL TRADE COMMISSION appoints Lina Kahn as Chair...\n"
     ]
    }
   ],
   "source": [
    "# Display graph structure\n",
    "print(\"\\n=== GRAPH NODES ===\")\n",
    "for node, attrs in list(G.nodes(data=True))[:10]:\n",
    "    print(f\"  {node} ({attrs.get('type', 'N/A')})\")\n",
    "\n",
    "print(\"\\n=== GRAPH EDGES ===\")\n",
    "for source, target, attrs in list(G.edges(data=True))[:10]:\n",
    "    print(f\"  {source} --> {target}\")\n",
    "    print(f\"    {attrs.get('description', 'N/A')[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute Graph Metrics\n",
    "\n",
    "Calculate centrality scores for ranking node importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph metrics computed: pagerank, degree_centrality, betweenness\n"
     ]
    }
   ],
   "source": [
    "# Convert to undirected for some algorithms\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "# PageRank - importance based on incoming connections\n",
    "pagerank = nx.pagerank(G, weight=\"weight\")\n",
    "\n",
    "# Degree centrality - number of connections\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Betweenness centrality - bridges between clusters\n",
    "betweenness = nx.betweenness_centrality(G_undirected)\n",
    "\n",
    "# Store metrics on nodes\n",
    "for node in G.nodes:\n",
    "    G.nodes[node][\"pagerank\"] = pagerank.get(node, 0)\n",
    "    G.nodes[node][\"degree_centrality\"] = degree_centrality.get(node, 0)\n",
    "    G.nodes[node][\"betweenness\"] = betweenness.get(node, 0)\n",
    "\n",
    "print(\"Graph metrics computed: pagerank, degree_centrality, betweenness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP ENTITIES BY PAGERANK ===\n",
      "  0.1215 | [PRODUCT] GPT-5\n",
      "  0.1190 | [PERSON] SUNDAR PICHAI\n",
      "  0.1048 | [PERSON] LINA KAHN\n",
      "  0.0733 | [ORGANIZATION] GOOGLE\n",
      "  0.0715 | [ORGANIZATION] GOLDMAN SACHS\n",
      "  0.0567 | [ORGANIZATION] OPENAI\n",
      "  0.0567 | [ORGANIZATION] MICROSOFT\n",
      "  0.0567 | [PERSON] SAM ALTMAN\n",
      "  0.0567 | [PERSON] SATYA NADELLA\n",
      "  0.0567 | [ORGANIZATION] FEDERAL TRADE COMMISSION\n"
     ]
    }
   ],
   "source": [
    "# Top entities by PageRank\n",
    "print(\"\\n=== TOP ENTITIES BY PAGERANK ===\")\n",
    "sorted_by_pagerank = sorted(pagerank.items(), key=lambda x: -x[1])\n",
    "for node, score in sorted_by_pagerank[:10]:\n",
    "    node_type = G.nodes[node].get(\"type\", \"N/A\")\n",
    "    print(f\"  {score:.4f} | [{node_type}] {node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Community Detection\n",
    "\n",
    "Using Louvain algorithm to find clusters of related entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 9 communities\n",
      "Modularity score: 0.4037\n"
     ]
    }
   ],
   "source": [
    "# Louvain community detection (works on undirected graphs)\n",
    "partition = community_louvain.best_partition(G_undirected, weight=\"weight\", resolution=1.0)\n",
    "\n",
    "# Store community assignment on nodes\n",
    "for node, community_id in partition.items():\n",
    "    G.nodes[node][\"community\"] = community_id\n",
    "\n",
    "# Count communities\n",
    "num_communities = max(partition.values()) + 1\n",
    "print(f\"Detected {num_communities} communities\")\n",
    "\n",
    "# Modularity score (quality of partition)\n",
    "modularity = community_louvain.modularity(partition, G_undirected, weight=\"weight\")\n",
    "print(f\"Modularity score: {modularity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMMUNITIES ===\n",
      "\n",
      "Community 0 (1 members):\n",
      "  [ORGANIZATION] STARTUPXYZ\n",
      "\n",
      "Community 1 (4 members):\n",
      "  [PRODUCT] GPT-5\n",
      "  [ORGANIZATION] GOLDMAN SACHS\n",
      "  [ORGANIZATION] OPENAI\n",
      "  [ORGANIZATION] MICROSOFT\n",
      "\n",
      "Community 2 (1 members):\n",
      "  [PERSON] SAM ALTMAN\n",
      "\n",
      "Community 3 (1 members):\n",
      "  [PERSON] SATYA NADELLA\n",
      "\n",
      "Community 4 (2 members):\n",
      "  [PERSON] SUNDAR PICHAI\n",
      "  [ORGANIZATION] GOOGLE\n",
      "\n",
      "Community 5 (2 members):\n",
      "  [PERSON] LINA KAHN\n",
      "  [ORGANIZATION] FEDERAL TRADE COMMISSION\n",
      "\n",
      "Community 6 (1 members):\n",
      "  [ORGANIZATION] FTC\n",
      "\n",
      "Community 7 (1 members):\n",
      "  [PERSON] TREASURY SECRETARY\n",
      "\n",
      "Community 8 (1 members):\n",
      "  [ORGANIZATION] EXAMPLE CORP\n"
     ]
    }
   ],
   "source": [
    "# Group entities by community\n",
    "communities: dict[int, list[str]] = {}\n",
    "for node, community_id in partition.items():\n",
    "    if community_id not in communities:\n",
    "        communities[community_id] = []\n",
    "    communities[community_id].append(node)\n",
    "\n",
    "print(\"\\n=== COMMUNITIES ===\")\n",
    "for comm_id, members in sorted(communities.items()):\n",
    "    # Sort members by PageRank within community\n",
    "    sorted_members = sorted(members, key=lambda x: -pagerank.get(x, 0))\n",
    "    print(f\"\\nCommunity {comm_id} ({len(members)} members):\")\n",
    "    for member in sorted_members[:5]:\n",
    "        node_type = G.nodes[member].get(\"type\", \"N/A\")\n",
    "        print(f\"  [{node_type}] {member}\")\n",
    "    if len(members) > 5:\n",
    "        print(f\"  ... and {len(members) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Community Summaries\n",
    "\n",
    "Create LLM-powered summaries for each community following GraphRAG's report format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CommunitySummary:\n",
    "    community_id: int\n",
    "    title: str\n",
    "    summary: str\n",
    "    key_entities: list[str]\n",
    "    key_insights: list[str]\n",
    "\n",
    "COMMUNITY_SUMMARY_PROMPT = \"\"\"\n",
    "You are an expert analyst creating a summary report for a knowledge graph community.\n",
    "\n",
    "Given the following entities and their relationships, create a structured summary.\n",
    "\n",
    "ENTITIES IN THIS COMMUNITY:\n",
    "{entities_info}\n",
    "\n",
    "RELATIONSHIPS:\n",
    "{relationships_info}\n",
    "\n",
    "RELEVANT CLAIMS:\n",
    "{claims_info}\n",
    "\n",
    "Create a JSON response with:\n",
    "1. title: A short descriptive title for this community (5-10 words)\n",
    "2. summary: A 2-3 sentence executive summary of what this community represents\n",
    "3. key_insights: 3-5 bullet points of key facts or relationships\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"title\": \"...\",\n",
    "  \"summary\": \"...\",\n",
    "  \"key_insights\": [\"...\", \"...\", \"...\"]\n",
    "}}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "def generate_community_summary(community_id: int, members: list[str], G: nx.DiGraph, claims: list[dict]) -> CommunitySummary:\n",
    "    \"\"\"Generate a summary for a community using the LLM.\"\"\"\n",
    "    \n",
    "    # Gather entity info\n",
    "    entities_info = []\n",
    "    for member in members:\n",
    "        node_data = G.nodes[member]\n",
    "        entities_info.append(f\"- {member} ({node_data.get('type', 'N/A')}): {node_data.get('description', 'N/A')}\")\n",
    "    \n",
    "    # Gather relationships within community\n",
    "    relationships_info = []\n",
    "    for source, target, data in G.edges(data=True):\n",
    "        if source in members and target in members:\n",
    "            relationships_info.append(f\"- {source} -> {target}: {data.get('description', 'N/A')}\")\n",
    "    \n",
    "    # Gather relevant claims\n",
    "    claims_info = []\n",
    "    for claim in claims:\n",
    "        if claim[\"subject\"] in members:\n",
    "            claims_info.append(f\"- [{claim['claim_type']}] {claim['subject']}: {claim['description']}\")\n",
    "    \n",
    "    prompt = COMMUNITY_SUMMARY_PROMPT.format(\n",
    "        entities_info=\"\\n\".join(entities_info) or \"No entities\",\n",
    "        relationships_info=\"\\n\".join(relationships_info) or \"No relationships\",\n",
    "        claims_info=\"\\n\".join(claims_info[:10]) or \"No claims\"  # Limit claims\n",
    "    )\n",
    "    \n",
    "    response = chat_ollama(prompt)\n",
    "    \n",
    "    # Parse JSON\n",
    "    json_str = response.strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str.split(\"```\")[1]\n",
    "        if json_str.startswith(\"json\"):\n",
    "            json_str = json_str[4:]\n",
    "    json_str = json_str.strip()\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        return CommunitySummary(\n",
    "            community_id=community_id,\n",
    "            title=data.get(\"title\", f\"Community {community_id}\"),\n",
    "            summary=data.get(\"summary\", \"\"),\n",
    "            key_entities=members[:5],  # Top 5 by PageRank\n",
    "            key_insights=data.get(\"key_insights\", [])\n",
    "        )\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"Failed to parse JSON for community {community_id}: {ex}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        return CommunitySummary(\n",
    "            community_id=community_id,\n",
    "            title=f\"Community {community_id}\",\n",
    "            summary=\"Summary generation failed\",\n",
    "            key_entities=members[:5],\n",
    "            key_insights=[]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary for Community 0 (1 members)...\n",
      "  Title: StartupXYZ Community\n",
      "Generating summary for Community 1 (4 members)...\n",
      "  Title: AI and Tech Community Insights\n",
      "Generating summary for Community 2 (1 members)...\n",
      "  Title: OpenAI Leadership Community\n",
      "Generating summary for Community 3 (1 members)...\n",
      "  Title: Microsoft Leadership Community\n",
      "Generating summary for Community 4 (2 members)...\n",
      "  Title: Google Cloud Services Community\n",
      "Generating summary for Community 5 (2 members)...\n",
      "  Title: FTC and AI Regulation Community\n",
      "Generating summary for Community 6 (1 members)...\n",
      "  Title: Microsoft-OpenAI Regulatory Community\n",
      "Generating summary for Community 7 (1 members)...\n",
      "  Title: Treasury Secretary Community\n",
      "Generating summary for Community 8 (1 members)...\n",
      "  Title: Technology Company Community\n",
      "\n",
      "Generated 9 community summaries\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries for each community\n",
    "community_summaries: list[CommunitySummary] = []\n",
    "\n",
    "for comm_id, members in sorted(communities.items()):\n",
    "    print(f\"Generating summary for Community {comm_id} ({len(members)} members)...\")\n",
    "    # Sort members by PageRank\n",
    "    sorted_members = sorted(members, key=lambda x: -pagerank.get(x, 0))\n",
    "    summary = generate_community_summary(comm_id, sorted_members, G, claims)\n",
    "    community_summaries.append(summary)\n",
    "    print(f\"  Title: {summary.title}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(community_summaries)} community summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMMUNITY SUMMARIES\n",
      "============================================================\n",
      "\n",
      "### Community 0: StartupXYZ Community\n",
      "\n",
      "The StartupXYZ community is a platform dedicated to startups, providing support and resources for entrepreneurs. It offers networking opportunities, educational content, and access to funding and mentorship.\n",
      "\n",
      "Key Entities: STARTUPXYZ\n",
      "\n",
      "Key Insights:\n",
      "  - No direct relationships or claims are provided for this community.\n",
      "  - Focuses on supporting startup ventures through various services.\n",
      "  - Aims to foster a supportive environment for entrepreneurial growth.\n",
      "  - Offers resources that can help startups succeed in their journey.\n",
      "  - Potential for collaboration and partnership opportunities within the community.\n",
      "\n",
      "### Community 1: AI and Tech Community Insights\n",
      "\n",
      "This community focuses on the integration of AI technologies like GPT-5 by Microsoft, its impact on stock prices, and insights from financial analysts such as Goldman Sachs.\n",
      "\n",
      "Key Entities: GPT-5, GOLDMAN SACHS, OPENAI, MICROSOFT\n",
      "\n",
      "Key Insights:\n",
      "  - GPT-5 is developed by OpenAI and integrated into Microsoft's product suite.\n",
      "  - Microsoft shares increased after a price target upgrade by Goldman Sachs.\n",
      "  - OpenAI and Microsoft have a collaborative relationship in the AI space.\n",
      "  - Financial markets react positively to advancements in AI technologies like GPT-5.\n",
      "  - The community highlights the intersection of technology, finance, and AI.\n",
      "\n",
      "### Community 2: OpenAI Leadership Community\n",
      "\n",
      "This community focuses on the leadership and strategic direction of OpenAI, a prominent AI research organization. It highlights the role of CEO Sam Altman in guiding the company's vision and initiatives.\n",
      "\n",
      "Key Entities: SAM ALTMAN\n",
      "\n",
      "Key Insights:\n",
      "  - The community centers around OpenAI’s leadership team, with a primary focus on CEO Sam Altman.\n",
      "  - It represents discussions and insights into OpenAI’s strategic direction and AI research advancements.\n",
      "  - Key to understanding the community is its role in shaping OpenAI's future plans and partnerships.\n",
      "\n",
      "### Community 3: Microsoft Leadership Community\n",
      "\n",
      "This community focuses on the leadership and strategic direction of Microsoft, with a primary focus on CEO Satya Naidu.\n",
      "\n",
      "Key Entities: SATYA NADELLA\n",
      "\n",
      "Key Insights:\n",
      "  - The community centers around Satya Naidu's role as the CEO of Microsoft.\n",
      "  - It represents discussions and insights into Microsoft’s business strategies and leadership decisions.\n",
      "  - No specific relationships or additional entities are mentioned in this community.\n",
      "\n",
      "### Community 4: Google Cloud Services Community\n",
      "\n",
      "This community focuses on Google's cloud services and leadership, with a particular emphasis on the company's recent announcements regarding its Gemini Ultra model.\n",
      "\n",
      "Key Entities: SUNDAR PICHAI, GOOGLE\n",
      "\n",
      "Key Insights:\n",
      "  - Sundar Pichai is the CEO of Google, leading the organization in various initiatives including cloud services.\n",
      "  - Google has announced an accelerated deployment for its Gemini Ultra model, indicating advancements in their AI and machine learning capabilities.\n",
      "  - The community highlights the competitive landscape with Google as a major player alongside other cloud service providers.\n",
      "\n",
      "### Community 5: FTC and AI Regulation Community\n",
      "\n",
      "This community focuses on the Federal Trade Commission's oversight of AI capabilities among tech giants, with Lina Kahn as its chairperson.\n",
      "\n",
      "Key Entities: LINA KAHN, FEDERAL TRADE COMMISSION\n",
      "\n",
      "Key Insights:\n",
      "  - Lina Kahn is currently serving as the Chair of the Federal Trade Commission (FTC).\n",
      "  - The FTC investigates and monitors Microsoft-OpenAI relationships and AI capabilities in collaboration with other regulatory bodies.\n",
      "  - Regulators like Lina Kahn emphasize the importance of closely monitoring the concentration of AI power among a few tech giants.\n",
      "  - Lina Kahn has stated that regulators are 'closely monitoring' the concentration of AI capabilities among a small number of tech giants.\n",
      "  - The FTC plays a crucial role in ensuring fair competition and consumer protection in the rapidly evolving field of artificial intelligence.\n",
      "\n",
      "### Community 6: Microsoft-OpenAI Regulatory Community\n",
      "\n",
      "This community focuses on the regulatory body, FTC, investigating the relationship between Microsoft and OpenAI. It serves as a platform for discussions and updates regarding this ongoing investigation.\n",
      "\n",
      "Key Entities: FTC\n",
      "\n",
      "Key Insights:\n",
      "  - The community is centered around the FTC's role in regulating the interaction between Microsoft and OpenAI.\n",
      "  - It provides a space for stakeholders to share information and insights about the regulatory process.\n",
      "  - Members of the community include representatives from both Microsoft, OpenAI, and the FTC.\n",
      "\n",
      "### Community 7: Treasury Secretary Community\n",
      "\n",
      "This community focuses on the role and responsibilities of the Treasury Secretary, providing a platform for discussions, insights, and updates related to this important government position.\n",
      "\n",
      "Key Entities: TREASURY SECRETARY\n",
      "\n",
      "Key Insights:\n",
      "  - The Treasury Secretary plays a crucial role in managing the country's finances and economic policies.\n",
      "  - Members engage in discussions about fiscal policy, tax reform, and international financial matters.\n",
      "  - The community serves as a hub for information sharing among experts and stakeholders interested in treasury affairs.\n",
      "\n",
      "### Community 8: Technology Company Community\n",
      "\n",
      "This community focuses on Technology companies, providing a platform for discussions and information sharing among members who are interested in the technological advancements and strategies of these organizations.\n",
      "\n",
      "Key Entities: EXAMPLE CORP\n",
      "\n",
      "Key Insights:\n",
      "  - The community centers around technology companies as its primary entity.\n",
      "  - Members engage in discussions about technological innovations and business strategies within tech firms.\n",
      "  - It serves as a hub for knowledge exchange and networking opportunities related to the tech industry.\n",
      "  - No specific relationships or claims are provided, indicating a broad focus on general information sharing.\n",
      "  - The community aims to foster understanding and collaboration among members interested in technology companies.\n"
     ]
    }
   ],
   "source": [
    "# Display community summaries\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMMUNITY SUMMARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for summary in community_summaries:\n",
    "    print(f\"\\n### Community {summary.community_id}: {summary.title}\")\n",
    "    print(f\"\\n{summary.summary}\")\n",
    "    print(f\"\\nKey Entities: {', '.join(summary.key_entities)}\")\n",
    "    print(f\"\\nKey Insights:\")\n",
    "    for insight in summary.key_insights:\n",
    "        print(f\"  - {insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Store in SQLite\n",
    "\n",
    "Persist the graph structure, metrics, and summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database tables created\n"
     ]
    }
   ],
   "source": [
    "# Create SQLite database\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.executescript(\"\"\"\n",
    "-- Entities table\n",
    "CREATE TABLE IF NOT EXISTS entities (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT UNIQUE NOT NULL,\n",
    "    type TEXT,\n",
    "    description TEXT,\n",
    "    pagerank REAL DEFAULT 0,\n",
    "    degree_centrality REAL DEFAULT 0,\n",
    "    betweenness REAL DEFAULT 0,\n",
    "    community_id INTEGER,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Relationships table\n",
    "CREATE TABLE IF NOT EXISTS relationships (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    source_id INTEGER REFERENCES entities(id),\n",
    "    target_id INTEGER REFERENCES entities(id),\n",
    "    description TEXT,\n",
    "    weight REAL DEFAULT 1.0,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Claims table\n",
    "CREATE TABLE IF NOT EXISTS claims (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    subject_id INTEGER REFERENCES entities(id),\n",
    "    claim_type TEXT,\n",
    "    description TEXT,\n",
    "    claim_date TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Community summaries table\n",
    "CREATE TABLE IF NOT EXISTS community_summaries (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    community_id INTEGER UNIQUE NOT NULL,\n",
    "    title TEXT,\n",
    "    summary TEXT,\n",
    "    key_entities TEXT,  -- JSON array\n",
    "    key_insights TEXT,  -- JSON array\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Chunks table (source text)\n",
    "CREATE TABLE IF NOT EXISTS chunks (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    content TEXT,\n",
    "    chunk_index INTEGER,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Create indexes\n",
    "CREATE INDEX IF NOT EXISTS idx_entities_name ON entities(name);\n",
    "CREATE INDEX IF NOT EXISTS idx_entities_community ON entities(community_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_relationships_source ON relationships(source_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_relationships_target ON relationships(target_id);\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"Database tables created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing data\n"
     ]
    }
   ],
   "source": [
    "# Clear existing data (for re-runs)\n",
    "cursor.executescript(\"\"\"\n",
    "DELETE FROM relationships;\n",
    "DELETE FROM claims;\n",
    "DELETE FROM community_summaries;\n",
    "DELETE FROM chunks;\n",
    "DELETE FROM entities;\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(\"Cleared existing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 14 entities\n"
     ]
    }
   ],
   "source": [
    "# Insert entities\n",
    "entity_id_map: dict[str, int] = {}\n",
    "\n",
    "for node, attrs in G.nodes(data=True):\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO entities (name, type, description, pagerank, degree_centrality, betweenness, community_id)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        node,\n",
    "        attrs.get(\"type\"),\n",
    "        attrs.get(\"description\"),\n",
    "        attrs.get(\"pagerank\", 0),\n",
    "        attrs.get(\"degree_centrality\", 0),\n",
    "        attrs.get(\"betweenness\", 0),\n",
    "        attrs.get(\"community\")\n",
    "    ))\n",
    "    entity_id_map[node] = cursor.lastrowid\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {len(entity_id_map)} entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 6 relationships\n"
     ]
    }
   ],
   "source": [
    "# Insert relationships\n",
    "rel_count = 0\n",
    "for source, target, attrs in G.edges(data=True):\n",
    "    source_id = entity_id_map.get(source)\n",
    "    target_id = entity_id_map.get(target)\n",
    "    if source_id and target_id:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO relationships (source_id, target_id, description, weight)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            source_id,\n",
    "            target_id,\n",
    "            attrs.get(\"description\"),\n",
    "            attrs.get(\"weight\", 1.0)\n",
    "        ))\n",
    "        rel_count += 1\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {rel_count} relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 3 claims\n"
     ]
    }
   ],
   "source": [
    "# Insert claims\n",
    "claim_count = 0\n",
    "for claim in claims:\n",
    "    subject_id = entity_id_map.get(claim[\"subject\"])\n",
    "    if subject_id:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO claims (subject_id, claim_type, description, claim_date)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            subject_id,\n",
    "            claim.get(\"claim_type\"),\n",
    "            claim.get(\"description\"),\n",
    "            claim.get(\"date\")\n",
    "        ))\n",
    "        claim_count += 1\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {claim_count} claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 9 community summaries\n"
     ]
    }
   ],
   "source": [
    "# Insert community summaries\n",
    "for summary in community_summaries:\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO community_summaries (community_id, title, summary, key_entities, key_insights)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        summary.community_id,\n",
    "        summary.title,\n",
    "        summary.summary,\n",
    "        json.dumps(summary.key_entities),\n",
    "        json.dumps(summary.key_insights)\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {len(community_summaries)} community summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 5 chunks\n"
     ]
    }
   ],
   "source": [
    "# Insert chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO chunks (content, chunk_index)\n",
    "        VALUES (?, ?)\n",
    "    \"\"\", (chunk, i))\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATABASE SUMMARY ===\n",
      "  entities: 14 rows\n",
      "  relationships: 6 rows\n",
      "  claims: 3 rows\n",
      "  community_summaries: 9 rows\n",
      "  chunks: 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Verify data\n",
    "print(\"\\n=== DATABASE SUMMARY ===\")\n",
    "for table in [\"entities\", \"relationships\", \"claims\", \"community_summaries\", \"chunks\"]:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"  {table}: {count} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOP ENTITIES (from DB) ===\n",
      "  0.1215 | [PRODUCT] GPT-5 (Community 1)\n",
      "  0.1190 | [PERSON] SUNDAR PICHAI (Community 4)\n",
      "  0.1048 | [PERSON] LINA KAHN (Community 5)\n",
      "  0.0733 | [ORGANIZATION] GOOGLE (Community 4)\n",
      "  0.0715 | [ORGANIZATION] GOLDMAN SACHS (Community 1)\n",
      "  0.0567 | [ORGANIZATION] OPENAI (Community 1)\n",
      "  0.0567 | [ORGANIZATION] MICROSOFT (Community 1)\n",
      "  0.0567 | [PERSON] SAM ALTMAN (Community 2)\n",
      "  0.0567 | [PERSON] SATYA NADELLA (Community 3)\n",
      "  0.0567 | [ORGANIZATION] FEDERAL TRADE COMMISSION (Community 5)\n"
     ]
    }
   ],
   "source": [
    "# Sample query: Top entities by PageRank\n",
    "print(\"\\n=== TOP ENTITIES (from DB) ===\")\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT name, type, pagerank, community_id \n",
    "    FROM entities \n",
    "    ORDER BY pagerank DESC \n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[2]:.4f} | [{row[1]}] {row[0]} (Community {row[3]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMMUNITY 0 DETAILS (from DB) ===\n",
      "Title: StartupXYZ Community\n",
      "Summary: The StartupXYZ community is a platform dedicated to startups, providing support and resources for entrepreneurs. It offers networking opportunities, educational content, and access to funding and mentorship.\n",
      "\n",
      "Top Members:\n",
      "  [ORGANIZATION] STARTUPXYZ\n"
     ]
    }
   ],
   "source": [
    "# Sample query: Get community with its entities\n",
    "print(\"\\n=== COMMUNITY 0 DETAILS (from DB) ===\")\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT title, summary FROM community_summaries WHERE community_id = 0\n",
    "\"\"\")\n",
    "row = cursor.fetchone()\n",
    "if row:\n",
    "    print(f\"Title: {row[0]}\")\n",
    "    print(f\"Summary: {row[1]}\")\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT name, type FROM entities WHERE community_id = 0 ORDER BY pagerank DESC LIMIT 5\n",
    "    \"\"\")\n",
    "    print(\"\\nTop Members:\")\n",
    "    for row in cursor.fetchall():\n",
    "        print(f\"  [{row[1]}] {row[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database saved to: /Users/pacho-home-server/daily-knowledge-ingestion-assistant/notebooks/graphrag.db\n"
     ]
    }
   ],
   "source": [
    "# Close connection\n",
    "conn.close()\n",
    "print(f\"\\nDatabase saved to: {DB_PATH.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed:\n",
    "\n",
    "1. **Graph Construction** - Built NetworkX graph from entities and relationships\n",
    "2. **Graph Metrics** - Computed PageRank, degree centrality, and betweenness\n",
    "3. **Community Detection** - Applied Louvain algorithm to find topic clusters\n",
    "4. **Community Summaries** - Generated LLM-powered summaries for each cluster\n",
    "5. **SQLite Storage** - Persisted everything for retrieval\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook we will:\n",
    "1. **Add embeddings** - Embed entities and chunks using nomic-embed-text\n",
    "2. **Vector search** - Set up sqlite-vec for semantic retrieval\n",
    "3. **Triple-factor retrieval** - Combine semantic + temporal + graph centrality\n",
    "4. **Query interface** - Build the Navigator chat interface"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKIA GraphRAG",
   "language": "python",
   "name": "dkia-graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
