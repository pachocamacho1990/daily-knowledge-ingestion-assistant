{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Step 3: Embeddings & Vector Search\n",
    "\n",
    "This notebook adds semantic search capabilities:\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Pull embedding model** - nomic-embed-text via Ollama\n",
    "2. **Generate embeddings** - Embed entities, claims, and chunks\n",
    "3. **Set up sqlite-vec** - Vector storage and similarity search\n",
    "4. **Triple-factor retrieval** - Combine semantic + temporal + graph centrality\n",
    "5. **Query interface** - Test retrieval with sample queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import struct\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import httpx\n",
    "import sqlite_vec\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "CHAT_MODEL = \"qwen2.5:3b\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"  # 768 dimensions\n",
    "EMBED_DIM = 768\n",
    "DB_PATH = Path(\"graphrag.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pull Embedding Model\n",
    "\n",
    "We need `nomic-embed-text` for generating embeddings. Run this cell to pull it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['nomic-embed-text:latest', 'qwen2.5:3b']\n",
      "\n",
      "✓ Embedding model 'nomic-embed-text' is available\n"
     ]
    }
   ],
   "source": [
    "# Check if embedding model is available\n",
    "response = httpx.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Available models: {models}\")\n",
    "\n",
    "if EMBED_MODEL not in models and f\"{EMBED_MODEL}:latest\" not in models:\n",
    "    print(f\"\\n⚠️  Embedding model '{EMBED_MODEL}' not found.\")\n",
    "    print(f\"Run this in terminal: ollama pull {EMBED_MODEL}\")\n",
    "    print(\"Then re-run this cell.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Embedding model '{EMBED_MODEL}' is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "First 5 values: [-0.006819655, 0.031062, -0.15550135, 0.03674758, 0.02267155]\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Get embedding vector from Ollama.\"\"\"\n",
    "    response = httpx.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/embed\",\n",
    "        json={\"model\": EMBED_MODEL, \"input\": text},\n",
    "        timeout=60.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    # Ollama returns {\"embeddings\": [[...]]}\n",
    "    embeddings = response.json().get(\"embeddings\", [[]])\n",
    "    return embeddings[0] if embeddings else []\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = get_embedding(\"Hello world\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ollama(prompt: str, system: str = \"\", temperature: float = 0.0) -> str:\n",
    "    \"\"\"Send a chat request to Ollama.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = httpx.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/chat\",\n",
    "        json={\n",
    "            \"model\": CHAT_MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        },\n",
    "        timeout=120.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up sqlite-vec\n",
    "\n",
    "Add vector tables to our existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite-vec version: v0.1.6\n"
     ]
    }
   ],
   "source": [
    "# Connect to database and load sqlite-vec extension\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "conn.enable_load_extension(True)\n",
    "sqlite_vec.load(conn)\n",
    "conn.enable_load_extension(False)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Verify sqlite-vec is loaded\n",
    "cursor.execute(\"SELECT vec_version()\")\n",
    "print(f\"sqlite-vec version: {cursor.fetchone()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector tables created\n"
     ]
    }
   ],
   "source": [
    "# Create vector tables\n",
    "cursor.executescript(f\"\"\"\n",
    "-- Entity embeddings (name + description)\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS entity_embeddings USING vec0(\n",
    "    entity_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\n",
    "-- Chunk embeddings (source text)\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS chunk_embeddings USING vec0(\n",
    "    chunk_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\n",
    "-- Claim embeddings\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS claim_embeddings USING vec0(\n",
    "    claim_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(\"Vector tables created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "\n",
    "Embed all entities, chunks, and claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_embedding(embedding: list[float]) -> bytes:\n",
    "    \"\"\"Serialize embedding to bytes for sqlite-vec.\"\"\"\n",
    "    return struct.pack(f\"{len(embedding)}f\", *embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing embeddings\n"
     ]
    }
   ],
   "source": [
    "# Clear existing embeddings (for re-runs)\n",
    "cursor.execute(\"DELETE FROM entity_embeddings\")\n",
    "cursor.execute(\"DELETE FROM chunk_embeddings\")\n",
    "cursor.execute(\"DELETE FROM claim_embeddings\")\n",
    "conn.commit()\n",
    "print(\"Cleared existing embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 14 entities...\n",
      "  Processed 5/14\n",
      "  Processed 10/14\n",
      "Embedded 14 entities\n"
     ]
    }
   ],
   "source": [
    "# Embed entities (name + type + description)\n",
    "cursor.execute(\"SELECT id, name, type, description FROM entities\")\n",
    "entities = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(entities)} entities...\")\n",
    "for i, (entity_id, name, etype, description) in enumerate(entities):\n",
    "    # Combine name, type, and description for richer embedding\n",
    "    text = f\"{name} ({etype}): {description or 'No description'}\"\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO entity_embeddings (entity_id, embedding) VALUES (?, ?)\",\n",
    "        (entity_id, serialize_embedding(embedding))\n",
    "    )\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(entities)}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(entities)} entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 5 chunks...\n",
      "  Processed chunk 1/5\n",
      "  Processed chunk 2/5\n",
      "  Processed chunk 3/5\n",
      "  Processed chunk 4/5\n",
      "  Processed chunk 5/5\n",
      "Embedded 5 chunks\n"
     ]
    }
   ],
   "source": [
    "# Embed chunks (source text)\n",
    "cursor.execute(\"SELECT id, content FROM chunks\")\n",
    "chunks = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(chunks)} chunks...\")\n",
    "for i, (chunk_id, content) in enumerate(chunks):\n",
    "    # Truncate if too long (embedding models have limits)\n",
    "    text = content[:8000] if len(content) > 8000 else content\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO chunk_embeddings (chunk_id, embedding) VALUES (?, ?)\",\n",
    "        (chunk_id, serialize_embedding(embedding))\n",
    "    )\n",
    "    \n",
    "    print(f\"  Processed chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 3 claims...\n",
      "Embedded 3 claims\n"
     ]
    }
   ],
   "source": [
    "# Embed claims\n",
    "cursor.execute(\"SELECT id, claim_type, description FROM claims\")\n",
    "claims = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(claims)} claims...\")\n",
    "for i, (claim_id, claim_type, description) in enumerate(claims):\n",
    "    text = f\"[{claim_type}] {description}\"\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO claim_embeddings (claim_id, embedding) VALUES (?, ?)\",\n",
    "        (claim_id, serialize_embedding(embedding))\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(claims)} claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EMBEDDING COUNTS ===\n",
      "  entity_embeddings: 14\n",
      "  chunk_embeddings: 5\n",
      "  claim_embeddings: 3\n"
     ]
    }
   ],
   "source": [
    "# Verify embeddings\n",
    "print(\"\\n=== EMBEDDING COUNTS ===\")\n",
    "for table in [\"entity_embeddings\", \"chunk_embeddings\", \"claim_embeddings\"]:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"  {table}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Vector Search\n",
    "\n",
    "Basic semantic similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entities(query: str, top_k: int = 5) -> list[tuple]:\n",
    "    \"\"\"Search entities by semantic similarity.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            e.id,\n",
    "            e.name,\n",
    "            e.type,\n",
    "            e.description,\n",
    "            e.pagerank,\n",
    "            vec_distance_cosine(ee.embedding, ?) as distance\n",
    "        FROM entity_embeddings ee\n",
    "        JOIN entities e ON e.id = ee.entity_id\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (serialize_embedding(query_embedding), top_k))\n",
    "    \n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'artificial intelligence companies'\n",
      "\n",
      "=== TOP MATCHING ENTITIES ===\n",
      "\n",
      "[ORGANIZATION] OPENAI\n",
      "  Similarity: 0.7021 | PageRank: 0.0567\n",
      "  Artificial intelligence research company | Company\n",
      "\n",
      "[ORGANIZATION] EXAMPLE CORP\n",
      "  Similarity: 0.6101 | PageRank: 0.0567\n",
      "  Technology company\n",
      "\n",
      "[ORGANIZATION] MICROSOFT\n",
      "  Similarity: 0.5469 | PageRank: 0.0567\n",
      "  Technology company | Company whose shares increased after announcement\n",
      "\n",
      "[ORGANIZATION] FEDERAL TRADE COMMISSION\n",
      "  Similarity: 0.5176 | PageRank: 0.0567\n",
      "  Regulatory body investigating Microsoft-OpenAI relationship | Regulatory body mo...\n",
      "\n",
      "[ORGANIZATION] GOLDMAN SACHS\n",
      "  Similarity: 0.5070 | PageRank: 0.0715\n",
      "  Financial institution providing price target for Microsoft stock\n"
     ]
    }
   ],
   "source": [
    "# Test entity search\n",
    "query = \"artificial intelligence companies\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "results = search_entities(query)\n",
    "print(\"=== TOP MATCHING ENTITIES ===\")\n",
    "for entity_id, name, etype, desc, pagerank, distance in results:\n",
    "    similarity = 1 - distance  # Convert distance to similarity\n",
    "    print(f\"\\n[{etype}] {name}\")\n",
    "    print(f\"  Similarity: {similarity:.4f} | PageRank: {pagerank:.4f}\")\n",
    "    print(f\"  {desc[:80]}...\" if desc and len(desc) > 80 else f\"  {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: 'CEO executives leadership'\n",
      "  0.627 | [ORGANIZATION] EXAMPLE CORP\n",
      "  0.576 | [ORGANIZATION] MICROSOFT\n",
      "  0.564 | [ORGANIZATION] OPENAI\n",
      "\n",
      "==================================================\n",
      "Query: 'stock market financial'\n",
      "  0.557 | [ORGANIZATION] GOLDMAN SACHS\n",
      "  0.509 | [ORGANIZATION] MICROSOFT\n",
      "  0.475 | [ORGANIZATION] EXAMPLE CORP\n",
      "\n",
      "==================================================\n",
      "Query: 'government regulation antitrust'\n",
      "  0.510 | [ORGANIZATION] GOLDMAN SACHS\n",
      "  0.440 | [ORGANIZATION] OPENAI\n",
      "  0.432 | [ORGANIZATION] MICROSOFT\n"
     ]
    }
   ],
   "source": [
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"CEO executives leadership\",\n",
    "    \"stock market financial\",\n",
    "    \"government regulation antitrust\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    results = search_entities(query, top_k=3)\n",
    "    for entity_id, name, etype, desc, pagerank, distance in results:\n",
    "        similarity = 1 - distance\n",
    "        print(f\"  {similarity:.3f} | [{etype}] {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Triple-Factor Retrieval\n",
    "\n",
    "Combine semantic similarity + temporal decay + graph centrality.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "final_score = 0.6 * semantic_similarity + 0.2 * temporal_score + 0.2 * graph_centrality\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    entity_id: int\n",
    "    name: str\n",
    "    entity_type: str\n",
    "    description: str\n",
    "    semantic_score: float\n",
    "    temporal_score: float\n",
    "    graph_score: float\n",
    "    final_score: float\n",
    "    community_id: int = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal decay examples (half_life=7 days):\n",
      "   0 days old: 1.0000\n",
      "   1 days old: 0.9057\n",
      "   3 days old: 0.7430\n",
      "   7 days old: 0.5000\n",
      "  14 days old: 0.2500\n",
      "  30 days old: 0.0513\n"
     ]
    }
   ],
   "source": [
    "def temporal_decay(age_days: float, half_life: float = 7.0) -> float:\n",
    "    \"\"\"Calculate temporal decay score using half-life formula.\n",
    "    \n",
    "    score = 0.5 ^ (age_days / half_life)\n",
    "    \n",
    "    Args:\n",
    "        age_days: Age of content in days\n",
    "        half_life: Days until relevance halves (default 7 for news)\n",
    "    \n",
    "    Returns:\n",
    "        Score between 0 and 1 (1 = fresh, 0 = very old)\n",
    "    \"\"\"\n",
    "    return 0.5 ** (age_days / half_life)\n",
    "\n",
    "# Test temporal decay\n",
    "print(\"Temporal decay examples (half_life=7 days):\")\n",
    "for days in [0, 1, 3, 7, 14, 30]:\n",
    "    score = temporal_decay(days, half_life=7)\n",
    "    print(f\"  {days:2d} days old: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_factor_search(\n",
    "    query: str,\n",
    "    top_k: int = 10,\n",
    "    semantic_weight: float = 0.6,\n",
    "    temporal_weight: float = 0.2,\n",
    "    graph_weight: float = 0.2,\n",
    "    content_age_days: float = 0.0,  # Assume fresh content for demo\n",
    "    half_life: float = 7.0\n",
    ") -> list[RetrievalResult]:\n",
    "    \"\"\"Triple-factor retrieval combining semantic, temporal, and graph signals.\"\"\"\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Fetch all entities with their embeddings and metrics\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            e.id,\n",
    "            e.name,\n",
    "            e.type,\n",
    "            e.description,\n",
    "            e.pagerank,\n",
    "            e.community_id,\n",
    "            vec_distance_cosine(ee.embedding, ?) as distance\n",
    "        FROM entity_embeddings ee\n",
    "        JOIN entities e ON e.id = ee.entity_id\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (serialize_embedding(query_embedding), top_k * 2))  # Fetch more for re-ranking\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Calculate triple-factor scores\n",
    "    results = []\n",
    "    \n",
    "    # Get max pagerank for normalization\n",
    "    max_pagerank = max(row[4] for row in rows) if rows else 1.0\n",
    "    \n",
    "    for entity_id, name, etype, desc, pagerank, community_id, distance in rows:\n",
    "        # Semantic similarity (convert distance to similarity)\n",
    "        semantic_score = 1.0 - distance\n",
    "        \n",
    "        # Temporal score (based on content age)\n",
    "        temporal_score = temporal_decay(content_age_days, half_life)\n",
    "        \n",
    "        # Graph centrality score (normalized pagerank)\n",
    "        graph_score = pagerank / max_pagerank if max_pagerank > 0 else 0\n",
    "        \n",
    "        # Combined score\n",
    "        final_score = (\n",
    "            semantic_weight * semantic_score +\n",
    "            temporal_weight * temporal_score +\n",
    "            graph_weight * graph_score\n",
    "        )\n",
    "        \n",
    "        results.append(RetrievalResult(\n",
    "            entity_id=entity_id,\n",
    "            name=name,\n",
    "            entity_type=etype,\n",
    "            description=desc,\n",
    "            semantic_score=semantic_score,\n",
    "            temporal_score=temporal_score,\n",
    "            graph_score=graph_score,\n",
    "            final_score=final_score,\n",
    "            community_id=community_id\n",
    "        ))\n",
    "    \n",
    "    # Re-rank by final score\n",
    "    results.sort(key=lambda x: -x.final_score)\n",
    "    \n",
    "    return results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'technology partnerships and deals'\n",
      "\n",
      "=== TRIPLE-FACTOR RETRIEVAL RESULTS ===\n",
      "Rank  Entity                    Final    Semantic   Temporal   Graph   \n",
      "---------------------------------------------------------------------------\n",
      "1     SUNDAR PICHAI             0.6461   0.4101     1.0000     1.0000\n",
      "2     FEDERAL TRADE COMMISSION  0.6296   0.5573     1.0000     0.4762\n",
      "3     GOLDMAN SACHS             0.6186   0.4974     1.0000     0.6007\n",
      "4     GOOGLE                    0.6155   0.4870     1.0000     0.6163\n",
      "5     MICROSOFT                 0.5993   0.5068     1.0000     0.4762\n"
     ]
    }
   ],
   "source": [
    "# Test triple-factor search\n",
    "query = \"technology partnerships and deals\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "results = triple_factor_search(query, top_k=5)\n",
    "\n",
    "print(\"=== TRIPLE-FACTOR RETRIEVAL RESULTS ===\")\n",
    "print(f\"{'Rank':<5} {'Entity':<25} {'Final':<8} {'Semantic':<10} {'Temporal':<10} {'Graph':<8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i:<5} {r.name[:24]:<25} {r.final_score:.4f}   {r.semantic_score:.4f}     {r.temporal_score:.4f}     {r.graph_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'AI regulation government'\n",
      "\n",
      "=== SEMANTIC ONLY (100% semantic) ===\n",
      "  1. [ORGANIZATION] EXAMPLE CORP (score: 0.5455)\n",
      "  2. [ORGANIZATION] STARTUPXYZ (score: 0.5384)\n",
      "  3. [PERSON] TREASURY SECRETARY (score: 0.5274)\n",
      "  4. [PERSON] SAM ALTMAN (score: 0.5221)\n",
      "  5. [PERSON] SATYA NADELLA (score: 0.5221)\n",
      "\n",
      "=== TRIPLE-FACTOR (60/20/20) ===\n",
      "  1. [PERSON] SUNDAR PICHAI (score: 0.7092, graph: 0.9794)\n",
      "  2. [PRODUCT] GPT-5 (score: 0.6879, graph: 1.0000)\n",
      "  3. [PERSON] LINA KAHN (score: 0.6848, graph: 0.8628)\n",
      "  4. [ORGANIZATION] GOLDMAN SACHS (score: 0.6273, graph: 0.5883)\n",
      "  5. [ORGANIZATION] EXAMPLE CORP (score: 0.6206, graph: 0.4664)\n"
     ]
    }
   ],
   "source": [
    "# Compare with and without graph centrality boost\n",
    "query = \"AI regulation government\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "print(\"=== SEMANTIC ONLY (100% semantic) ===\")\n",
    "results_semantic = triple_factor_search(query, semantic_weight=1.0, temporal_weight=0.0, graph_weight=0.0)\n",
    "for i, r in enumerate(results_semantic[:5], 1):\n",
    "    print(f\"  {i}. [{r.entity_type}] {r.name} (score: {r.final_score:.4f})\")\n",
    "\n",
    "print(\"\\n=== TRIPLE-FACTOR (60/20/20) ===\")\n",
    "results_triple = triple_factor_search(query, semantic_weight=0.6, temporal_weight=0.2, graph_weight=0.2)\n",
    "for i, r in enumerate(results_triple[:5], 1):\n",
    "    print(f\"  {i}. [{r.entity_type}] {r.name} (score: {r.final_score:.4f}, graph: {r.graph_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Query Interface\n",
    "\n",
    "Build a simple RAG query function that retrieves context and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_community_context(community_id: int) -> str:\n",
    "    \"\"\"Get community summary for additional context.\"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT title, summary, key_insights \n",
    "        FROM community_summaries \n",
    "        WHERE community_id = ?\n",
    "    \"\"\", (community_id,))\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        title, summary, insights_json = row\n",
    "        insights = json.loads(insights_json) if insights_json else []\n",
    "        return f\"Topic: {title}\\nSummary: {summary}\\nKey insights: {'; '.join(insights)}\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_claims(entity_ids: list[int]) -> list[str]:\n",
    "    \"\"\"Get claims related to the retrieved entities.\"\"\"\n",
    "    if not entity_ids:\n",
    "        return []\n",
    "    \n",
    "    placeholders = \",\".join(\"?\" for _ in entity_ids)\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT c.claim_type, c.description, e.name\n",
    "        FROM claims c\n",
    "        JOIN entities e ON e.id = c.subject_id\n",
    "        WHERE c.subject_id IN ({placeholders})\n",
    "    \"\"\", entity_ids)\n",
    "    \n",
    "    claims = []\n",
    "    for claim_type, description, entity_name in cursor.fetchall():\n",
    "        claims.append(f\"[{claim_type}] {entity_name}: {description}\")\n",
    "    return claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigator_query(question: str, top_k: int = 5) -> str:\n",
    "    \"\"\"The Navigator: Answer questions using GraphRAG retrieval.\"\"\"\n",
    "    \n",
    "    # 1. Retrieve relevant entities using triple-factor search\n",
    "    results = triple_factor_search(question, top_k=top_k)\n",
    "    \n",
    "    if not results:\n",
    "        return \"I don't have enough information to answer that question.\"\n",
    "    \n",
    "    # 2. Build context from retrieved entities\n",
    "    entity_context = []\n",
    "    for r in results:\n",
    "        entity_context.append(f\"- {r.name} ({r.entity_type}): {r.description}\")\n",
    "    \n",
    "    # 3. Get community context for the top result\n",
    "    community_context = \"\"\n",
    "    if results[0].community_id is not None:\n",
    "        community_context = get_community_context(results[0].community_id)\n",
    "    \n",
    "    # 4. Get related claims\n",
    "    entity_ids = [r.entity_id for r in results]\n",
    "    claims = get_related_claims(entity_ids)\n",
    "    claims_context = \"\\n\".join(claims[:5]) if claims else \"No specific claims.\"\n",
    "    \n",
    "    # 5. Build prompt\n",
    "    prompt = f\"\"\"You are a knowledgeable assistant. Answer the question based on the following context.\n",
    "\n",
    "RELEVANT ENTITIES:\n",
    "{chr(10).join(entity_context)}\n",
    "\n",
    "TOPIC CONTEXT:\n",
    "{community_context or 'No additional topic context.'}\n",
    "\n",
    "SPECIFIC FACTS:\n",
    "{claims_context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide a clear, concise answer based on the context above. If the context doesn't contain enough information, say so.\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    # 6. Generate answer\n",
    "    answer = chat_ollama(prompt, temperature=0.3)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the partnership between OpenAI and Microsoft?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Answer:\n",
      "Based on the provided context, there is no specific mention or information about any partnership between OpenAI and Microsoft. The context focuses on Google's cloud services, their Gemini Ultra model announcement, and regulatory scrutiny of AI capabilities among tech giants like Microsoft and Google. Therefore, I cannot provide details about a partnership between OpenAI and Microsoft from this given context.\n"
     ]
    }
   ],
   "source": [
    "# Test the Navigator\n",
    "question = \"What is the partnership between OpenAI and Microsoft?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=\" * 50)\n",
    "answer = navigator_query(question)\n",
    "print(f\"\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Q: Who are the key people involved in AI companies?\n",
      "------------------------------------------------------------\n",
      "A: Based on the provided context, Lina Kahn is identified as a key person involved in AI companies through her role as Chair of the Federal Trade Commission (FTC). The context does not provide specific details about other individuals or entities directly involved in AI companies. However, it mentions that regulators like Lina Kahn are closely monitoring the concentration of AI capabilities among a small number of tech giants, which includes Microsoft and OpenAI.\n",
      "\n",
      "============================================================\n",
      "Q: What regulatory concerns exist around AI?\n",
      "------------------------------------------------------------\n",
      "A: Based on the provided context, there are no specific regulatory concerns related to AI mentioned. The context focuses on Google's announcement regarding its Gemini Ultra model and the competitive landscape of cloud services. Therefore, I cannot provide a clear, concise answer about regulatory concerns around AI from this particular context.\n",
      "\n",
      "============================================================\n",
      "Q: How did the stock market react to the AI news?\n",
      "------------------------------------------------------------\n",
      "A: The stock market reacted positively to the AI news as indicated by Microsoft's shares rising 4.2% in after-hours trading following Google's announcement regarding its Gemini Ultra model. This suggests that investors were likely enthusiastic about the advancements in Google's AI and machine learning capabilities, which could be seen as a competitive advantage in the tech industry.\n"
     ]
    }
   ],
   "source": [
    "# Test with more questions\n",
    "questions = [\n",
    "    \"Who are the key people involved in AI companies?\",\n",
    "    \"What regulatory concerns exist around AI?\",\n",
    "    \"How did the stock market react to the AI news?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(\"-\" * 60)\n",
    "    answer = navigator_query(q)\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"\\nDatabase connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed:\n",
    "\n",
    "1. **Embeddings** - Generated 768-dim vectors for entities, chunks, and claims\n",
    "2. **sqlite-vec** - Set up vector storage with cosine similarity search\n",
    "3. **Triple-factor retrieval** - Combined semantic (60%) + temporal (20%) + graph (20%)\n",
    "4. **Navigator query** - Built RAG pipeline for question answering\n",
    "\n",
    "## GraphRAG Pipeline Complete!\n",
    "\n",
    "You now have a working GraphRAG system with:\n",
    "- Entity/relationship/claim extraction\n",
    "- Knowledge graph with community detection\n",
    "- Semantic search with graph-aware ranking\n",
    "- Conversational query interface\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "For the full DKIA system:\n",
    "1. **Source connectors** - Add RSS, HN, arXiv ingestion\n",
    "2. **Temporal tracking** - Add timestamps and decay per content type\n",
    "3. **FastAPI server** - Build the Navigator API\n",
    "4. **Cytoscape.js UI** - Visualize the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKIA GraphRAG",
   "language": "python",
   "name": "dkia-graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
