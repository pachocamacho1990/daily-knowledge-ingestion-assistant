{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Step 3: Embeddings & Vector Search\n",
    "\n",
    "This notebook adds semantic search capabilities:\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Pull embedding model** - nomic-embed-text via Ollama\n",
    "2. **Generate embeddings** - Embed entities, claims, and chunks\n",
    "3. **Set up sqlite-vec** - Vector storage and similarity search\n",
    "4. **Triple-factor retrieval** - Combine semantic + temporal + graph centrality\n",
    "5. **Query interface** - Test retrieval with sample queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import struct\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import httpx\n",
    "import sqlite_vec\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "CHAT_MODEL = \"qwen2.5:3b\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"  # 768 dimensions\n",
    "EMBED_DIM = 768\n",
    "DB_PATH = Path(\"graphrag.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pull Embedding Model\n",
    "\n",
    "We need `nomic-embed-text` for generating embeddings. Run this cell to pull it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['nomic-embed-text:latest', 'qwen2.5:3b']\n",
      "\n",
      "✓ Embedding model 'nomic-embed-text' is available\n"
     ]
    }
   ],
   "source": [
    "# Check if embedding model is available\n",
    "response = httpx.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Available models: {models}\")\n",
    "\n",
    "if EMBED_MODEL not in models and f\"{EMBED_MODEL}:latest\" not in models:\n",
    "    print(f\"\\n⚠️  Embedding model '{EMBED_MODEL}' not found.\")\n",
    "    print(f\"Run this in terminal: ollama pull {EMBED_MODEL}\")\n",
    "    print(\"Then re-run this cell.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Embedding model '{EMBED_MODEL}' is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "First 5 values: [-0.006819655, 0.031062, -0.15550135, 0.03674758, 0.02267155]\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Get embedding vector from Ollama.\"\"\"\n",
    "    response = httpx.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/embed\",\n",
    "        json={\"model\": EMBED_MODEL, \"input\": text},\n",
    "        timeout=60.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    # Ollama returns {\"embeddings\": [[...]]}\n",
    "    embeddings = response.json().get(\"embeddings\", [[]])\n",
    "    return embeddings[0] if embeddings else []\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = get_embedding(\"Hello world\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ollama(prompt: str, system: str = \"\", temperature: float = 0.0) -> str:\n",
    "    \"\"\"Send a chat request to Ollama.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = httpx.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/chat\",\n",
    "        json={\n",
    "            \"model\": CHAT_MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        },\n",
    "        timeout=120.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up sqlite-vec\n",
    "\n",
    "Add vector tables to our existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite-vec version: v0.1.6\n"
     ]
    }
   ],
   "source": [
    "# Connect to database and load sqlite-vec extension\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "conn.enable_load_extension(True)\n",
    "sqlite_vec.load(conn)\n",
    "conn.enable_load_extension(False)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Verify sqlite-vec is loaded\n",
    "cursor.execute(\"SELECT vec_version()\")\n",
    "print(f\"sqlite-vec version: {cursor.fetchone()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector tables created\n"
     ]
    }
   ],
   "source": [
    "# Create vector tables\n",
    "cursor.executescript(f\"\"\"\n",
    "-- Entity embeddings (name + description)\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS entity_embeddings USING vec0(\n",
    "    entity_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\n",
    "-- Chunk embeddings (source text)\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS chunk_embeddings USING vec0(\n",
    "    chunk_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\n",
    "-- Claim embeddings\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS claim_embeddings USING vec0(\n",
    "    claim_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(\"Vector tables created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "\n",
    "Embed all entities, chunks, and claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_embedding(embedding: list[float]) -> bytes:\n",
    "    \"\"\"Serialize embedding to bytes for sqlite-vec.\"\"\"\n",
    "    return struct.pack(f\"{len(embedding)}f\", *embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing embeddings\n"
     ]
    }
   ],
   "source": [
    "# Clear existing embeddings (for re-runs)\n",
    "cursor.execute(\"DELETE FROM entity_embeddings\")\n",
    "cursor.execute(\"DELETE FROM chunk_embeddings\")\n",
    "cursor.execute(\"DELETE FROM claim_embeddings\")\n",
    "conn.commit()\n",
    "print(\"Cleared existing embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 101 entities...\n",
      "  Processed 5/101\n",
      "  Processed 10/101\n",
      "  Processed 15/101\n",
      "  Processed 20/101\n",
      "  Processed 25/101\n",
      "  Processed 30/101\n",
      "  Processed 35/101\n",
      "  Processed 40/101\n",
      "  Processed 45/101\n",
      "  Processed 50/101\n",
      "  Processed 55/101\n",
      "  Processed 60/101\n",
      "  Processed 65/101\n",
      "  Processed 70/101\n",
      "  Processed 75/101\n",
      "  Processed 80/101\n",
      "  Processed 85/101\n",
      "  Processed 90/101\n",
      "  Processed 95/101\n",
      "  Processed 100/101\n",
      "Embedded 101 entities\n"
     ]
    }
   ],
   "source": [
    "# Embed entities (name + type + description)\n",
    "cursor.execute(\"SELECT id, name, type, description FROM entities\")\n",
    "entities = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(entities)} entities...\")\n",
    "for i, (entity_id, name, etype, description) in enumerate(entities):\n",
    "    # Combine name, type, and description for richer embedding\n",
    "    text = f\"{name} ({etype}): {description or 'No description'}\"\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO entity_embeddings (entity_id, embedding) VALUES (?, ?)\",\n",
    "        (entity_id, serialize_embedding(embedding))\n",
    "    )\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(entities)}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(entities)} entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 52 chunks...\n",
      "  Processed chunk 1/52\n",
      "  Processed chunk 2/52\n",
      "  Processed chunk 3/52\n",
      "  Processed chunk 4/52\n",
      "  Processed chunk 5/52\n",
      "  Processed chunk 6/52\n",
      "  Processed chunk 7/52\n",
      "  Processed chunk 8/52\n",
      "  Processed chunk 9/52\n",
      "  Processed chunk 10/52\n",
      "  Processed chunk 11/52\n",
      "  Processed chunk 12/52\n",
      "  Processed chunk 13/52\n",
      "  Processed chunk 14/52\n",
      "  Processed chunk 15/52\n",
      "  Processed chunk 16/52\n",
      "  Processed chunk 17/52\n",
      "  Processed chunk 18/52\n",
      "  Processed chunk 19/52\n",
      "  Processed chunk 20/52\n",
      "  Processed chunk 21/52\n",
      "  Processed chunk 22/52\n",
      "  Processed chunk 23/52\n",
      "  Processed chunk 24/52\n",
      "  Processed chunk 25/52\n",
      "  Processed chunk 26/52\n",
      "  Processed chunk 27/52\n",
      "  Processed chunk 28/52\n",
      "  Processed chunk 29/52\n",
      "  Processed chunk 30/52\n",
      "  Processed chunk 31/52\n",
      "  Processed chunk 32/52\n",
      "  Processed chunk 33/52\n",
      "  Processed chunk 34/52\n",
      "  Processed chunk 35/52\n",
      "  Processed chunk 36/52\n",
      "  Processed chunk 37/52\n",
      "  Processed chunk 38/52\n",
      "  Processed chunk 39/52\n",
      "  Processed chunk 40/52\n",
      "  Processed chunk 41/52\n",
      "  Processed chunk 42/52\n",
      "  Processed chunk 43/52\n",
      "  Processed chunk 44/52\n",
      "  Processed chunk 45/52\n",
      "  Processed chunk 46/52\n",
      "  Processed chunk 47/52\n",
      "  Processed chunk 48/52\n",
      "  Processed chunk 49/52\n",
      "  Processed chunk 50/52\n",
      "  Processed chunk 51/52\n",
      "  Processed chunk 52/52\n",
      "Embedded 52 chunks\n"
     ]
    }
   ],
   "source": [
    "# Embed chunks (source text)\n",
    "cursor.execute(\"SELECT id, content FROM chunks\")\n",
    "chunks = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(chunks)} chunks...\")\n",
    "for i, (chunk_id, content) in enumerate(chunks):\n",
    "    # Truncate if too long (embedding models have limits)\n",
    "    text = content[:8000] if len(content) > 8000 else content\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO chunk_embeddings (chunk_id, embedding) VALUES (?, ?)\",\n",
    "        (chunk_id, serialize_embedding(embedding))\n",
    "    )\n",
    "    \n",
    "    print(f\"  Processed chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 claims...\n",
      "Embedded 1 claims\n"
     ]
    }
   ],
   "source": [
    "# Embed claims\n",
    "cursor.execute(\"SELECT id, claim_type, description FROM claims\")\n",
    "claims = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(claims)} claims...\")\n",
    "for i, (claim_id, claim_type, description) in enumerate(claims):\n",
    "    text = f\"[{claim_type}] {description}\"\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO claim_embeddings (claim_id, embedding) VALUES (?, ?)\",\n",
    "        (claim_id, serialize_embedding(embedding))\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(claims)} claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EMBEDDING COUNTS ===\n",
      "  entity_embeddings: 101\n",
      "  chunk_embeddings: 52\n",
      "  claim_embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "# Verify embeddings\n",
    "print(\"\\n=== EMBEDDING COUNTS ===\")\n",
    "for table in [\"entity_embeddings\", \"chunk_embeddings\", \"claim_embeddings\"]:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"  {table}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Vector Search\n",
    "\n",
    "Basic semantic similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entities(query: str, top_k: int = 5) -> list[tuple]:\n",
    "    \"\"\"Search entities by semantic similarity.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            e.id,\n",
    "            e.name,\n",
    "            e.type,\n",
    "            e.description,\n",
    "            e.pagerank,\n",
    "            vec_distance_cosine(ee.embedding, ?) as distance\n",
    "        FROM entity_embeddings ee\n",
    "        JOIN entities e ON e.id = ee.entity_id\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (serialize_embedding(query_embedding), top_k))\n",
    "    \n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'artificial intelligence companies'\n",
      "\n",
      "=== TOP MATCHING ENTITIES ===\n",
      "\n",
      "[ORGANIZATION] LLM\n",
      "  Similarity: 0.5818 | PageRank: 0.0175\n",
      "  Large Language Model | Large language model used in the approach to build a grap...\n",
      "\n",
      "[ORGANIZATION] EXAMPLE CORP\n",
      "  Similarity: 0.5535 | PageRank: 0.0078\n",
      "  Technology company acquiring StartupXYZ\n",
      "\n",
      "[CONCEPT] CRISPR-GPT\n",
      "  Similarity: 0.5510 | PageRank: 0.0144\n",
      "  AI model for agentic automation in gene editing experiments | LLM agent for auto...\n",
      "\n",
      "[ORGANIZATION] RAG SYSTEMS\n",
      "  Similarity: 0.5468 | PageRank: 0.0078\n",
      "  Systems that use retrieval-augmented generation\n",
      "\n",
      "[LOCATION] EXTERNAL KNOWLEDGE SOURCE\n",
      "  Similarity: 0.5246 | PageRank: 0.0113\n",
      "  Source of information for retrieval-augmented generation\n"
     ]
    }
   ],
   "source": [
    "# Test entity search\n",
    "query = \"artificial intelligence companies\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "results = search_entities(query)\n",
    "print(\"=== TOP MATCHING ENTITIES ===\")\n",
    "for entity_id, name, etype, desc, pagerank, distance in results:\n",
    "    similarity = 1 - distance  # Convert distance to similarity\n",
    "    print(f\"\\n[{etype}] {name}\")\n",
    "    print(f\"  Similarity: {similarity:.4f} | PageRank: {pagerank:.4f}\")\n",
    "    print(f\"  {desc[:80]}...\" if desc and len(desc) > 80 else f\"  {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: 'CEO executives leadership'\n",
      "  0.584 | [CONCEPT] SOCIAL DECENTRALIZED AUTONOMOUS ORGANIZATIONS\n",
      "  0.568 | [LOCATION] JUPITER\n",
      "  0.568 | [LOCATION] URANUS\n",
      "\n",
      "==================================================\n",
      "Query: 'stock market financial'\n",
      "  0.516 | [LOCATION] ADVANCED ECONOMIES\n",
      "  0.494 | [LOCATION] EMERGING MARKET AND DEVELOPING ECONOMIES\n",
      "  0.480 | [ORGANIZATION] EXAMPLE CORP\n",
      "\n",
      "==================================================\n",
      "Query: 'government regulation antitrust'\n",
      "  0.562 | [LOCATION] ADVANCED ECONOMIES\n",
      "  0.537 | [PRODUCT] JAMES WEBB SPACE TELESCOPE\n",
      "  0.507 | [PRODUCT] TRUSTED BLOCKCHAIN DECENTRALIZED APPLICATIONS\n"
     ]
    }
   ],
   "source": [
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"CEO executives leadership\",\n",
    "    \"stock market financial\",\n",
    "    \"government regulation antitrust\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    results = search_entities(query, top_k=3)\n",
    "    for entity_id, name, etype, desc, pagerank, distance in results:\n",
    "        similarity = 1 - distance\n",
    "        print(f\"  {similarity:.3f} | [{etype}] {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Triple-Factor Retrieval\n",
    "\n",
    "Combine semantic similarity + temporal decay + graph centrality.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "final_score = 0.6 * semantic_similarity + 0.2 * temporal_score + 0.2 * graph_centrality\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    entity_id: int\n",
    "    name: str\n",
    "    entity_type: str\n",
    "    description: str\n",
    "    semantic_score: float\n",
    "    temporal_score: float\n",
    "    graph_score: float\n",
    "    final_score: float\n",
    "    community_id: int = None\n",
    "    source_refs: list[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.source_refs is None:\n",
    "            self.source_refs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal decay by content type:\n",
      "  Age   news (7d)   paper (30d)    ref (365d)\n",
      "---------------------------------------------\n",
      "    0      1.0000        1.0000        1.0000\n",
      "    1      0.9057        0.9772        0.9981\n",
      "    7      0.5000        0.8507        0.9868\n",
      "   14      0.2500        0.7236        0.9738\n",
      "   30      0.0513        0.5000        0.9446\n",
      "   90      0.0001        0.1250        0.8429\n",
      "  365      0.0000        0.0002        0.5000\n"
     ]
    }
   ],
   "source": [
    "HALF_LIVES = {\n",
    "    \"news\": 7,\n",
    "    \"research_paper\": 30,\n",
    "    \"reference\": 365,\n",
    "}\n",
    "\n",
    "def temporal_decay(age_days: float, half_life: float = 7.0) -> float:\n",
    "    \"\"\"Calculate temporal decay score using half-life formula.\n",
    "    \n",
    "    score = 0.5 ^ (age_days / half_life)\n",
    "    \n",
    "    Args:\n",
    "        age_days: Age of content in days\n",
    "        half_life: Days until relevance halves\n",
    "    \n",
    "    Returns:\n",
    "        Score between 0 and 1 (1 = fresh, 0 = very old)\n",
    "    \"\"\"\n",
    "    return 0.5 ** (age_days / half_life)\n",
    "\n",
    "\n",
    "def get_half_life_for_entity(entity_id: int) -> float:\n",
    "    \"\"\"Look up the content_type(s) of sources for an entity and return the appropriate half-life.\n",
    "    \n",
    "    If an entity appears in multiple sources with different content types,\n",
    "    use the longest half-life (most persistent content type wins).\n",
    "    \"\"\"\n",
    "    cursor.execute(\"SELECT source_refs FROM entities WHERE id = ?\", (entity_id,))\n",
    "    row = cursor.fetchone()\n",
    "    if not row or not row[0]:\n",
    "        return HALF_LIVES[\"news\"]  # Default\n",
    "    \n",
    "    try:\n",
    "        source_ids = json.loads(row[0])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return HALF_LIVES[\"news\"]\n",
    "    \n",
    "    if not source_ids:\n",
    "        return HALF_LIVES[\"news\"]\n",
    "    \n",
    "    # Look up content_type for each source\n",
    "    max_half_life = HALF_LIVES[\"news\"]\n",
    "    for sid in source_ids:\n",
    "        cursor.execute(\"SELECT content_type FROM sources WHERE source_id = ?\", (sid,))\n",
    "        src_row = cursor.fetchone()\n",
    "        if src_row and src_row[0]:\n",
    "            hl = HALF_LIVES.get(src_row[0], HALF_LIVES[\"news\"])\n",
    "            max_half_life = max(max_half_life, hl)\n",
    "    \n",
    "    return max_half_life\n",
    "\n",
    "\n",
    "# Test temporal decay with content-type awareness\n",
    "print(\"Temporal decay by content type:\")\n",
    "print(f\"{'Age':>5}  {'news (7d)':>10}  {'paper (30d)':>12}  {'ref (365d)':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for days in [0, 1, 7, 14, 30, 90, 365]:\n",
    "    scores = [temporal_decay(days, HALF_LIVES[ct]) for ct in [\"news\", \"research_paper\", \"reference\"]]\n",
    "    print(f\"{days:>5}  {scores[0]:>10.4f}  {scores[1]:>12.4f}  {scores[2]:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_factor_search(\n",
    "    query: str,\n",
    "    top_k: int = 10,\n",
    "    semantic_weight: float = 0.6,\n",
    "    temporal_weight: float = 0.2,\n",
    "    graph_weight: float = 0.2,\n",
    "    content_age_days: float = 0.0,  # Assume fresh content for demo\n",
    ") -> list[RetrievalResult]:\n",
    "    \"\"\"Triple-factor retrieval combining semantic, temporal, and graph signals.\n",
    "    \n",
    "    Uses per-entity half-life based on source content_type:\n",
    "    - news: 7 days\n",
    "    - research_paper: 30 days\n",
    "    - reference: 365 days\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Fetch all entities with their embeddings and metrics\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            e.id,\n",
    "            e.name,\n",
    "            e.type,\n",
    "            e.description,\n",
    "            e.pagerank,\n",
    "            e.community_id,\n",
    "            e.source_refs,\n",
    "            vec_distance_cosine(ee.embedding, ?) as distance\n",
    "        FROM entity_embeddings ee\n",
    "        JOIN entities e ON e.id = ee.entity_id\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (serialize_embedding(query_embedding), top_k * 2))  # Fetch more for re-ranking\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Calculate triple-factor scores\n",
    "    results = []\n",
    "    \n",
    "    # Get max pagerank for normalization\n",
    "    max_pagerank = max(row[4] for row in rows) if rows else 1.0\n",
    "    \n",
    "    for entity_id, name, etype, desc, pagerank_val, community_id, source_refs_json, distance in rows:\n",
    "        # Semantic similarity (convert distance to similarity)\n",
    "        semantic_score = 1.0 - distance\n",
    "        \n",
    "        # Temporal score (per-entity half-life based on content type)\n",
    "        entity_half_life = get_half_life_for_entity(entity_id)\n",
    "        temporal_score = temporal_decay(content_age_days, entity_half_life)\n",
    "        \n",
    "        # Graph centrality score (normalized pagerank)\n",
    "        graph_score = pagerank_val / max_pagerank if max_pagerank > 0 else 0\n",
    "        \n",
    "        # Combined score\n",
    "        final_score = (\n",
    "            semantic_weight * semantic_score +\n",
    "            temporal_weight * temporal_score +\n",
    "            graph_weight * graph_score\n",
    "        )\n",
    "        \n",
    "        # Parse source refs\n",
    "        try:\n",
    "            source_refs = json.loads(source_refs_json) if source_refs_json else []\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            source_refs = []\n",
    "        \n",
    "        results.append(RetrievalResult(\n",
    "            entity_id=entity_id,\n",
    "            name=name,\n",
    "            entity_type=etype,\n",
    "            description=desc,\n",
    "            semantic_score=semantic_score,\n",
    "            temporal_score=temporal_score,\n",
    "            graph_score=graph_score,\n",
    "            final_score=final_score,\n",
    "            community_id=community_id,\n",
    "            source_refs=source_refs,\n",
    "        ))\n",
    "    \n",
    "    # Re-rank by final score\n",
    "    results.sort(key=lambda x: -x.final_score)\n",
    "    \n",
    "    return results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'technology partnerships and deals'\n",
      "\n",
      "=== TRIPLE-FACTOR RETRIEVAL RESULTS ===\n",
      "Rank  Entity                    Final    Semantic   Temporal   Graph   \n",
      "---------------------------------------------------------------------------\n",
      "1     AI                        0.7728   0.6213     1.0000     1.0000\n",
      "2     STARTUPXYZ                0.6821   0.5269     1.0000     0.8298\n",
      "3     ADVANCED ECONOMIES        0.6379   0.5252     1.0000     0.6140\n",
      "4     MOBILE PHONES             0.6360   0.5278     1.0000     0.5968\n",
      "5     EMERGING MARKET AND DEVE  0.6322   0.5702     1.0000     0.4505\n"
     ]
    }
   ],
   "source": [
    "# Test triple-factor search\n",
    "query = \"technology partnerships and deals\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "results = triple_factor_search(query, top_k=5)\n",
    "\n",
    "print(\"=== TRIPLE-FACTOR RETRIEVAL RESULTS ===\")\n",
    "print(f\"{'Rank':<5} {'Entity':<25} {'Final':<8} {'Semantic':<10} {'Temporal':<10} {'Graph':<8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i:<5} {r.name[:24]:<25} {r.final_score:.4f}   {r.semantic_score:.4f}     {r.temporal_score:.4f}     {r.graph_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'AI regulation government'\n",
      "\n",
      "=== SEMANTIC ONLY (100% semantic) ===\n",
      "  1. [LOCATION] ADVANCED ECONOMIES (score: 0.6413)\n",
      "  2. [PRODUCT] JAMES WEBB SPACE TELESCOPE (score: 0.6270)\n",
      "  3. [CONCEPT] LABOR-MARKET (score: 0.5802)\n",
      "  4. [LOCATION] JUPITER (score: 0.5796)\n",
      "  5. [LOCATION] URANUS (score: 0.5796)\n",
      "\n",
      "=== TRIPLE-FACTOR (60/20/20) ===\n",
      "  1. [LOCATION] ADVANCED ECONOMIES (score: 0.7275, graph: 0.7134)\n",
      "  2. [PRODUCT] TRUSTED BLOCKCHAIN DECENTRALIZED APPLICATIONS (score: 0.7151, graph: 1.0000)\n",
      "  3. [PRODUCT] JAMES WEBB SPACE TELESCOPE (score: 0.6512, graph: 0.3748)\n",
      "  4. [CONCEPT] LABOR-MARKET (score: 0.6231, graph: 0.3748)\n",
      "  5. [LOCATION] JUPITER (score: 0.6227, graph: 0.3748)\n"
     ]
    }
   ],
   "source": [
    "# Compare with and without graph centrality boost\n",
    "query = \"AI regulation government\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "print(\"=== SEMANTIC ONLY (100% semantic) ===\")\n",
    "results_semantic = triple_factor_search(query, semantic_weight=1.0, temporal_weight=0.0, graph_weight=0.0)\n",
    "for i, r in enumerate(results_semantic[:5], 1):\n",
    "    print(f\"  {i}. [{r.entity_type}] {r.name} (score: {r.final_score:.4f})\")\n",
    "\n",
    "print(\"\\n=== TRIPLE-FACTOR (60/20/20) ===\")\n",
    "results_triple = triple_factor_search(query, semantic_weight=0.6, temporal_weight=0.2, graph_weight=0.2)\n",
    "for i, r in enumerate(results_triple[:5], 1):\n",
    "    print(f\"  {i}. [{r.entity_type}] {r.name} (score: {r.final_score:.4f}, graph: {r.graph_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Query Interface\n",
    "\n",
    "Build a simple RAG query function that retrieves context and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_community_context(community_id: int) -> str:\n",
    "    \"\"\"Get community summary for additional context.\"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT title, summary, key_insights \n",
    "        FROM community_summaries \n",
    "        WHERE community_id = ?\n",
    "    \"\"\", (community_id,))\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        title, summary, insights_json = row\n",
    "        insights = json.loads(insights_json) if insights_json else []\n",
    "        return f\"Topic: {title}\\nSummary: {summary}\\nKey insights: {'; '.join(insights)}\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_claims(entity_ids: list[int]) -> list[str]:\n",
    "    \"\"\"Get claims related to the retrieved entities.\"\"\"\n",
    "    if not entity_ids:\n",
    "        return []\n",
    "    \n",
    "    placeholders = \",\".join(\"?\" for _ in entity_ids)\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT c.claim_type, c.description, e.name\n",
    "        FROM claims c\n",
    "        JOIN entities e ON e.id = c.subject_id\n",
    "        WHERE c.subject_id IN ({placeholders})\n",
    "    \"\"\", entity_ids)\n",
    "    \n",
    "    claims = []\n",
    "    for claim_type, description, entity_name in cursor.fetchall():\n",
    "        claims.append(f\"[{claim_type}] {entity_name}: {description}\")\n",
    "    return claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigator_query(question: str, top_k: int = 5) -> str:\n",
    "    \"\"\"The Navigator: Answer questions using GraphRAG retrieval.\"\"\"\n",
    "    \n",
    "    # 1. Retrieve relevant entities using triple-factor search\n",
    "    results = triple_factor_search(question, top_k=top_k)\n",
    "    \n",
    "    if not results:\n",
    "        return \"I don't have enough information to answer that question.\"\n",
    "    \n",
    "    # 2. Build context from retrieved entities (with source provenance)\n",
    "    entity_context = []\n",
    "    for r in results:\n",
    "        sources_str = \"\"\n",
    "        if r.source_refs:\n",
    "            sources_str = f\" [from: {', '.join(r.source_refs)}]\"\n",
    "        entity_context.append(f\"- {r.name} ({r.entity_type}): {r.description}{sources_str}\")\n",
    "    \n",
    "    # 3. Get community context for the top result\n",
    "    community_context = \"\"\n",
    "    if results[0].community_id is not None:\n",
    "        community_context = get_community_context(results[0].community_id)\n",
    "    \n",
    "    # 4. Get related claims\n",
    "    entity_ids = [r.entity_id for r in results]\n",
    "    claims = get_related_claims(entity_ids)\n",
    "    claims_context = \"\\n\".join(claims[:5]) if claims else \"No specific claims.\"\n",
    "    \n",
    "    # 5. Collect unique source provenance\n",
    "    all_sources = set()\n",
    "    for r in results:\n",
    "        all_sources.update(r.source_refs)\n",
    "    source_provenance = f\"\\nSources consulted: {', '.join(sorted(all_sources))}\" if all_sources else \"\"\n",
    "    \n",
    "    # 6. Build prompt\n",
    "    prompt = f\"\"\"You are a knowledgeable assistant. Answer the question based on the following context.\n",
    "\n",
    "RELEVANT ENTITIES:\n",
    "{chr(10).join(entity_context)}\n",
    "\n",
    "TOPIC CONTEXT:\n",
    "{community_context or 'No additional topic context.'}\n",
    "\n",
    "SPECIFIC FACTS:\n",
    "{claims_context}\n",
    "{source_provenance}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide a clear, concise answer based on the context above. If the context doesn't contain enough information, say so. Reference the source domains when relevant.\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    # 7. Generate answer\n",
    "    answer = chat_ollama(prompt, temperature=0.3)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the partnership between OpenAI and Microsoft?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Answer:\n",
      "The provided context does not contain any information about the partnership between OpenAI and Microsoft. The context is related to the International Monetary Fund's study on the impact of artificial intelligence on labor markets, which involves staff analysis and use of an index to assess country readiness for AI adoption in the labor market. It does not mention anything about partnerships or collaborations involving companies like OpenAI or Microsoft.\n"
     ]
    }
   ],
   "source": [
    "# Test the Navigator\n",
    "question = \"What is the partnership between OpenAI and Microsoft?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=\" * 50)\n",
    "answer = navigator_query(question)\n",
    "print(f\"\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Q: What is the role of AI in scientific research?\n",
      "------------------------------------------------------------\n",
      "Sources: arxiv:2304.04869, arxiv:2312.14090, web:imf-ai-economy, web:planetary-voyager, web:quanta-memory\n",
      "A: Based on the provided context, Artificial Intelligence (AI) plays a significant role in scientific research, particularly in analyzing complex data and improving methodologies for designing blockchain applications. The context mentions that AI is utilized to analyze sextortion cases and enhance the design of blockchain applications. This indicates that AI contributes to more efficient analysis processes within these fields.\n",
      "\n",
      "However, it's important to note that the specific details about AI’s role in scientific research are not provided directly in the given context. Therefore, while we can infer that AI aids in data analysis and methodology development, a comprehensive understanding of its exact contributions would require additional information from sources related to AI applications in scientific research.\n",
      "\n",
      "============================================================\n",
      "Q: What large-scale observation instruments are used in science?\n",
      "------------------------------------------------------------\n",
      "Sources: arxiv:2304.04869, arxiv:2312.14090, web:imf-ai-economy, web:planetary-voyager\n",
      "A: Based on the provided context, there is not enough specific information about large-scale observation instruments used in science. The context discusses AI and blockchain applications for sextortion analysis, as well as observations related to planets and satellite rings of Saturn, but does not mention any large-scale observation instruments commonly used in scientific research.\n",
      "\n",
      "============================================================\n",
      "Q: What are the economic impacts of emerging technologies?\n",
      "------------------------------------------------------------\n",
      "Sources: arxiv:2312.14090, web:imf-ai-economy\n",
      "A: Based on the provided context and sources, emerging technologies like AI have significant potential economic impacts, particularly affecting low-income countries and emerging markets. The introduction of advanced economies suggests that about 60 percent of jobs in these regions may be impacted by AI. This could lead to both productivity gains and shifts in income inequality, with some areas facing challenges due to automation.\n",
      "\n",
      "Emerging market and developing economies are prioritizing digital infrastructure and workforce development as they leverage AI for opportunities such as improving sextortion analysis through the use of AI and blockchain applications. These regions may face even lower exposure to AI compared to other regions, highlighting a need for targeted initiatives to ensure equitable access and benefits from these technologies.\n",
      "\n",
      "In contrast, advanced economies are focusing on AI innovation and regulation, indicating that they might be more adept at managing the economic impacts of emerging technologies like AI. The context does not provide specific details about blockchain applications' economic impacts in emerging markets or their potential role in sextortion cases, but it suggests that these technologies could contribute to improved security and transparency.\n",
      "\n",
      "In summary, while AI has significant implications for productivity and income inequality across various economies, the impact on low-income countries is expected to be even more pronounced. Emerging market and developing economies are positioning themselves to benefit from AI through digital infrastructure development and workforce training, which may mitigate some of these negative impacts.\n",
      "\n",
      "============================================================\n",
      "Q: How is graph-based knowledge discovery used in information retrieval?\n",
      "------------------------------------------------------------\n",
      "Sources: arxiv:2312.14090, arxiv:2404.16130, arxiv:2404.18021, web:imf-ai-economy\n",
      "A: Graph-based knowledge discovery is not directly mentioned or discussed within the provided context for information retrieval. However, the context does mention GraphRAG, a proposed method that uses Large Language Models (LLMs) in two stages to build graph indexes for efficient question answering over private text corpora. This approach suggests that graph-based indexing and knowledge discovery are used in information retrieval by enabling more efficient and structured ways of querying and understanding documents within a corpus.\n"
     ]
    }
   ],
   "source": [
    "# Cross-domain test queries spanning multiple source domains\n",
    "questions = [\n",
    "    \"What is the role of AI in scientific research?\",\n",
    "    \"What large-scale observation instruments are used in science?\",\n",
    "    \"What are the economic impacts of emerging technologies?\",\n",
    "    \"How is graph-based knowledge discovery used in information retrieval?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Show which sources the retrieval pulls from\n",
    "    results = triple_factor_search(q, top_k=5)\n",
    "    source_domains = set()\n",
    "    for r in results:\n",
    "        source_domains.update(r.source_refs)\n",
    "    if source_domains:\n",
    "        print(f\"Sources: {', '.join(sorted(source_domains))}\")\n",
    "    \n",
    "    answer = navigator_query(q)\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"\\nDatabase connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed:\n",
    "\n",
    "1. **Embeddings** - Generated 768-dim vectors for entities, chunks, and claims\n",
    "2. **sqlite-vec** - Set up vector storage with cosine similarity search\n",
    "3. **Content-type-aware temporal decay** - Different half-lives for news (7d), papers (30d), reference (365d)\n",
    "4. **Triple-factor retrieval** - Combined semantic (60%) + temporal (20%) + graph (20%) with per-entity half-lives\n",
    "5. **Source provenance** - Navigator answers reference which source domains contributed\n",
    "6. **Cross-domain queries** - Tested retrieval spanning AI, biology, climate, astrophysics, neuroscience, economics, space\n",
    "\n",
    "## GraphRAG Multi-Source Pipeline Complete!\n",
    "\n",
    "You now have a working multi-source GraphRAG system with:\n",
    "- 7 diverse sources across 5+ domains\n",
    "- Entity/relationship/claim extraction with cross-document merge\n",
    "- Knowledge graph with community detection and interactive visualization\n",
    "- Semantic search with content-type-aware temporal decay and graph-aware ranking\n",
    "- Conversational query interface with source provenance\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "For the full DKIA system:\n",
    "1. **Source connectors** - Add RSS, HN, arXiv ingestion (automated)\n",
    "2. **FastAPI server** - Build the Navigator API\n",
    "3. **Cytoscape.js UI** - Production graph visualization\n",
    "4. **APScheduler** - Overnight pipeline automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKIA GraphRAG",
   "language": "python",
   "name": "dkia-graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
