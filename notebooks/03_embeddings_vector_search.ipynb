{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Step 3: Embeddings & Vector Search\n",
    "\n",
    "This notebook adds semantic search capabilities:\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Pull embedding model** - nomic-embed-text via Ollama\n",
    "2. **Generate embeddings** - Embed entities, claims, and chunks\n",
    "3. **Set up sqlite-vec** - Vector storage and similarity search\n",
    "4. **Triple-factor retrieval** - Combine semantic + temporal + graph centrality\n",
    "5. **Query interface** - Test retrieval with sample queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import struct\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import httpx\n",
    "import sqlite_vec\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "CHAT_MODEL = \"qwen2.5:3b\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"  # 768 dimensions\n",
    "EMBED_DIM = 768\n",
    "DB_PATH = Path(\"graphrag.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pull Embedding Model\n",
    "\n",
    "We need `nomic-embed-text` for generating embeddings. Run this cell to pull it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['nomic-embed-text:latest', 'qwen2.5:3b']\n",
      "\n",
      "✓ Embedding model 'nomic-embed-text' is available\n"
     ]
    }
   ],
   "source": [
    "# Check if embedding model is available\n",
    "response = httpx.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Available models: {models}\")\n",
    "\n",
    "if EMBED_MODEL not in models and f\"{EMBED_MODEL}:latest\" not in models:\n",
    "    print(f\"\\n⚠️  Embedding model '{EMBED_MODEL}' not found.\")\n",
    "    print(f\"Run this in terminal: ollama pull {EMBED_MODEL}\")\n",
    "    print(\"Then re-run this cell.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Embedding model '{EMBED_MODEL}' is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "First 5 values: [-0.006819655, 0.031062, -0.15550135, 0.03674758, 0.02267155]\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Get embedding vector from Ollama.\"\"\"\n",
    "    response = httpx.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/embed\",\n",
    "        json={\"model\": EMBED_MODEL, \"input\": text},\n",
    "        timeout=60.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    # Ollama returns {\"embeddings\": [[...]]}\n",
    "    embeddings = response.json().get(\"embeddings\", [[]])\n",
    "    return embeddings[0] if embeddings else []\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = get_embedding(\"Hello world\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ollama(prompt: str, system: str = \"\", temperature: float = 0.0) -> str:\n",
    "    \"\"\"Send a chat request to Ollama.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = httpx.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/chat\",\n",
    "        json={\n",
    "            \"model\": CHAT_MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        },\n",
    "        timeout=120.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up sqlite-vec\n",
    "\n",
    "Add vector tables to our existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite-vec version: v0.1.6\n"
     ]
    }
   ],
   "source": [
    "# Connect to database and load sqlite-vec extension\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "conn.enable_load_extension(True)\n",
    "sqlite_vec.load(conn)\n",
    "conn.enable_load_extension(False)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Verify sqlite-vec is loaded\n",
    "cursor.execute(\"SELECT vec_version()\")\n",
    "print(f\"sqlite-vec version: {cursor.fetchone()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector tables created\n"
     ]
    }
   ],
   "source": [
    "# Create vector tables\n",
    "cursor.executescript(f\"\"\"\n",
    "-- Entity embeddings (name + description)\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS entity_embeddings USING vec0(\n",
    "    entity_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\n",
    "-- Chunk embeddings (source text)\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS chunk_embeddings USING vec0(\n",
    "    chunk_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\n",
    "-- Claim embeddings\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS claim_embeddings USING vec0(\n",
    "    claim_id INTEGER PRIMARY KEY,\n",
    "    embedding FLOAT[{EMBED_DIM}]\n",
    ");\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(\"Vector tables created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "\n",
    "Embed all entities, chunks, and claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_embedding(embedding: list[float]) -> bytes:\n",
    "    \"\"\"Serialize embedding to bytes for sqlite-vec.\"\"\"\n",
    "    return struct.pack(f\"{len(embedding)}f\", *embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing embeddings\n"
     ]
    }
   ],
   "source": [
    "# Clear existing embeddings (for re-runs)\n",
    "cursor.execute(\"DELETE FROM entity_embeddings\")\n",
    "cursor.execute(\"DELETE FROM chunk_embeddings\")\n",
    "cursor.execute(\"DELETE FROM claim_embeddings\")\n",
    "conn.commit()\n",
    "print(\"Cleared existing embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 864 entities...\n",
      "  Processed 5/864\n",
      "  Processed 10/864\n",
      "  Processed 15/864\n",
      "  Processed 20/864\n",
      "  Processed 25/864\n",
      "  Processed 30/864\n",
      "  Processed 35/864\n",
      "  Processed 40/864\n",
      "  Processed 45/864\n",
      "  Processed 50/864\n",
      "  Processed 55/864\n",
      "  Processed 60/864\n",
      "  Processed 65/864\n",
      "  Processed 70/864\n",
      "  Processed 75/864\n",
      "  Processed 80/864\n",
      "  Processed 85/864\n",
      "  Processed 90/864\n",
      "  Processed 95/864\n",
      "  Processed 100/864\n",
      "  Processed 105/864\n",
      "  Processed 110/864\n",
      "  Processed 115/864\n",
      "  Processed 120/864\n",
      "  Processed 125/864\n",
      "  Processed 130/864\n",
      "  Processed 135/864\n",
      "  Processed 140/864\n",
      "  Processed 145/864\n",
      "  Processed 150/864\n",
      "  Processed 155/864\n",
      "  Processed 160/864\n",
      "  Processed 165/864\n",
      "  Processed 170/864\n",
      "  Processed 175/864\n",
      "  Processed 180/864\n",
      "  Processed 185/864\n",
      "  Processed 190/864\n",
      "  Processed 195/864\n",
      "  Processed 200/864\n",
      "  Processed 205/864\n",
      "  Processed 210/864\n",
      "  Processed 215/864\n",
      "  Processed 220/864\n",
      "  Processed 225/864\n",
      "  Processed 230/864\n",
      "  Processed 235/864\n",
      "  Processed 240/864\n",
      "  Processed 245/864\n",
      "  Processed 250/864\n",
      "  Processed 255/864\n",
      "  Processed 260/864\n",
      "  Processed 265/864\n",
      "  Processed 270/864\n",
      "  Processed 275/864\n",
      "  Processed 280/864\n",
      "  Processed 285/864\n",
      "  Processed 290/864\n",
      "  Processed 295/864\n",
      "  Processed 300/864\n",
      "  Processed 305/864\n",
      "  Processed 310/864\n",
      "  Processed 315/864\n",
      "  Processed 320/864\n",
      "  Processed 325/864\n",
      "  Processed 330/864\n",
      "  Processed 335/864\n",
      "  Processed 340/864\n",
      "  Processed 345/864\n",
      "  Processed 350/864\n",
      "  Processed 355/864\n",
      "  Processed 360/864\n",
      "  Processed 365/864\n",
      "  Processed 370/864\n",
      "  Processed 375/864\n",
      "  Processed 380/864\n",
      "  Processed 385/864\n",
      "  Processed 390/864\n",
      "  Processed 395/864\n",
      "  Processed 400/864\n",
      "  Processed 405/864\n",
      "  Processed 410/864\n",
      "  Processed 415/864\n",
      "  Processed 420/864\n",
      "  Processed 425/864\n",
      "  Processed 430/864\n",
      "  Processed 435/864\n",
      "  Processed 440/864\n",
      "  Processed 445/864\n",
      "  Processed 450/864\n",
      "  Processed 455/864\n",
      "  Processed 460/864\n",
      "  Processed 465/864\n",
      "  Processed 470/864\n",
      "  Processed 475/864\n",
      "  Processed 480/864\n",
      "  Processed 485/864\n",
      "  Processed 490/864\n",
      "  Processed 495/864\n",
      "  Processed 500/864\n",
      "  Processed 505/864\n",
      "  Processed 510/864\n",
      "  Processed 515/864\n",
      "  Processed 520/864\n",
      "  Processed 525/864\n",
      "  Processed 530/864\n",
      "  Processed 535/864\n",
      "  Processed 540/864\n",
      "  Processed 545/864\n",
      "  Processed 550/864\n",
      "  Processed 555/864\n",
      "  Processed 560/864\n",
      "  Processed 565/864\n",
      "  Processed 570/864\n",
      "  Processed 575/864\n",
      "  Processed 580/864\n",
      "  Processed 585/864\n",
      "  Processed 590/864\n",
      "  Processed 595/864\n",
      "  Processed 600/864\n",
      "  Processed 605/864\n",
      "  Processed 610/864\n",
      "  Processed 615/864\n",
      "  Processed 620/864\n",
      "  Processed 625/864\n",
      "  Processed 630/864\n",
      "  Processed 635/864\n",
      "  Processed 640/864\n",
      "  Processed 645/864\n",
      "  Processed 650/864\n",
      "  Processed 655/864\n",
      "  Processed 660/864\n",
      "  Processed 665/864\n",
      "  Processed 670/864\n",
      "  Processed 675/864\n",
      "  Processed 680/864\n",
      "  Processed 685/864\n",
      "  Processed 690/864\n",
      "  Processed 695/864\n",
      "  Processed 700/864\n",
      "  Processed 705/864\n",
      "  Processed 710/864\n",
      "  Processed 715/864\n",
      "  Processed 720/864\n",
      "  Processed 725/864\n",
      "  Processed 730/864\n",
      "  Processed 735/864\n",
      "  Processed 740/864\n",
      "  Processed 745/864\n",
      "  Processed 750/864\n",
      "  Processed 755/864\n",
      "  Processed 760/864\n",
      "  Processed 765/864\n",
      "  Processed 770/864\n",
      "  Processed 775/864\n",
      "  Processed 780/864\n",
      "  Processed 785/864\n",
      "  Processed 790/864\n",
      "  Processed 795/864\n",
      "  Processed 800/864\n",
      "  Processed 805/864\n",
      "  Processed 810/864\n",
      "  Processed 815/864\n",
      "  Processed 820/864\n",
      "  Processed 825/864\n",
      "  Processed 830/864\n",
      "  Processed 835/864\n",
      "  Processed 840/864\n",
      "  Processed 845/864\n",
      "  Processed 850/864\n",
      "  Processed 855/864\n",
      "  Processed 860/864\n",
      "Embedded 864 entities\n"
     ]
    }
   ],
   "source": [
    "# Embed entities (name + type + description)\n",
    "cursor.execute(\"SELECT id, name, type, description FROM entities\")\n",
    "entities = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(entities)} entities...\")\n",
    "for i, (entity_id, name, etype, description) in enumerate(entities):\n",
    "    # Combine name, type, and description for richer embedding\n",
    "    text = f\"{name} ({etype}): {description or 'No description'}\"\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO entity_embeddings (entity_id, embedding) VALUES (?, ?)\",\n",
    "        (entity_id, serialize_embedding(embedding))\n",
    "    )\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(entities)}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(entities)} entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 359 chunks...\n",
      "  Processed chunk 1/359\n",
      "  Processed chunk 2/359\n",
      "  Processed chunk 3/359\n",
      "  Processed chunk 4/359\n",
      "  Processed chunk 5/359\n",
      "  Processed chunk 6/359\n",
      "  Processed chunk 7/359\n",
      "  Processed chunk 8/359\n",
      "  Processed chunk 9/359\n",
      "  Processed chunk 10/359\n",
      "  Processed chunk 11/359\n",
      "  Processed chunk 12/359\n",
      "  Processed chunk 13/359\n",
      "  Processed chunk 14/359\n",
      "  Processed chunk 15/359\n",
      "  Processed chunk 16/359\n",
      "  Processed chunk 17/359\n",
      "  Processed chunk 18/359\n",
      "  Processed chunk 19/359\n",
      "  Processed chunk 20/359\n",
      "  Processed chunk 21/359\n",
      "  Processed chunk 22/359\n",
      "  Processed chunk 23/359\n",
      "  Processed chunk 24/359\n",
      "  Processed chunk 25/359\n",
      "  Processed chunk 26/359\n",
      "  Processed chunk 27/359\n",
      "  Processed chunk 28/359\n",
      "  Processed chunk 29/359\n",
      "  Processed chunk 30/359\n",
      "  Processed chunk 31/359\n",
      "  Processed chunk 32/359\n",
      "  Processed chunk 33/359\n",
      "  Processed chunk 34/359\n",
      "  Processed chunk 35/359\n",
      "  Processed chunk 36/359\n",
      "  Processed chunk 37/359\n",
      "  Processed chunk 38/359\n",
      "  Processed chunk 39/359\n",
      "  Processed chunk 40/359\n",
      "  Processed chunk 41/359\n",
      "  Processed chunk 42/359\n",
      "  Processed chunk 43/359\n",
      "  Processed chunk 44/359\n",
      "  Processed chunk 45/359\n",
      "  Processed chunk 46/359\n",
      "  Processed chunk 47/359\n",
      "  Processed chunk 48/359\n",
      "  Processed chunk 49/359\n",
      "  Processed chunk 50/359\n",
      "  Processed chunk 51/359\n",
      "  Processed chunk 52/359\n",
      "  Processed chunk 53/359\n",
      "  Processed chunk 54/359\n",
      "  Processed chunk 55/359\n",
      "  Processed chunk 56/359\n",
      "  Processed chunk 57/359\n",
      "  Processed chunk 58/359\n",
      "  Processed chunk 59/359\n",
      "  Processed chunk 60/359\n",
      "  Processed chunk 61/359\n",
      "  Processed chunk 62/359\n",
      "  Processed chunk 63/359\n",
      "  Processed chunk 64/359\n",
      "  Processed chunk 65/359\n",
      "  Processed chunk 66/359\n",
      "  Processed chunk 67/359\n",
      "  Processed chunk 68/359\n",
      "  Processed chunk 69/359\n",
      "  Processed chunk 70/359\n",
      "  Processed chunk 71/359\n",
      "  Processed chunk 72/359\n",
      "  Processed chunk 73/359\n",
      "  Processed chunk 74/359\n",
      "  Processed chunk 75/359\n",
      "  Processed chunk 76/359\n",
      "  Processed chunk 77/359\n",
      "  Processed chunk 78/359\n",
      "  Processed chunk 79/359\n",
      "  Processed chunk 80/359\n",
      "  Processed chunk 81/359\n",
      "  Processed chunk 82/359\n",
      "  Processed chunk 83/359\n",
      "  Processed chunk 84/359\n",
      "  Processed chunk 85/359\n",
      "  Processed chunk 86/359\n",
      "  Processed chunk 87/359\n",
      "  Processed chunk 88/359\n",
      "  Processed chunk 89/359\n",
      "  Processed chunk 90/359\n",
      "  Processed chunk 91/359\n",
      "  Processed chunk 92/359\n",
      "  Processed chunk 93/359\n",
      "  Processed chunk 94/359\n",
      "  Processed chunk 95/359\n",
      "  Processed chunk 96/359\n",
      "  Processed chunk 97/359\n",
      "  Processed chunk 98/359\n",
      "  Processed chunk 99/359\n",
      "  Processed chunk 100/359\n",
      "  Processed chunk 101/359\n",
      "  Processed chunk 102/359\n",
      "  Processed chunk 103/359\n",
      "  Processed chunk 104/359\n",
      "  Processed chunk 105/359\n",
      "  Processed chunk 106/359\n",
      "  Processed chunk 107/359\n",
      "  Processed chunk 108/359\n",
      "  Processed chunk 109/359\n",
      "  Processed chunk 110/359\n",
      "  Processed chunk 111/359\n",
      "  Processed chunk 112/359\n",
      "  Processed chunk 113/359\n",
      "  Processed chunk 114/359\n",
      "  Processed chunk 115/359\n",
      "  Processed chunk 116/359\n",
      "  Processed chunk 117/359\n",
      "  Processed chunk 118/359\n",
      "  Processed chunk 119/359\n",
      "  Processed chunk 120/359\n",
      "  Processed chunk 121/359\n",
      "  Processed chunk 122/359\n",
      "  Processed chunk 123/359\n",
      "  Processed chunk 124/359\n",
      "  Processed chunk 125/359\n",
      "  Processed chunk 126/359\n",
      "  Processed chunk 127/359\n",
      "  Processed chunk 128/359\n",
      "  Processed chunk 129/359\n",
      "  Processed chunk 130/359\n",
      "  Processed chunk 131/359\n",
      "  Processed chunk 132/359\n",
      "  Processed chunk 133/359\n",
      "  Processed chunk 134/359\n",
      "  Processed chunk 135/359\n",
      "  Processed chunk 136/359\n",
      "  Processed chunk 137/359\n",
      "  Processed chunk 138/359\n",
      "  Processed chunk 139/359\n",
      "  Processed chunk 140/359\n",
      "  Processed chunk 141/359\n",
      "  Processed chunk 142/359\n",
      "  Processed chunk 143/359\n",
      "  Processed chunk 144/359\n",
      "  Processed chunk 145/359\n",
      "  Processed chunk 146/359\n",
      "  Processed chunk 147/359\n",
      "  Processed chunk 148/359\n",
      "  Processed chunk 149/359\n",
      "  Processed chunk 150/359\n",
      "  Processed chunk 151/359\n",
      "  Processed chunk 152/359\n",
      "  Processed chunk 153/359\n",
      "  Processed chunk 154/359\n",
      "  Processed chunk 155/359\n",
      "  Processed chunk 156/359\n",
      "  Processed chunk 157/359\n",
      "  Processed chunk 158/359\n",
      "  Processed chunk 159/359\n",
      "  Processed chunk 160/359\n",
      "  Processed chunk 161/359\n",
      "  Processed chunk 162/359\n",
      "  Processed chunk 163/359\n",
      "  Processed chunk 164/359\n",
      "  Processed chunk 165/359\n",
      "  Processed chunk 166/359\n",
      "  Processed chunk 167/359\n",
      "  Processed chunk 168/359\n",
      "  Processed chunk 169/359\n",
      "  Processed chunk 170/359\n",
      "  Processed chunk 171/359\n",
      "  Processed chunk 172/359\n",
      "  Processed chunk 173/359\n",
      "  Processed chunk 174/359\n",
      "  Processed chunk 175/359\n",
      "  Processed chunk 176/359\n",
      "  Processed chunk 177/359\n",
      "  Processed chunk 178/359\n",
      "  Processed chunk 179/359\n",
      "  Processed chunk 180/359\n",
      "  Processed chunk 181/359\n",
      "  Processed chunk 182/359\n",
      "  Processed chunk 183/359\n",
      "  Processed chunk 184/359\n",
      "  Processed chunk 185/359\n",
      "  Processed chunk 186/359\n",
      "  Processed chunk 187/359\n",
      "  Processed chunk 188/359\n",
      "  Processed chunk 189/359\n",
      "  Processed chunk 190/359\n",
      "  Processed chunk 191/359\n",
      "  Processed chunk 192/359\n",
      "  Processed chunk 193/359\n",
      "  Processed chunk 194/359\n",
      "  Processed chunk 195/359\n",
      "  Processed chunk 196/359\n",
      "  Processed chunk 197/359\n",
      "  Processed chunk 198/359\n",
      "  Processed chunk 199/359\n",
      "  Processed chunk 200/359\n",
      "  Processed chunk 201/359\n",
      "  Processed chunk 202/359\n",
      "  Processed chunk 203/359\n",
      "  Processed chunk 204/359\n",
      "  Processed chunk 205/359\n",
      "  Processed chunk 206/359\n",
      "  Processed chunk 207/359\n",
      "  Processed chunk 208/359\n",
      "  Processed chunk 209/359\n",
      "  Processed chunk 210/359\n",
      "  Processed chunk 211/359\n",
      "  Processed chunk 212/359\n",
      "  Processed chunk 213/359\n",
      "  Processed chunk 214/359\n",
      "  Processed chunk 215/359\n",
      "  Processed chunk 216/359\n",
      "  Processed chunk 217/359\n",
      "  Processed chunk 218/359\n",
      "  Processed chunk 219/359\n",
      "  Processed chunk 220/359\n",
      "  Processed chunk 221/359\n",
      "  Processed chunk 222/359\n",
      "  Processed chunk 223/359\n",
      "  Processed chunk 224/359\n",
      "  Processed chunk 225/359\n",
      "  Processed chunk 226/359\n",
      "  Processed chunk 227/359\n",
      "  Processed chunk 228/359\n",
      "  Processed chunk 229/359\n",
      "  Processed chunk 230/359\n",
      "  Processed chunk 231/359\n",
      "  Processed chunk 232/359\n",
      "  Processed chunk 233/359\n",
      "  Processed chunk 234/359\n",
      "  Processed chunk 235/359\n",
      "  Processed chunk 236/359\n",
      "  Processed chunk 237/359\n",
      "  Processed chunk 238/359\n",
      "  Processed chunk 239/359\n",
      "  Processed chunk 240/359\n",
      "  Processed chunk 241/359\n",
      "  Processed chunk 242/359\n",
      "  Processed chunk 243/359\n",
      "  Processed chunk 244/359\n",
      "  Processed chunk 245/359\n",
      "  Processed chunk 246/359\n",
      "  Processed chunk 247/359\n",
      "  Processed chunk 248/359\n",
      "  Processed chunk 249/359\n",
      "  Processed chunk 250/359\n",
      "  Processed chunk 251/359\n",
      "  Processed chunk 252/359\n",
      "  Processed chunk 253/359\n",
      "  Processed chunk 254/359\n",
      "  Processed chunk 255/359\n",
      "  Processed chunk 256/359\n",
      "  Processed chunk 257/359\n",
      "  Processed chunk 258/359\n",
      "  Processed chunk 259/359\n",
      "  Processed chunk 260/359\n",
      "  Processed chunk 261/359\n",
      "  Processed chunk 262/359\n",
      "  Processed chunk 263/359\n",
      "  Processed chunk 264/359\n",
      "  Processed chunk 265/359\n",
      "  Processed chunk 266/359\n",
      "  Processed chunk 267/359\n",
      "  Processed chunk 268/359\n",
      "  Processed chunk 269/359\n",
      "  Processed chunk 270/359\n",
      "  Processed chunk 271/359\n",
      "  Processed chunk 272/359\n",
      "  Processed chunk 273/359\n",
      "  Processed chunk 274/359\n",
      "  Processed chunk 275/359\n",
      "  Processed chunk 276/359\n",
      "  Processed chunk 277/359\n",
      "  Processed chunk 278/359\n",
      "  Processed chunk 279/359\n",
      "  Processed chunk 280/359\n",
      "  Processed chunk 281/359\n",
      "  Processed chunk 282/359\n",
      "  Processed chunk 283/359\n",
      "  Processed chunk 284/359\n",
      "  Processed chunk 285/359\n",
      "  Processed chunk 286/359\n",
      "  Processed chunk 287/359\n",
      "  Processed chunk 288/359\n",
      "  Processed chunk 289/359\n",
      "  Processed chunk 290/359\n",
      "  Processed chunk 291/359\n",
      "  Processed chunk 292/359\n",
      "  Processed chunk 293/359\n",
      "  Processed chunk 294/359\n",
      "  Processed chunk 295/359\n",
      "  Processed chunk 296/359\n",
      "  Processed chunk 297/359\n",
      "  Processed chunk 298/359\n",
      "  Processed chunk 299/359\n",
      "  Processed chunk 300/359\n",
      "  Processed chunk 301/359\n",
      "  Processed chunk 302/359\n",
      "  Processed chunk 303/359\n",
      "  Processed chunk 304/359\n",
      "  Processed chunk 305/359\n",
      "  Processed chunk 306/359\n",
      "  Processed chunk 307/359\n",
      "  Processed chunk 308/359\n",
      "  Processed chunk 309/359\n",
      "  Processed chunk 310/359\n",
      "  Processed chunk 311/359\n",
      "  Processed chunk 312/359\n",
      "  Processed chunk 313/359\n",
      "  Processed chunk 314/359\n",
      "  Processed chunk 315/359\n",
      "  Processed chunk 316/359\n",
      "  Processed chunk 317/359\n",
      "  Processed chunk 318/359\n",
      "  Processed chunk 319/359\n",
      "  Processed chunk 320/359\n",
      "  Processed chunk 321/359\n",
      "  Processed chunk 322/359\n",
      "  Processed chunk 323/359\n",
      "  Processed chunk 324/359\n",
      "  Processed chunk 325/359\n",
      "  Processed chunk 326/359\n",
      "  Processed chunk 327/359\n",
      "  Processed chunk 328/359\n",
      "  Processed chunk 329/359\n",
      "  Processed chunk 330/359\n",
      "  Processed chunk 331/359\n",
      "  Processed chunk 332/359\n",
      "  Processed chunk 333/359\n",
      "  Processed chunk 334/359\n",
      "  Processed chunk 335/359\n",
      "  Processed chunk 336/359\n",
      "  Processed chunk 337/359\n",
      "  Processed chunk 338/359\n",
      "  Processed chunk 339/359\n",
      "  Processed chunk 340/359\n",
      "  Processed chunk 341/359\n",
      "  Processed chunk 342/359\n",
      "  Processed chunk 343/359\n",
      "  Processed chunk 344/359\n",
      "  Processed chunk 345/359\n",
      "  Processed chunk 346/359\n",
      "  Processed chunk 347/359\n",
      "  Processed chunk 348/359\n",
      "  Processed chunk 349/359\n",
      "  Processed chunk 350/359\n",
      "  Processed chunk 351/359\n",
      "  Processed chunk 352/359\n",
      "  Processed chunk 353/359\n",
      "  Processed chunk 354/359\n",
      "  Processed chunk 355/359\n",
      "  Processed chunk 356/359\n",
      "  Processed chunk 357/359\n",
      "  Processed chunk 358/359\n",
      "  Processed chunk 359/359\n",
      "Embedded 359 chunks\n"
     ]
    }
   ],
   "source": [
    "# Embed chunks (source text)\n",
    "cursor.execute(\"SELECT id, content FROM chunks\")\n",
    "chunks = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(chunks)} chunks...\")\n",
    "for i, (chunk_id, content) in enumerate(chunks):\n",
    "    # Truncate if too long (embedding models have limits)\n",
    "    text = content[:8000] if len(content) > 8000 else content\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO chunk_embeddings (chunk_id, embedding) VALUES (?, ?)\",\n",
    "        (chunk_id, serialize_embedding(embedding))\n",
    "    )\n",
    "    \n",
    "    print(f\"  Processed chunk {i + 1}/{len(chunks)}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 5 claims...\n",
      "Embedded 5 claims\n"
     ]
    }
   ],
   "source": [
    "# Embed claims\n",
    "cursor.execute(\"SELECT id, claim_type, description FROM claims\")\n",
    "claims = cursor.fetchall()\n",
    "\n",
    "print(f\"Embedding {len(claims)} claims...\")\n",
    "for i, (claim_id, claim_type, description) in enumerate(claims):\n",
    "    text = f\"[{claim_type}] {description}\"\n",
    "    embedding = get_embedding(text)\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO claim_embeddings (claim_id, embedding) VALUES (?, ?)\",\n",
    "        (claim_id, serialize_embedding(embedding))\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Embedded {len(claims)} claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EMBEDDING COUNTS ===\n",
      "  entity_embeddings: 864\n",
      "  chunk_embeddings: 359\n",
      "  claim_embeddings: 5\n"
     ]
    }
   ],
   "source": [
    "# Verify embeddings\n",
    "print(\"\\n=== EMBEDDING COUNTS ===\")\n",
    "for table in [\"entity_embeddings\", \"chunk_embeddings\", \"claim_embeddings\"]:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"  {table}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Vector Search\n",
    "\n",
    "Basic semantic similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entities(query: str, top_k: int = 5) -> list[tuple]:\n",
    "    \"\"\"Search entities by semantic similarity.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            e.id,\n",
    "            e.name,\n",
    "            e.type,\n",
    "            e.description,\n",
    "            e.pagerank,\n",
    "            vec_distance_cosine(ee.embedding, ?) as distance\n",
    "        FROM entity_embeddings ee\n",
    "        JOIN entities e ON e.id = ee.entity_id\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (serialize_embedding(query_embedding), top_k))\n",
    "    \n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'artificial intelligence companies'\n",
      "\n",
      "=== TOP MATCHING ENTITIES ===\n",
      "\n",
      "[ORGANIZATION] LLMS\n",
      "  Similarity: 0.7051 | PageRank: 0.0009\n",
      "  Type of artificial intelligence model\n",
      "\n",
      "[CONCEPT] GENERAL PURPOSE LLMS\n",
      "  Similarity: 0.6843 | PageRank: 0.0009\n",
      "  Type of artificial intelligence models used for generating responses\n",
      "\n",
      "[ORGANIZATION] LARGE LANGUAGE MODELS\n",
      "  Similarity: 0.6380 | PageRank: 0.0012\n",
      "  Type of artificial intelligence model | Models that demonstrate exceptional lang...\n",
      "\n",
      "[CONCEPT] LLM-AGENTS\n",
      "  Similarity: 0.6313 | PageRank: 0.0010\n",
      "  General artificial intelligence models | Large Language Model agents used for ge...\n",
      "\n",
      "[ORGANIZATION] DEEPMIND\n",
      "  Similarity: 0.6286 | PageRank: 0.0009\n",
      "  AI research company\n"
     ]
    }
   ],
   "source": [
    "# Test entity search\n",
    "query = \"artificial intelligence companies\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "results = search_entities(query)\n",
    "print(\"=== TOP MATCHING ENTITIES ===\")\n",
    "for entity_id, name, etype, desc, pagerank, distance in results:\n",
    "    similarity = 1 - distance  # Convert distance to similarity\n",
    "    print(f\"\\n[{etype}] {name}\")\n",
    "    print(f\"  Similarity: {similarity:.4f} | PageRank: {pagerank:.4f}\")\n",
    "    print(f\"  {desc[:80]}...\" if desc and len(desc) > 80 else f\"  {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: 'CEO executives leadership'\n",
      "  0.790 | [PERSON] EXECUTIVES\n",
      "  0.627 | [ORGANIZATION] CELL GENOMICS\n",
      "  0.622 | [PERSON] ENTREPRENEURS\n",
      "\n",
      "==================================================\n",
      "Query: 'stock market financial'\n",
      "  0.530 | [ORGANIZATION] FEDERAL OPEN MARKET COMMITTEE\n",
      "  0.521 | [ORGANIZATION] FED\n",
      "  0.513 | [LOCATION] NEWTECH EXCHANGE\n",
      "\n",
      "==================================================\n",
      "Query: 'government regulation antitrust'\n",
      "  0.491 | [ORGANIZATION] NEB\n",
      "  0.491 | [CONCEPT] HIPAA PRIVACY RULE\n",
      "  0.484 | [ORGANIZATION] FED\n"
     ]
    }
   ],
   "source": [
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"CEO executives leadership\",\n",
    "    \"stock market financial\",\n",
    "    \"government regulation antitrust\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    results = search_entities(query, top_k=3)\n",
    "    for entity_id, name, etype, desc, pagerank, distance in results:\n",
    "        similarity = 1 - distance\n",
    "        print(f\"  {similarity:.3f} | [{etype}] {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Triple-Factor Retrieval\n",
    "\n",
    "Combine semantic similarity + temporal decay + graph centrality.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "final_score = 0.6 * semantic_similarity + 0.2 * temporal_score + 0.2 * graph_centrality\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    entity_id: int\n",
    "    name: str\n",
    "    entity_type: str\n",
    "    description: str\n",
    "    semantic_score: float\n",
    "    temporal_score: float\n",
    "    graph_score: float\n",
    "    final_score: float\n",
    "    community_id: int = None\n",
    "    source_refs: list[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.source_refs is None:\n",
    "            self.source_refs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal decay by content type:\n",
      "  Age   news (7d)   paper (30d)    ref (365d)\n",
      "---------------------------------------------\n",
      "    0      1.0000        1.0000        1.0000\n",
      "    1      0.9057        0.9772        0.9981\n",
      "    7      0.5000        0.8507        0.9868\n",
      "   14      0.2500        0.7236        0.9738\n",
      "   30      0.0513        0.5000        0.9446\n",
      "   90      0.0001        0.1250        0.8429\n",
      "  365      0.0000        0.0002        0.5000\n"
     ]
    }
   ],
   "source": [
    "HALF_LIVES = {\n",
    "    \"news\": 7,\n",
    "    \"research_paper\": 30,\n",
    "    \"reference\": 365,\n",
    "}\n",
    "\n",
    "def temporal_decay(age_days: float, half_life: float = 7.0) -> float:\n",
    "    \"\"\"Calculate temporal decay score using half-life formula.\n",
    "    \n",
    "    score = 0.5 ^ (age_days / half_life)\n",
    "    \n",
    "    Args:\n",
    "        age_days: Age of content in days\n",
    "        half_life: Days until relevance halves\n",
    "    \n",
    "    Returns:\n",
    "        Score between 0 and 1 (1 = fresh, 0 = very old)\n",
    "    \"\"\"\n",
    "    return 0.5 ** (age_days / half_life)\n",
    "\n",
    "\n",
    "def get_half_life_for_entity(entity_id: int) -> float:\n",
    "    \"\"\"Look up the content_type(s) of sources for an entity and return the appropriate half-life.\n",
    "    \n",
    "    If an entity appears in multiple sources with different content types,\n",
    "    use the longest half-life (most persistent content type wins).\n",
    "    \"\"\"\n",
    "    cursor.execute(\"SELECT source_refs FROM entities WHERE id = ?\", (entity_id,))\n",
    "    row = cursor.fetchone()\n",
    "    if not row or not row[0]:\n",
    "        return HALF_LIVES[\"news\"]  # Default\n",
    "    \n",
    "    try:\n",
    "        source_ids = json.loads(row[0])\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return HALF_LIVES[\"news\"]\n",
    "    \n",
    "    if not source_ids:\n",
    "        return HALF_LIVES[\"news\"]\n",
    "    \n",
    "    # Look up content_type for each source\n",
    "    max_half_life = HALF_LIVES[\"news\"]\n",
    "    for sid in source_ids:\n",
    "        cursor.execute(\"SELECT content_type FROM sources WHERE source_id = ?\", (sid,))\n",
    "        src_row = cursor.fetchone()\n",
    "        if src_row and src_row[0]:\n",
    "            hl = HALF_LIVES.get(src_row[0], HALF_LIVES[\"news\"])\n",
    "            max_half_life = max(max_half_life, hl)\n",
    "    \n",
    "    return max_half_life\n",
    "\n",
    "\n",
    "# Test temporal decay with content-type awareness\n",
    "print(\"Temporal decay by content type:\")\n",
    "print(f\"{'Age':>5}  {'news (7d)':>10}  {'paper (30d)':>12}  {'ref (365d)':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for days in [0, 1, 7, 14, 30, 90, 365]:\n",
    "    scores = [temporal_decay(days, HALF_LIVES[ct]) for ct in [\"news\", \"research_paper\", \"reference\"]]\n",
    "    print(f\"{days:>5}  {scores[0]:>10.4f}  {scores[1]:>12.4f}  {scores[2]:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_factor_search(\n",
    "    query: str,\n",
    "    top_k: int = 10,\n",
    "    semantic_weight: float = 0.6,\n",
    "    temporal_weight: float = 0.2,\n",
    "    graph_weight: float = 0.2,\n",
    "    content_age_days: float = 0.0,  # Assume fresh content for demo\n",
    ") -> list[RetrievalResult]:\n",
    "    \"\"\"Triple-factor retrieval combining semantic, temporal, and graph signals.\n",
    "    \n",
    "    Uses per-entity half-life based on source content_type:\n",
    "    - news: 7 days\n",
    "    - research_paper: 30 days\n",
    "    - reference: 365 days\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Fetch all entities with their embeddings and metrics\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            e.id,\n",
    "            e.name,\n",
    "            e.type,\n",
    "            e.description,\n",
    "            e.pagerank,\n",
    "            e.community_id,\n",
    "            e.source_refs,\n",
    "            vec_distance_cosine(ee.embedding, ?) as distance\n",
    "        FROM entity_embeddings ee\n",
    "        JOIN entities e ON e.id = ee.entity_id\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT ?\n",
    "    \"\"\", (serialize_embedding(query_embedding), top_k * 2))  # Fetch more for re-ranking\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Calculate triple-factor scores\n",
    "    results = []\n",
    "    \n",
    "    # Get max pagerank for normalization\n",
    "    max_pagerank = max(row[4] for row in rows) if rows else 1.0\n",
    "    \n",
    "    for entity_id, name, etype, desc, pagerank_val, community_id, source_refs_json, distance in rows:\n",
    "        # Semantic similarity (convert distance to similarity)\n",
    "        semantic_score = 1.0 - distance\n",
    "        \n",
    "        # Temporal score (per-entity half-life based on content type)\n",
    "        entity_half_life = get_half_life_for_entity(entity_id)\n",
    "        temporal_score = temporal_decay(content_age_days, entity_half_life)\n",
    "        \n",
    "        # Graph centrality score (normalized pagerank)\n",
    "        graph_score = pagerank_val / max_pagerank if max_pagerank > 0 else 0\n",
    "        \n",
    "        # Combined score\n",
    "        final_score = (\n",
    "            semantic_weight * semantic_score +\n",
    "            temporal_weight * temporal_score +\n",
    "            graph_weight * graph_score\n",
    "        )\n",
    "        \n",
    "        # Parse source refs\n",
    "        try:\n",
    "            source_refs = json.loads(source_refs_json) if source_refs_json else []\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            source_refs = []\n",
    "        \n",
    "        results.append(RetrievalResult(\n",
    "            entity_id=entity_id,\n",
    "            name=name,\n",
    "            entity_type=etype,\n",
    "            description=desc,\n",
    "            semantic_score=semantic_score,\n",
    "            temporal_score=temporal_score,\n",
    "            graph_score=graph_score,\n",
    "            final_score=final_score,\n",
    "            community_id=community_id,\n",
    "            source_refs=source_refs,\n",
    "        ))\n",
    "    \n",
    "    # Re-rank by final score\n",
    "    results.sort(key=lambda x: -x.final_score)\n",
    "    \n",
    "    return results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'technology partnerships and deals'\n",
      "\n",
      "=== TRIPLE-FACTOR RETRIEVAL RESULTS ===\n",
      "Rank  Entity                    Final    Semantic   Temporal   Graph   \n",
      "---------------------------------------------------------------------------\n",
      "1     CRISPR                    0.7307   0.5511     1.0000     1.0000\n",
      "2     NEOCHIP                   0.6569   0.6352     1.0000     0.3786\n",
      "3     AGENTIC AUTOMATION        0.6515   0.6262     1.0000     0.3786\n",
      "4     ABE                       0.6370   0.6022     1.0000     0.3786\n",
      "5     IEEE                      0.6104   0.5578     1.0000     0.3786\n"
     ]
    }
   ],
   "source": [
    "# Test triple-factor search\n",
    "query = \"technology partnerships and deals\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "results = triple_factor_search(query, top_k=5)\n",
    "\n",
    "print(\"=== TRIPLE-FACTOR RETRIEVAL RESULTS ===\")\n",
    "print(f\"{'Rank':<5} {'Entity':<25} {'Final':<8} {'Semantic':<10} {'Temporal':<10} {'Graph':<8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i:<5} {r.name[:24]:<25} {r.final_score:.4f}   {r.semantic_score:.4f}     {r.temporal_score:.4f}     {r.graph_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'AI regulation government'\n",
      "\n",
      "=== SEMANTIC ONLY (100% semantic) ===\n",
      "  1. [ORGANIZATION] ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (score: 0.5970)\n",
      "  2. [EVENT] VDA (score: 0.5796)\n",
      "  3. [EVENT] HEMOPHILIA (score: 0.5796)\n",
      "  4. [EVENT] CANCER (score: 0.5796)\n",
      "  5. [EVENT] INFECTIONS (score: 0.5796)\n",
      "\n",
      "=== TRIPLE-FACTOR (60/20/20) ===\n",
      "  1. []  (score: 0.7397, graph: 1.0000)\n",
      "  2. [EVENT] NEURODEGENERATIVE DISORDERS (score: 0.6201, graph: 0.3737)\n",
      "  3. [PERSON] ENTREPRENEURS (score: 0.5975, graph: 0.2842)\n",
      "  4. [ORGANIZATION] ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (score: 0.5792, graph: 0.1052)\n",
      "  5. [EVENT] VDA (score: 0.5688, graph: 0.1052)\n"
     ]
    }
   ],
   "source": [
    "# Compare with and without graph centrality boost\n",
    "query = \"AI regulation government\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "print(\"=== SEMANTIC ONLY (100% semantic) ===\")\n",
    "results_semantic = triple_factor_search(query, semantic_weight=1.0, temporal_weight=0.0, graph_weight=0.0)\n",
    "for i, r in enumerate(results_semantic[:5], 1):\n",
    "    print(f\"  {i}. [{r.entity_type}] {r.name} (score: {r.final_score:.4f})\")\n",
    "\n",
    "print(\"\\n=== TRIPLE-FACTOR (60/20/20) ===\")\n",
    "results_triple = triple_factor_search(query, semantic_weight=0.6, temporal_weight=0.2, graph_weight=0.2)\n",
    "for i, r in enumerate(results_triple[:5], 1):\n",
    "    print(f\"  {i}. [{r.entity_type}] {r.name} (score: {r.final_score:.4f}, graph: {r.graph_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Query Interface\n",
    "\n",
    "Build a simple RAG query function that retrieves context and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_community_context(community_id: int) -> str:\n",
    "    \"\"\"Get community summary for additional context.\"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT title, summary, key_insights \n",
    "        FROM community_summaries \n",
    "        WHERE community_id = ?\n",
    "    \"\"\", (community_id,))\n",
    "    row = cursor.fetchone()\n",
    "    if row:\n",
    "        title, summary, insights_json = row\n",
    "        insights = json.loads(insights_json) if insights_json else []\n",
    "        return f\"Topic: {title}\\nSummary: {summary}\\nKey insights: {'; '.join(insights)}\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_claims(entity_ids: list[int]) -> list[str]:\n",
    "    \"\"\"Get claims related to the retrieved entities.\"\"\"\n",
    "    if not entity_ids:\n",
    "        return []\n",
    "    \n",
    "    placeholders = \",\".join(\"?\" for _ in entity_ids)\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT c.claim_type, c.description, e.name\n",
    "        FROM claims c\n",
    "        JOIN entities e ON e.id = c.subject_id\n",
    "        WHERE c.subject_id IN ({placeholders})\n",
    "    \"\"\", entity_ids)\n",
    "    \n",
    "    claims = []\n",
    "    for claim_type, description, entity_name in cursor.fetchall():\n",
    "        claims.append(f\"[{claim_type}] {entity_name}: {description}\")\n",
    "    return claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigator_query(question: str, top_k: int = 5) -> str:\n",
    "    \"\"\"The Navigator: Answer questions using GraphRAG retrieval.\"\"\"\n",
    "    \n",
    "    # 1. Retrieve relevant entities using triple-factor search\n",
    "    results = triple_factor_search(question, top_k=top_k)\n",
    "    \n",
    "    if not results:\n",
    "        return \"I don't have enough information to answer that question.\"\n",
    "    \n",
    "    # 2. Build context from retrieved entities (with source provenance)\n",
    "    entity_context = []\n",
    "    for r in results:\n",
    "        sources_str = \"\"\n",
    "        if r.source_refs:\n",
    "            sources_str = f\" [from: {', '.join(r.source_refs)}]\"\n",
    "        entity_context.append(f\"- {r.name} ({r.entity_type}): {r.description}{sources_str}\")\n",
    "    \n",
    "    # 3. Get community context for the top result\n",
    "    community_context = \"\"\n",
    "    if results[0].community_id is not None:\n",
    "        community_context = get_community_context(results[0].community_id)\n",
    "    \n",
    "    # 4. Get related claims\n",
    "    entity_ids = [r.entity_id for r in results]\n",
    "    claims = get_related_claims(entity_ids)\n",
    "    claims_context = \"\\n\".join(claims[:5]) if claims else \"No specific claims.\"\n",
    "    \n",
    "    # 5. Collect unique source provenance\n",
    "    all_sources = set()\n",
    "    for r in results:\n",
    "        all_sources.update(r.source_refs)\n",
    "    source_provenance = f\"\\nSources consulted: {', '.join(sorted(all_sources))}\" if all_sources else \"\"\n",
    "    \n",
    "    # 6. Build prompt\n",
    "    prompt = f\"\"\"You are a knowledgeable assistant. Answer the question based on the following context.\n",
    "\n",
    "RELEVANT ENTITIES:\n",
    "{chr(10).join(entity_context)}\n",
    "\n",
    "TOPIC CONTEXT:\n",
    "{community_context or 'No additional topic context.'}\n",
    "\n",
    "SPECIFIC FACTS:\n",
    "{claims_context}\n",
    "{source_provenance}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide a clear, concise answer based on the context above. If the context doesn't contain enough information, say so. Reference the source domains when relevant.\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    # 7. Generate answer\n",
    "    answer = chat_ollama(prompt, temperature=0.3)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the partnership between OpenAI and Microsoft?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Answer:\n",
      "The provided context does not contain any information about a partnership between OpenAI and Microsoft. The context discusses CRISPR-Cas9 technology and its potential applications in treating various complex medical conditions such as cancer, cardiovascular diseases, infections, and neurodegenerative disorders. It also mentions the Unity March by Tribune Spotlight, which is associated with a conference called VDA (Event: Conference). There is no reference to OpenAI or Microsoft in this context.\n"
     ]
    }
   ],
   "source": [
    "# Test the Navigator\n",
    "question = \"What is the partnership between OpenAI and Microsoft?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=\" * 50)\n",
    "answer = navigator_query(question)\n",
    "print(f\"\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Q: What is the role of AI in scientific research?\n",
      "------------------------------------------------------------\n",
      "Sources: arxiv:2404.16130, arxiv:2404.18021\n",
      "A: Based on the provided context, there isn't specific information about the role of AI in scientific research. However, we can infer that AI is being used as a tool or technology in the community focused on 'BloombergGPT'. The context mentions that Ozan Irsoy has co-authored a paper titled 'BloombergGPT' with other researchers. This suggests that AI might be involved in aspects of this research, such as generating text for the paper or possibly in developing related technologies like BloombergGPT itself. But to provide a more definitive answer about the role of AI in scientific research, we would need additional context from sources relevant to this topic.\n",
      "\n",
      "============================================================\n",
      "Q: What large-scale observation instruments are used in science?\n",
      "------------------------------------------------------------\n",
      "Sources: arxiv:2404.16130, arxiv:2404.18021\n",
      "A: Based on the provided context, there is no mention of large-scale observation instruments being used in science. The context discusses Microsoft's study on the impact of large language models (like GPT-4) on scientific discovery and mentions specific products such as ASCAS12A (a Guide RNA (sgRNA)) and GUIDE SEQUENCES (components used in experiments). Therefore, I do not have enough information to answer the question about what large-scale observation instruments are used in science.\n",
      "\n",
      "============================================================\n",
      "Q: What are the economic impacts of emerging technologies?\n",
      "------------------------------------------------------------\n",
      "Sources: arxiv:2404.16130, arxiv:2404.18021\n",
      "A: Based solely on the provided context, there is not sufficient information to determine the economic impacts of emerging technologies such as GraphRAG and related advancements like NebulaGraph's graph RAG technology, Germline cell and embryo genome editing, AGENTIC AUTOMATION's CRISPR-GPT technology, or ABE (Adenine Base Editor) technology. The context primarily focuses on the technological developments and their applications within specific communities, such as those focused on GraphRAG and related technologies.\n",
      "\n",
      "For a comprehensive analysis of economic impacts, sources discussing market trends, investment opportunities, job creation, and potential challenges associated with these emerging technologies would be necessary. These factors are typically covered in research from economics journals, industry reports, or financial news outlets.\n",
      "\n",
      "============================================================\n",
      "Q: How is graph-based knowledge discovery used in information retrieval?\n",
      "------------------------------------------------------------\n",
      "Sources: arxiv:2404.16130\n",
      "A: Based on the provided context, there isn't specific information about how graph-based knowledge discovery is used in information retrieval (IR). The context discusses various studies and projects related to knowledge graphs but does not provide details on the application of these graphs within IR. Therefore, I cannot directly answer your question based solely on this context.\n",
      "\n",
      "However, it's worth noting that the context mentions several researchers who have worked on different aspects of knowledge graph extraction, evaluation, and construction. These works could potentially inform how graph-based knowledge discovery might be applied in information retrieval systems, but a direct connection is not established within the given information.\n",
      "\n",
      "For more specific insights into how graph-based knowledge discovery is used in IR, additional sources would need to be consulted that focus explicitly on these applications.\n"
     ]
    }
   ],
   "source": [
    "# Cross-domain test queries spanning multiple source domains\n",
    "questions = [\n",
    "    \"What is the role of AI in scientific research?\",\n",
    "    \"What large-scale observation instruments are used in science?\",\n",
    "    \"What are the economic impacts of emerging technologies?\",\n",
    "    \"How is graph-based knowledge discovery used in information retrieval?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Show which sources the retrieval pulls from\n",
    "    results = triple_factor_search(q, top_k=5)\n",
    "    source_domains = set()\n",
    "    for r in results:\n",
    "        source_domains.update(r.source_refs)\n",
    "    if source_domains:\n",
    "        print(f\"Sources: {', '.join(sorted(source_domains))}\")\n",
    "    \n",
    "    answer = navigator_query(q)\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"\\nDatabase connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed:\n",
    "\n",
    "1. **Embeddings** - Generated 768-dim vectors for entities, chunks, and claims\n",
    "2. **sqlite-vec** - Set up vector storage with cosine similarity search\n",
    "3. **Content-type-aware temporal decay** - Different half-lives for news (7d), papers (30d), reference (365d)\n",
    "4. **Triple-factor retrieval** - Combined semantic (60%) + temporal (20%) + graph (20%) with per-entity half-lives\n",
    "5. **Source provenance** - Navigator answers reference which source domains contributed\n",
    "6. **Cross-domain queries** - Tested retrieval spanning AI, biology, climate, astrophysics, neuroscience, economics, space\n",
    "\n",
    "## GraphRAG Multi-Source Pipeline Complete!\n",
    "\n",
    "You now have a working multi-source GraphRAG system with:\n",
    "- 7 diverse sources across 5+ domains\n",
    "- Entity/relationship/claim extraction with cross-document merge\n",
    "- Knowledge graph with community detection and interactive visualization\n",
    "- Semantic search with content-type-aware temporal decay and graph-aware ranking\n",
    "- Conversational query interface with source provenance\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "For the full DKIA system:\n",
    "1. **Source connectors** - Add RSS, HN, arXiv ingestion (automated)\n",
    "2. **FastAPI server** - Build the Navigator API\n",
    "3. **Cytoscape.js UI** - Production graph visualization\n",
    "4. **APScheduler** - Overnight pipeline automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKIA GraphRAG",
   "language": "python",
   "name": "dkia-graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
