{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Source Document Processing & Knowledge Extraction\n",
    "\n",
    "This notebook implements the core GraphRAG pipeline from Microsoft's paper [\"From Local to Global: A Graph RAG Approach\"](https://arxiv.org/abs/2404.16130).\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Define and fetch sources** - 7 documents from arXiv + web (with offline fallbacks)\n",
    "2. **Chunk all documents** - Split into 600-token chunks with 100-token overlap\n",
    "3. **Entity extraction** - LLM, GLiNER NER, or hybrid (configurable via `EXTRACTION_MODE`)\n",
    "4. **Relationship extraction** - LLM or co-occurrence (depends on extraction mode)\n",
    "5. **Claims extraction** - LLM-only (no NLP alternative exists)\n",
    "6. **Cross-document merge** - Deduplicate entities by exact name across all sources\n",
    "7. **Semantic grouping** - Group near-duplicate entities by embedding similarity (non-destructive overlay)\n",
    "\n",
    "## Extraction Modes\n",
    "- `\"llm\"` — All extraction via LLM (slowest, highest quality)\n",
    "- `\"nlp\"` — GLiNER NER + co-occurrence relationships, no claims (fastest)\n",
    "- `\"hybrid\"` — GLiNER NER + LLM relationships + LLM claims (recommended balance)\n",
    "\n",
    "## Models\n",
    "- **LLM**: `qwen2.5:3b` via Ollama (extraction, relationships, claims)\n",
    "- **NER**: `urchade/gliner_small-v2.1` via GLiNER (zero-shot, multilingual entity extraction in nlp/hybrid modes)\n",
    "- **Embeddings**: `nomic-embed-text` via Ollama (semantic grouping)\n",
    "\n",
    "## Sources\n",
    "- 4 arXiv papers (AI, biology, climate, astrophysics)\n",
    "- 3 web articles (neuroscience, economics, space exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import time\n",
    "import tempfile\n",
    "from typing import Any\n",
    "from datetime import datetime, timezone\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import arxiv\n",
    "import pymupdf\n",
    "import trafilatura\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "MODEL = \"qwen2.5:3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['nomic-embed-text:latest', 'qwen2.5:3b']\n"
     ]
    }
   ],
   "source": [
    "# Verify Ollama is running\n",
    "response = httpx.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Available models: {models}\")\n",
    "assert MODEL in models, f\"Model {MODEL} not found. Please run: ollama pull {MODEL}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define and Fetch Sources\n",
    "\n",
    "We fetch 7 documents from diverse domains to stress-test the extraction pipeline. Each source has hardcoded fallback text so the notebook runs offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Source limits (for debugging/testing) ---\n",
    "# Set to None to use all sources, or a number to limit.\n",
    "# Example: ARXIV_LIMIT=1, WEB_LIMIT=0 → only the first arXiv paper\n",
    "ARXIV_LIMIT = 1   # None = all 4, or 1/2/3\n",
    "WEB_LIMIT = 0     # None = all 3, or 0/1/2\n",
    "\n",
    "# --- Extraction mode ---\n",
    "# \"llm\":    All extraction via LLM (slowest, highest quality)\n",
    "# \"nlp\":    GLiNER NER + co-occurrence relationships, no claims (fastest)\n",
    "# \"hybrid\": GLiNER NER + LLM relationships + LLM claims (balanced)\n",
    "EXTRACTION_MODE = \"hybrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source limits: ARXIV_LIMIT=1, WEB_LIMIT=0\n",
      "  Using 1/4 arXiv sources, 0/3 web sources\n",
      "Extraction mode: hybrid\n",
      "\n",
      "Fetching arXiv sources (downloading full PDFs)...\n",
      "  [OK] arxiv:2404.16130: fetched full PDF from arXiv (89608 chars)\n",
      "\n",
      "Fetching web sources...\n",
      "\n",
      "============================================================\n",
      "SOURCES LOADED: 1\n",
      "============================================================\n",
      "  [arxiv] arxiv:2404.16130: From Local to Global: A Graph RAG Approach to Query-Focused \n",
      "         89608 chars | research_paper\n",
      "\n",
      "Total content: 89,608 characters across 1 sources\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SourceDocument:\n",
    "    source_id: str       # e.g. \"arxiv:2404.16130\"\n",
    "    source_type: str     # \"arxiv\" or \"web\"\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "    content_type: str    # \"research_paper\", \"news\", \"reference\"\n",
    "    fetched_at: str = \"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.fetched_at:\n",
    "            self.fetched_at = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "# --- Source configurations ---\n",
    "\n",
    "ARXIV_SOURCES = [\n",
    "    {\"id\": \"2404.16130\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"},\n",
    "    {\"id\": \"2404.18021\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments\"},\n",
    "    {\"id\": \"2312.14090\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"Climate Economics and Finance: A Review\"},\n",
    "    {\"id\": \"2304.04869\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"The JWST Mission: Astrophysics Enabled\"},\n",
    "]\n",
    "\n",
    "WEB_SOURCES = [\n",
    "    {\"url\": \"https://www.quantamagazine.org/memory-and-perception-are-intertwined-20240416/\",\n",
    "     \"source_id\": \"web:quanta-memory\", \"content_type\": \"news\",\n",
    "     \"title\": \"Memory and Perception Are Intertwined in the Brain\"},\n",
    "    {\"url\": \"https://www.imf.org/en/Blogs/Articles/2024/01/14/ai-will-transform-the-global-economy-lets-make-sure-it-benefits-humanity\",\n",
    "     \"source_id\": \"web:imf-ai-economy\", \"content_type\": \"news\",\n",
    "     \"title\": \"AI Will Transform the Global Economy\"},\n",
    "    {\"url\": \"https://www.planetary.org/space-missions/voyager\",\n",
    "     \"source_id\": \"web:planetary-voyager\", \"content_type\": \"reference\",\n",
    "     \"title\": \"Voyager: The Grand Tour of the Solar System\"},\n",
    "]\n",
    "\n",
    "# Apply source limits\n",
    "_arxiv_configs = ARXIV_SOURCES[:ARXIV_LIMIT] if ARXIV_LIMIT is not None else ARXIV_SOURCES\n",
    "_web_configs = WEB_SOURCES[:WEB_LIMIT] if WEB_LIMIT is not None else WEB_SOURCES\n",
    "\n",
    "print(f\"Source limits: ARXIV_LIMIT={ARXIV_LIMIT}, WEB_LIMIT={WEB_LIMIT}\")\n",
    "print(f\"  Using {len(_arxiv_configs)}/{len(ARXIV_SOURCES)} arXiv sources, {len(_web_configs)}/{len(WEB_SOURCES)} web sources\")\n",
    "print(f\"Extraction mode: {EXTRACTION_MODE}\")\n",
    "\n",
    "# --- Hardcoded fallback content (offline mode) ---\n",
    "\n",
    "FALLBACK_CONTENT = {\n",
    "    \"arxiv:2404.16130\": \"\"\"From Local to Global: A Graph RAG Approach to Query-Focused Summarization.\n",
    "The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as \"What are the main themes in the dataset?\", since this is inherently a query-focused summarization (QFS) task. Prior QFS methods fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pre-generate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are summarized into a final response. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a naive RAG baseline for both the comprehensiveness and diversity of generated answers.\"\"\",\n",
    "\n",
    "    \"arxiv:2404.18021\": \"\"\"CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.\n",
    "The integration of large language models (LLMs) with biological research holds significant promise for accelerating scientific discovery. CRISPR-GPT is a novel LLM agent that automates the design of CRISPR gene-editing experiments. It combines domain-specific knowledge of CRISPR biology with the reasoning capabilities of GPT-4 to guide researchers through the entire experimental workflow—from target gene selection and guide RNA design to delivery method optimization and off-target analysis. CRISPR-GPT integrates multiple bioinformatics tools, including sequence alignment algorithms, off-target prediction models, and primer design software, enabling end-to-end experimental planning. Evaluation across diverse gene-editing tasks demonstrates that CRISPR-GPT matches the performance of expert human researchers while significantly reducing experiment design time. The system handles multiple CRISPR platforms including Cas9, Cas12a, and base editors, adapting its recommendations based on the specific biological context. This work demonstrates the potential of AI-assisted scientific research and raises important questions about the responsible development of autonomous biological research agents.\"\"\",\n",
    "\n",
    "    \"arxiv:2312.14090\": \"\"\"Climate Economics and Finance: A Comprehensive Review.\n",
    "Climate change poses fundamental challenges to economic systems worldwide. This review synthesizes research at the intersection of climate science, economics, and finance, covering carbon pricing mechanisms, green bond markets, stranded asset risks, and the economic modeling of climate damages. Integrated assessment models (IAMs) such as DICE and FUND translate climate projections into economic impacts, estimating the social cost of carbon (SCC) at $50-200 per ton. Carbon markets like the EU Emissions Trading System (EU ETS) and California's cap-and-trade program demonstrate market-based approaches to emissions reduction. The green bond market has grown from $3 billion in 2012 to over $500 billion in 2023, financing renewable energy, energy efficiency, and sustainable infrastructure. Central banks, including the European Central Bank (ECB) and Bank of England, are conducting climate stress tests on financial institutions to assess systemic risk. Physical climate risks—extreme weather, sea-level rise, and agricultural disruption—threaten $43 trillion in global assets by 2100. Transition risks from policy changes and technological shifts could render fossil fuel reserves worth $1-4 trillion as stranded assets. This review identifies carbon border adjustment mechanisms, climate-related financial disclosure frameworks such as TCFD, and natural capital accounting as critical frontiers in climate economics.\"\"\",\n",
    "\n",
    "    \"arxiv:2304.04869\": \"\"\"The James Webb Space Telescope Mission: Scientific Capabilities and Early Results.\n",
    "The James Webb Space Telescope (JWST), launched in December 2021 on an Ariane 5 rocket, represents the most ambitious space observatory ever built. With its 6.5-meter primary mirror composed of 18 gold-coated beryllium segments, JWST operates at the second Lagrange point (L2), 1.5 million kilometers from Earth. The telescope carries four science instruments: NIRCam (Near-Infrared Camera), NIRSpec (Near-Infrared Spectrograph), MIRI (Mid-Infrared Instrument), and FGS/NIRISS (Fine Guidance Sensor/Near Infrared Imager and Slitless Spectrograph). Early science results have been transformative across multiple astrophysics domains. In exoplanet science, JWST detected carbon dioxide in the atmosphere of WASP-39b, a gas giant 700 light-years away. Deep field observations revealed galaxies forming just 300 million years after the Big Bang, challenging models of early galaxy formation. JWST has captured unprecedented images of the Carina Nebula, revealing star-forming regions previously hidden by dust. The telescope's sensitivity enables detailed spectroscopy of stellar nurseries, providing insights into the chemical composition of protoplanetary disks. The mission, led by NASA in partnership with ESA and CSA, has an expected operational lifetime of 10-20 years, far exceeding the original 5-year design goal.\"\"\",\n",
    "\n",
    "    \"web:quanta-memory\": \"\"\"Memory and Perception Are Deeply Intertwined in the Brain.\n",
    "Neuroscience research reveals that the boundary between memory and perception is far more blurred than previously believed. The hippocampus, long considered the brain's memory center, plays an active role in perception. Studies show that the hippocampus predicts what we are about to see based on past experience, creating an internal model that shapes conscious perception in real time. Researchers at University College London used fMRI to demonstrate that hippocampal activity precedes visual cortex activation during familiar scene recognition, suggesting the brain uses memory templates to \"pre-render\" expected visual information. This predictive coding framework has implications for understanding Alzheimer's disease, where the breakdown of hippocampal prediction circuits may explain why patients experience perceptual disturbances before overt memory loss. The prefrontal cortex mediates between memory-based predictions and incoming sensory data, resolving conflicts when reality differs from expectation. Similar predictive mechanisms have been found in the auditory system, where the hippocampus anticipates sounds in familiar sequences. These findings challenge the traditional cognitive architecture that separates perception, memory, and prediction into distinct modules, instead suggesting a unified system where the brain constantly generates and tests predictions about the world using stored knowledge.\"\"\",\n",
    "\n",
    "    \"web:imf-ai-economy\": \"\"\"AI Will Transform the Global Economy — Let's Make Sure It Benefits Humanity.\n",
    "The International Monetary Fund analysis indicates that artificial intelligence will affect approximately 40 percent of all jobs worldwide. In advanced economies, the exposure rises to 60 percent of jobs, where AI could either complement human workers—boosting productivity by 15-25 percent in affected sectors—or replace them entirely. The IMF proposes a comprehensive AI Preparedness Index measuring countries across digital infrastructure, human capital, labor market policies, and regulatory frameworks. Nordic countries, Singapore, and the United States score highest on AI readiness. Emerging markets and developing economies face a different challenge: while less immediately exposed to AI displacement (26 percent of jobs affected), they lack the infrastructure and skilled workforce to capture AI's productivity benefits, potentially widening the global inequality gap. The IMF recommends establishing social safety nets calibrated to the pace of AI adoption, investing in education programs that emphasize skills complementary to AI—such as critical thinking, creativity, and emotional intelligence—and creating international frameworks for AI governance. The analysis warns that without proactive policies, AI could increase income inequality within countries by up to 15 percentage points over the next decade, as high-skilled workers who effectively leverage AI tools see disproportionate wage gains.\"\"\",\n",
    "\n",
    "    \"web:planetary-voyager\": \"\"\"Voyager: The Grand Tour of the Solar System.\n",
    "The Voyager program, launched by NASA in 1977, represents one of humanity's greatest exploration achievements. Voyager 1 and Voyager 2 were designed to take advantage of a rare planetary alignment occurring once every 176 years, enabling gravity-assist flybys of the outer planets. Voyager 1 visited Jupiter in 1979 and Saturn in 1980, discovering active volcanoes on Jupiter's moon Io—the first found beyond Earth—and the complex structure of Saturn's rings. Voyager 2 is the only spacecraft to have visited Uranus (1986) and Neptune (1989), revealing Uranus's extreme axial tilt and Neptune's Great Dark Spot. The spacecraft carry the Golden Record, a 12-inch gold-plated copper disc containing sounds and images representing life and culture on Earth, curated by a team led by Carl Sagan. Voyager 1 entered interstellar space in August 2012, becoming the first human-made object to leave the heliosphere, at a distance of 121 astronomical units from the Sun. Voyager 2 followed in November 2018. Both spacecraft continue to transmit scientific data using their 23-watt radio transmitters—roughly the power of a refrigerator light bulb—communicating with NASA's Deep Space Network. As of 2024, Voyager 1 is over 24 billion kilometers from Earth, traveling at 17 kilometers per second. The mission, originally designed for 5 years, has operated for over 46 years, with power from their radioisotope thermoelectric generators expected to sustain limited operations until approximately 2030.\"\"\",\n",
    "}\n",
    "\n",
    "# --- PDF text extraction ---\n",
    "\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file using pymupdf.\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    text_parts = []\n",
    "    for page in doc:\n",
    "        text_parts.append(page.get_text())\n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "# --- Fetch functions ---\n",
    "\n",
    "def fetch_arxiv_sources(configs: list[dict]) -> list[SourceDocument]:\n",
    "    \"\"\"Fetch full paper content from arXiv PDFs. Falls back to abstract, then hardcoded content.\"\"\"\n",
    "    documents = []\n",
    "    for cfg in configs:\n",
    "        paper_id = cfg[\"id\"]\n",
    "        source_id = f\"arxiv:{paper_id}\"\n",
    "        fallback = FALLBACK_CONTENT.get(source_id, \"\")\n",
    "\n",
    "        try:\n",
    "            client = arxiv.Client()\n",
    "            search = arxiv.Search(id_list=[paper_id])\n",
    "            results = list(client.results(search))\n",
    "            if results:\n",
    "                paper = results[0]\n",
    "                title = paper.title\n",
    "                url = paper.entry_id\n",
    "\n",
    "                # Download and extract full PDF text\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    pdf_path = paper.download_pdf(dirpath=tmpdir)\n",
    "                    content = extract_pdf_text(pdf_path)\n",
    "\n",
    "                if len(content) < 500:\n",
    "                    raise ValueError(f\"PDF text too short ({len(content)} chars), falling back to abstract\")\n",
    "\n",
    "                print(f\"  [OK] {source_id}: fetched full PDF from arXiv ({len(content)} chars)\")\n",
    "            else:\n",
    "                raise ValueError(\"No results\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [FALLBACK] {source_id}: {e}\")\n",
    "            content = fallback\n",
    "            title = cfg.get(\"title\", paper_id)\n",
    "            url = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "\n",
    "        documents.append(SourceDocument(\n",
    "            source_id=source_id,\n",
    "            source_type=\"arxiv\",\n",
    "            title=title,\n",
    "            url=url,\n",
    "            content=content,\n",
    "            content_type=cfg[\"content_type\"],\n",
    "        ))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def fetch_web_sources(configs: list[dict]) -> list[SourceDocument]:\n",
    "    \"\"\"Fetch article text from web pages. Falls back to hardcoded content.\"\"\"\n",
    "    documents = []\n",
    "    for cfg in configs:\n",
    "        source_id = cfg[\"source_id\"]\n",
    "        fallback = FALLBACK_CONTENT.get(source_id, \"\")\n",
    "\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(cfg[\"url\"])\n",
    "            if downloaded:\n",
    "                content = trafilatura.extract(downloaded)\n",
    "                if content and len(content) > 200:\n",
    "                    print(f\"  [OK] {source_id}: fetched from web ({len(content)} chars)\")\n",
    "                else:\n",
    "                    raise ValueError(\"Extracted content too short\")\n",
    "            else:\n",
    "                raise ValueError(\"Download failed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [FALLBACK] {source_id}: {e}\")\n",
    "            content = fallback\n",
    "\n",
    "        documents.append(SourceDocument(\n",
    "            source_id=source_id,\n",
    "            source_type=\"web\",\n",
    "            title=cfg[\"title\"],\n",
    "            url=cfg[\"url\"],\n",
    "            content=content,\n",
    "            content_type=cfg[\"content_type\"],\n",
    "        ))\n",
    "    return documents\n",
    "\n",
    "\n",
    "# --- Fetch all sources ---\n",
    "print(f\"\\nFetching arXiv sources (downloading full PDFs)...\")\n",
    "arxiv_docs = fetch_arxiv_sources(_arxiv_configs)\n",
    "\n",
    "print(f\"\\nFetching web sources...\")\n",
    "web_docs = fetch_web_sources(_web_configs)\n",
    "\n",
    "all_documents = arxiv_docs + web_docs\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SOURCES LOADED: {len(all_documents)}\")\n",
    "print(f\"{'='*60}\")\n",
    "total_chars = 0\n",
    "for doc in all_documents:\n",
    "    print(f\"  [{doc.source_type}] {doc.source_id}: {doc.title[:60]}\")\n",
    "    print(f\"         {len(doc.content)} chars | {doc.content_type}\")\n",
    "    total_chars += len(doc.content)\n",
    "print(f\"\\nTotal content: {total_chars:,} characters across {len(all_documents)} sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Chunk All Documents\n",
    "\n",
    "Following GraphRAG methodology: ~600 tokens per chunk with 100 token overlap.\n",
    "Using character-based approximation (1 token ≈ 4 characters).\n",
    "Each document is chunked independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  arxiv:2404.16130: 193 chunks (89608 chars)\n",
      "\n",
      "========================================\n",
      "Total: 193 chunks across 1 sources\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG uses 600 tokens with 100 token overlap\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Chunk each document independently\n",
    "source_chunks: dict[str, list[str]] = {}\n",
    "total_chunks = 0\n",
    "\n",
    "for doc in all_documents:\n",
    "    doc_chunks = text_splitter.split_text(doc.content)\n",
    "    source_chunks[doc.source_id] = doc_chunks\n",
    "    total_chunks += len(doc_chunks)\n",
    "    print(f\"  {doc.source_id}: {len(doc_chunks)} chunks ({len(doc.content)} chars)\")\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Total: {total_chunks} chunks across {len(all_documents)} sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Ollama Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama test: Hello GraphRAG!\n",
      "Config: timeout=180.0s, max_retries=2\n"
     ]
    }
   ],
   "source": [
    "MAX_RETRIES = 2\n",
    "OLLAMA_TIMEOUT = 180.0  # seconds per request\n",
    "\n",
    "def chat_ollama(prompt: str, system: str = \"\", temperature: float = 0.0) -> str:\n",
    "    \"\"\"Send a chat request to Ollama with retry logic.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            response = httpx.post(\n",
    "                f\"{OLLAMA_BASE_URL}/api/chat\",\n",
    "                json={\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": temperature}\n",
    "                },\n",
    "                timeout=OLLAMA_TIMEOUT,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"message\"][\"content\"]\n",
    "        except (httpx.TimeoutException, httpx.HTTPStatusError) as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                wait = 2 * attempt\n",
    "                print(f\"[retry {attempt}/{MAX_RETRIES}: {type(e).__name__}, waiting {wait}s] \", end=\"\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test the connection\n",
    "test_response = chat_ollama(\"Say 'Hello GraphRAG!' and nothing else.\")\n",
    "print(f\"Ollama test: {test_response}\")\n",
    "print(f\"Config: timeout={OLLAMA_TIMEOUT}s, max_retries={MAX_RETRIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Entity, Relationship & Claims Extraction\n",
    "\n",
    "Extract named entities, relationships, and claims from each document.\n",
    "\n",
    "**Entity types:** PERSON, ORGANIZATION, LOCATION, EVENT, PRODUCT, DATE, MONEY, CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction functions defined: extract_entities, extract_relationships, extract_claims\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    name: str\n",
    "    type: str\n",
    "    description: str\n",
    "    source_chunk: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    source: str\n",
    "    target: str\n",
    "    description: str\n",
    "    strength: float = 1.0\n",
    "    source_chunk: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Claim:\n",
    "    subject: str\n",
    "    claim_type: str\n",
    "    description: str\n",
    "    date: str = \"\"\n",
    "    source_chunk: int = 0\n",
    "\n",
    "\n",
    "# --- Extraction prompts ---\n",
    "\n",
    "ENTITY_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting named entities from text.\n",
    "\n",
    "Extract all named entities from the following text. For each entity provide:\n",
    "1. name: The entity name (use UPPERCASE for consistency)\n",
    "2. type: One of [PERSON, ORGANIZATION, LOCATION, EVENT, PRODUCT, DATE, MONEY, CONCEPT]\n",
    "3. description: A brief description of the entity based on the text\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"name\": \"JOHN SMITH\", \"type\": \"PERSON\", \"description\": \"CEO of Example Corp who announced the merger\"}},\n",
    "  {{\"name\": \"EXAMPLE CORP\", \"type\": \"ORGANIZATION\", \"description\": \"Technology company acquiring StartupXYZ\"}}\n",
    "]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "RELATIONSHIP_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting relationships between entities.\n",
    "\n",
    "Given the following text and list of entities, extract all relationships between them.\n",
    "For each relationship provide:\n",
    "1. source: The source entity name (UPPERCASE)\n",
    "2. target: The target entity name (UPPERCASE)\n",
    "3. description: A description of how these entities are related\n",
    "4. strength: A score from 1-10 indicating relationship strength (10 = very strong)\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"source\": \"JOHN SMITH\", \"target\": \"EXAMPLE CORP\", \"description\": \"John Smith is the CEO of Example Corp\", \"strength\": 9}},\n",
    "  {{\"source\": \"EXAMPLE CORP\", \"target\": \"STARTUPXYZ\", \"description\": \"Example Corp is acquiring StartupXYZ\", \"strength\": 8}}\n",
    "]\n",
    "\n",
    "ENTITIES:\n",
    "{entities}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "CLAIMS_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting factual claims from text.\n",
    "\n",
    "Extract all specific factual claims from the following text. For each claim provide:\n",
    "1. subject: The entity the claim is about (UPPERCASE)\n",
    "2. claim_type: One of [FACT, EVENT, STATEMENT, METRIC, PREDICTION]\n",
    "3. description: The specific claim or fact\n",
    "4. date: Associated date/timeframe if mentioned (otherwise empty string)\n",
    "\n",
    "Focus on:\n",
    "- Numerical facts (prices, percentages, amounts)\n",
    "- Events (announcements, launches, decisions)\n",
    "- Quotes and statements by people\n",
    "- Predictions and forecasts\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"subject\": \"EXAMPLE CORP\", \"claim_type\": \"METRIC\", \"description\": \"Stock rose 15% in after-hours trading\", \"date\": \"2026-02-10\"}},\n",
    "  {{\"subject\": \"JOHN SMITH\", \"claim_type\": \"STATEMENT\", \"description\": \"Stated that the merger will create 1000 new jobs\", \"date\": \"\"}}\n",
    "]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- Extraction functions ---\n",
    "\n",
    "def _parse_llm_json(response: str) -> list:\n",
    "    \"\"\"Parse JSON from LLM response, handling markdown code blocks.\"\"\"\n",
    "    json_str = response.strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str.split(\"```\")[1]\n",
    "        if json_str.startswith(\"json\"):\n",
    "            json_str = json_str[4:]\n",
    "    json_str = json_str.strip()\n",
    "    return json.loads(json_str)\n",
    "\n",
    "\n",
    "def extract_entities(text: str, chunk_id: int = 0) -> list[Entity]:\n",
    "    \"\"\"Extract entities from a text chunk using the LLM.\"\"\"\n",
    "    prompt = ENTITY_EXTRACTION_PROMPT.format(text=text)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        entities_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Entity(\n",
    "                name=e.get(\"name\", \"\").upper(),\n",
    "                type=e.get(\"type\", \"UNKNOWN\"),\n",
    "                description=e.get(\"description\", \"\"),\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for e in entities_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[E parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_relationships(text: str, entities: list[Entity], chunk_id: int = 0) -> list[Relationship]:\n",
    "    \"\"\"Extract relationships between entities from a text chunk.\"\"\"\n",
    "    entity_list = \", \".join([e.name for e in entities])\n",
    "    prompt = RELATIONSHIP_EXTRACTION_PROMPT.format(text=text, entities=entity_list)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        rels_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Relationship(\n",
    "                source=r.get(\"source\", \"\").upper(),\n",
    "                target=r.get(\"target\", \"\").upper(),\n",
    "                description=r.get(\"description\", \"\"),\n",
    "                strength=float(r.get(\"strength\", 5)) / 10.0,\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for r in rels_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[R parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_claims(text: str, chunk_id: int = 0) -> list[Claim]:\n",
    "    \"\"\"Extract factual claims from a text chunk.\"\"\"\n",
    "    prompt = CLAIMS_EXTRACTION_PROMPT.format(text=text)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        claims_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Claim(\n",
    "                subject=c.get(\"subject\", \"\").upper(),\n",
    "                claim_type=c.get(\"claim_type\", \"FACT\"),\n",
    "                description=c.get(\"description\", \"\"),\n",
    "                date=c.get(\"date\", \"\"),\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for c in claims_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[C parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"Extraction functions defined: extract_entities, extract_relationships, extract_claims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Extraction: NLP + LLM\n",
    "\n",
    "The extraction pipeline supports three modes controlled by `EXTRACTION_MODE` (set in the source config cell above):\n",
    "\n",
    "| Mode | Entities | Relationships | Claims | Speed |\n",
    "|------|----------|---------------|--------|-------|\n",
    "| `\"llm\"` | LLM | LLM | LLM | ~18s/chunk |\n",
    "| `\"nlp\"` | GLiNER NER | Co-occurrence | Skipped | <0.5s/chunk |\n",
    "| `\"hybrid\"` | GLiNER NER | LLM | LLM | ~12s/chunk |\n",
    "\n",
    "**Why hybrid?** GLiNER is a zero-shot NER model -- entity types are specified at inference time using our exact project schema (no label mapping needed). It's multilingual by default (single model handles English, Spanish, and more), much faster than LLM entity extraction, and doesn't hallucinate entities. The LLM excels at relationships (understanding semantic connections) and claims (extracting factual statements) -- tasks that require deeper reasoning.\n",
    "\n",
    "**Model**: `urchade/gliner_small-v2.1` (~166 MB, runs on CPU). A single model handles all languages -- no per-language model switching needed.\n",
    "\n",
    "**Language detection**: `langdetect` is still used per document for metadata export, but doesn't affect the NER model routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NLP extraction helpers (GLiNER + language detection) ---\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from gliner import GLiNER\n",
    "    from langdetect import detect as _detect_lang\n",
    "    _NLP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    _NLP_AVAILABLE = False\n",
    "\n",
    "# GLiNER model (lazy-loaded, single multilingual model)\n",
    "_GLINER_MODEL = None\n",
    "GLINER_MODEL_NAME = \"urchade/gliner_small-v2.1\"\n",
    "GLINER_THRESHOLD = 0.3  # Confidence threshold for entity detection\n",
    "\n",
    "# Zero-shot labels matching our project entity types (lowercase for GLiNER)\n",
    "GLINER_LABELS = [\"person\", \"organization\", \"location\", \"event\", \"product\", \"date\", \"money\", \"concept\"]\n",
    "\n",
    "# GLiNER label → project entity type\n",
    "GLINER_TYPE_MAP = {\n",
    "    \"person\": \"PERSON\",\n",
    "    \"organization\": \"ORGANIZATION\",\n",
    "    \"location\": \"LOCATION\",\n",
    "    \"event\": \"EVENT\",\n",
    "    \"product\": \"PRODUCT\",\n",
    "    \"date\": \"DATE\",\n",
    "    \"money\": \"MONEY\",\n",
    "    \"concept\": \"CONCEPT\",\n",
    "}\n",
    "\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Detect language of text. Returns ISO 639-1 code ('en', 'es', etc.).\"\"\"\n",
    "    try:\n",
    "        return _detect_lang(text[:2000])\n",
    "    except Exception:\n",
    "        return \"en\"\n",
    "\n",
    "\n",
    "def get_gliner_model():\n",
    "    \"\"\"Lazy-load and cache the GLiNER model.\"\"\"\n",
    "    global _GLINER_MODEL\n",
    "    if _GLINER_MODEL is None:\n",
    "        print(f\"  [NLP] Loading {GLINER_MODEL_NAME}...\")\n",
    "        _GLINER_MODEL = GLiNER.from_pretrained(GLINER_MODEL_NAME)\n",
    "    return _GLINER_MODEL\n",
    "\n",
    "\n",
    "def _split_sentences(text: str) -> list[str]:\n",
    "    \"\"\"Simple sentence splitter for co-occurrence extraction.\"\"\"\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "\n",
    "\n",
    "def extract_entities_nlp(text: str, chunk_id: int = 0) -> list[Entity]:\n",
    "    \"\"\"Extract entities from a text chunk using GLiNER zero-shot NER.\"\"\"\n",
    "    model = get_gliner_model()\n",
    "    predictions = model.predict_entities(text, GLINER_LABELS, threshold=GLINER_THRESHOLD)\n",
    "\n",
    "    entities = []\n",
    "    seen: set[str] = set()\n",
    "\n",
    "    for pred in predictions:\n",
    "        name = pred[\"text\"].strip().upper()\n",
    "        if not name or len(name) < 2 or name in seen:\n",
    "            continue\n",
    "        seen.add(name)\n",
    "\n",
    "        entity_type = GLINER_TYPE_MAP.get(pred[\"label\"], \"CONCEPT\")\n",
    "\n",
    "        # Find sentence context for description\n",
    "        for sent in _split_sentences(text):\n",
    "            if pred[\"text\"] in sent:\n",
    "                description = sent[:200]\n",
    "                break\n",
    "        else:\n",
    "            description = f\"{entity_type} entity\"\n",
    "\n",
    "        entities.append(Entity(\n",
    "            name=name,\n",
    "            type=entity_type,\n",
    "            description=description,\n",
    "            source_chunk=chunk_id,\n",
    "        ))\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def extract_relationships_nlp(\n",
    "    text: str, entities: list[Entity], chunk_id: int = 0\n",
    ") -> list[Relationship]:\n",
    "    \"\"\"Extract co-occurrence relationships (entities appearing in the same sentence).\"\"\"\n",
    "    if len(entities) < 2:\n",
    "        return []\n",
    "\n",
    "    sentences = _split_sentences(text)\n",
    "    relationships = []\n",
    "    entity_names = {e.name for e in entities}\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_upper = sent.upper()\n",
    "        present = [n for n in entity_names if n in sent_upper]\n",
    "        for i, src in enumerate(present):\n",
    "            for tgt in present[i + 1:]:\n",
    "                relationships.append(Relationship(\n",
    "                    source=src,\n",
    "                    target=tgt,\n",
    "                    description=f\"Co-occur in: {sent[:150]}\",\n",
    "                    strength=0.5,\n",
    "                    source_chunk=chunk_id,\n",
    "                ))\n",
    "\n",
    "    return relationships\n",
    "\n",
    "\n",
    "# --- Validate mode and pre-load model ---\n",
    "\n",
    "assert EXTRACTION_MODE in (\"llm\", \"nlp\", \"hybrid\"), \\\n",
    "    f\"Invalid EXTRACTION_MODE='{EXTRACTION_MODE}'. Use 'llm', 'nlp', or 'hybrid'.\"\n",
    "\n",
    "if EXTRACTION_MODE in (\"nlp\", \"hybrid\"):\n",
    "    assert _NLP_AVAILABLE, (\n",
    "        \"GLiNER and langdetect required for nlp/hybrid mode.\\n\"\n",
    "        \"Install: pip install gliner langdetect\"\n",
    "    )\n",
    "    first_lang = detect_language(all_documents[0].content)\n",
    "    print(f\"NLP extraction enabled (mode={EXTRACTION_MODE})\")\n",
    "    print(f\"  First document language: {first_lang}\")\n",
    "    _ = get_gliner_model()\n",
    "    print(f\"  GLiNER model ready (threshold={GLINER_THRESHOLD})\")\n",
    "    print(f\"  Entity labels: {GLINER_LABELS}\")\n",
    "else:\n",
    "    print(f\"Extraction mode: {EXTRACTION_MODE} (pure LLM, no NLP dependencies needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Extract entities, relationships, and claims from all documents\n",
    "source_results: dict[str, dict] = {}\n",
    "global_chunk_offset = 0\n",
    "skipped_chunks: list[dict] = []  # track failures for diagnostics\n",
    "doc_languages: dict[str, str] = {}  # source_id -> detected language\n",
    "\n",
    "for doc_idx, doc in enumerate(all_documents):\n",
    "    doc_chunks = source_chunks[doc.source_id]\n",
    "    doc_entities: list[Entity] = []\n",
    "    doc_relationships: list[Relationship] = []\n",
    "    doc_claims: list[Claim] = []\n",
    "\n",
    "    # Detect document language (metadata; GLiNER is multilingual by default)\n",
    "    if EXTRACTION_MODE in (\"nlp\", \"hybrid\"):\n",
    "        lang = detect_language(doc.content)\n",
    "        doc_languages[doc.source_id] = lang\n",
    "    else:\n",
    "        lang = \"en\"\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{doc_idx+1}/{len(all_documents)}] {doc.source_id}: {doc.title[:50]}\")\n",
    "    print(f\"  {len(doc_chunks)} chunks | mode={EXTRACTION_MODE} | lang={lang}\")\n",
    "\n",
    "    for i, chunk in enumerate(doc_chunks):\n",
    "        chunk_id = global_chunk_offset + i\n",
    "        print(f\"  Chunk {i+1}/{len(doc_chunks)}: \", end=\"\")\n",
    "\n",
    "        try:\n",
    "            # --- Entity extraction ---\n",
    "            if EXTRACTION_MODE == \"llm\":\n",
    "                entities = extract_entities(chunk, chunk_id=chunk_id)\n",
    "                time.sleep(0.5)\n",
    "            else:  # nlp or hybrid\n",
    "                entities = extract_entities_nlp(chunk, chunk_id=chunk_id)\n",
    "            doc_entities.extend(entities)\n",
    "            print(f\"{len(entities)}E \", end=\"\")\n",
    "\n",
    "            # --- Relationship extraction ---\n",
    "            chunk_entities = [e for e in doc_entities if e.source_chunk == chunk_id]\n",
    "            if len(chunk_entities) >= 2:\n",
    "                if EXTRACTION_MODE == \"nlp\":\n",
    "                    relationships = extract_relationships_nlp(\n",
    "                        chunk, chunk_entities, chunk_id=chunk_id\n",
    "                    )\n",
    "                else:  # llm or hybrid (both use LLM for relationships)\n",
    "                    relationships = extract_relationships(\n",
    "                        chunk, chunk_entities, chunk_id=chunk_id\n",
    "                    )\n",
    "                    time.sleep(0.5)\n",
    "                doc_relationships.extend(relationships)\n",
    "                print(f\"{len(relationships)}R \", end=\"\")\n",
    "            else:\n",
    "                print(\"0R \", end=\"\")\n",
    "\n",
    "            # --- Claims extraction (LLM-only; skipped in pure NLP mode) ---\n",
    "            if EXTRACTION_MODE == \"nlp\":\n",
    "                print(\"--C\")\n",
    "            else:  # llm or hybrid\n",
    "                claims = extract_claims(chunk, chunk_id=chunk_id)\n",
    "                doc_claims.extend(claims)\n",
    "                print(f\"{len(claims)}C\")\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            err_type = type(e).__name__\n",
    "            print(f\"SKIPPED ({err_type}: {e})\")\n",
    "            skipped_chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"source_id\": doc.source_id,\n",
    "                \"chunk_index\": i,\n",
    "                \"error\": f\"{err_type}: {e}\",\n",
    "                \"chunk_len\": len(chunk),\n",
    "            })\n",
    "\n",
    "    global_chunk_offset += len(doc_chunks)\n",
    "\n",
    "    source_results[doc.source_id] = {\n",
    "        \"entities\": doc_entities,\n",
    "        \"relationships\": doc_relationships,\n",
    "        \"claims\": doc_claims,\n",
    "        \"chunks\": doc_chunks,\n",
    "    }\n",
    "\n",
    "    print(f\"  => {len(doc_entities)}E, {len(doc_relationships)}R, {len(doc_claims)}C\")\n",
    "\n",
    "# Grand totals\n",
    "total_e = sum(len(r[\"entities\"]) for r in source_results.values())\n",
    "total_r = sum(len(r[\"relationships\"]) for r in source_results.values())\n",
    "total_c = sum(len(r[\"claims\"]) for r in source_results.values())\n",
    "total_processed = sum(len(r[\"chunks\"]) for r in source_results.values())\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EXTRACTION COMPLETE (mode={EXTRACTION_MODE})\")\n",
    "print(f\"Total: {total_e} entities, {total_r} relationships, {total_c} claims\")\n",
    "print(f\"Chunks: {total_processed - len(skipped_chunks)} succeeded, {len(skipped_chunks)} skipped\")\n",
    "\n",
    "if skipped_chunks:\n",
    "    print(f\"\\n--- Skipped Chunks ---\")\n",
    "    for sc in skipped_chunks:\n",
    "        print(f\"  [{sc['source_id']}] chunk {sc['chunk_index']} ({sc['chunk_len']} chars): {sc['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PER-SOURCE EXTRACTION RESULTS ===\n",
      "\n",
      "Source                      Entities  Relations     Claims\n",
      "------------------------------------------------------------\n",
      "web:quanta-memory                  4          0          0\n",
      "------------------------------------------------------------\n",
      "TOTAL                              4          0          0\n"
     ]
    }
   ],
   "source": [
    "# Display per-source extraction summary\n",
    "print(\"=== PER-SOURCE EXTRACTION RESULTS ===\\n\")\n",
    "print(f\"{'Source':<25} {'Entities':>10} {'Relations':>10} {'Claims':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for source_id, result in source_results.items():\n",
    "    print(f\"{source_id:<25} {len(result['entities']):>10} {len(result['relationships']):>10} {len(result['claims']):>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'TOTAL':<25} {total_e:>10} {total_r:>10} {total_c:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PER-SOURCE DEDUPLICATION ===\n",
      "\n",
      "  web:quanta-memory: 4 raw -> 3 unique\n",
      "\n",
      "=== GLOBAL MERGE ===\n",
      "  Global unique entities: 3\n",
      "  Global relationships: 0\n",
      "  Global claims: 0\n",
      "\n",
      "=== CHUNK PROVENANCE ===\n",
      "  Entities with chunk tracking: 3\n",
      "  Entities in 2+ chunks: 1\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate entities by name (merge descriptions)\n",
    "def deduplicate_entities(entities: list[Entity]) -> list[Entity]:\n",
    "    \"\"\"Merge duplicate entities, combining their descriptions.\"\"\"\n",
    "    entity_map: dict[str, Entity] = {}\n",
    "    \n",
    "    for entity in entities:\n",
    "        key = entity.name\n",
    "        if key in entity_map:\n",
    "            existing = entity_map[key]\n",
    "            if entity.description and entity.description not in existing.description:\n",
    "                existing.description = f\"{existing.description} | {entity.description}\"\n",
    "        else:\n",
    "            entity_map[key] = Entity(\n",
    "                name=entity.name,\n",
    "                type=entity.type,\n",
    "                description=entity.description,\n",
    "                source_chunk=entity.source_chunk\n",
    "            )\n",
    "    \n",
    "    return list(entity_map.values())\n",
    "\n",
    "\n",
    "def merge_entities_across_sources(\n",
    "    source_results: dict[str, dict]\n",
    ") -> tuple[list[Entity], list[Relationship], list[Claim]]:\n",
    "    \"\"\"Deduplicate within each source, then merge across all sources.\n",
    "    \n",
    "    Returns global_entities, global_relationships, global_claims.\n",
    "    Also builds entity_source_map: entity_name -> list of source_ids.\n",
    "    \"\"\"\n",
    "    # Phase 1: Deduplicate within each source\n",
    "    for source_id, result in source_results.items():\n",
    "        result[\"unique_entities\"] = deduplicate_entities(result[\"entities\"])\n",
    "    \n",
    "    # Phase 2: Merge across all sources\n",
    "    global_entity_map: dict[str, Entity] = {}\n",
    "    entity_source_map: dict[str, list[str]] = {}\n",
    "    \n",
    "    for source_id, result in source_results.items():\n",
    "        for entity in result[\"unique_entities\"]:\n",
    "            key = entity.name\n",
    "            if key in global_entity_map:\n",
    "                existing = global_entity_map[key]\n",
    "                if entity.description and entity.description not in existing.description:\n",
    "                    existing.description = f\"{existing.description} | {entity.description}\"\n",
    "                entity_source_map[key].append(source_id)\n",
    "            else:\n",
    "                global_entity_map[key] = Entity(\n",
    "                    name=entity.name,\n",
    "                    type=entity.type,\n",
    "                    description=entity.description,\n",
    "                    source_chunk=entity.source_chunk\n",
    "                )\n",
    "                entity_source_map[key] = [source_id]\n",
    "    \n",
    "    global_entities = list(global_entity_map.values())\n",
    "    \n",
    "    # Merge all relationships (keep all, some may reference same entities)\n",
    "    global_relationships = []\n",
    "    for result in source_results.values():\n",
    "        global_relationships.extend(result[\"relationships\"])\n",
    "    \n",
    "    # Merge all claims\n",
    "    global_claims = []\n",
    "    for result in source_results.values():\n",
    "        global_claims.extend(result[\"claims\"])\n",
    "    \n",
    "    return global_entities, global_relationships, global_claims, entity_source_map\n",
    "\n",
    "\n",
    "# Run cross-document merge\n",
    "global_entities, global_relationships, global_claims, entity_source_map = merge_entities_across_sources(source_results)\n",
    "\n",
    "# Build entity -> chunk provenance mapping from raw extraction data.\n",
    "# Each entity was extracted from a specific chunk; this records ALL chunks\n",
    "# that produced each entity (before deduplication collapsed them).\n",
    "entity_chunk_map: dict[str, list[dict]] = {}\n",
    "for doc in all_documents:\n",
    "    result = source_results[doc.source_id]\n",
    "    for entity in result[\"entities\"]:\n",
    "        key = entity.name\n",
    "        if key not in entity_chunk_map:\n",
    "            entity_chunk_map[key] = []\n",
    "        entry = {\"chunk_index\": entity.source_chunk, \"source_id\": doc.source_id}\n",
    "        if entry not in entity_chunk_map[key]:\n",
    "            entity_chunk_map[key].append(entry)\n",
    "\n",
    "# Per-source dedup stats\n",
    "print(\"=== PER-SOURCE DEDUPLICATION ===\\n\")\n",
    "for source_id, result in source_results.items():\n",
    "    raw = len(result[\"entities\"])\n",
    "    unique = len(result[\"unique_entities\"])\n",
    "    print(f\"  {source_id}: {raw} raw -> {unique} unique\")\n",
    "\n",
    "print(f\"\\n=== GLOBAL MERGE ===\")\n",
    "print(f\"  Global unique entities: {len(global_entities)}\")\n",
    "print(f\"  Global relationships: {len(global_relationships)}\")\n",
    "print(f\"  Global claims: {len(global_claims)}\")\n",
    "\n",
    "# Chunk provenance stats\n",
    "entities_with_chunks = sum(1 for v in entity_chunk_map.values() if v)\n",
    "multi_chunk = sum(1 for v in entity_chunk_map.values() if len(v) > 1)\n",
    "print(f\"\\n=== CHUNK PROVENANCE ===\")\n",
    "print(f\"  Entities with chunk tracking: {entities_with_chunks}\")\n",
    "print(f\"  Entities in 2+ chunks: {multi_chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Semantic Entity Grouping\n",
    "\n",
    "Exact-name matching (Step 6) catches entities that appear identically across chunks, but misses near-duplicates — e.g., \"GRAPH RAG\" vs \"GRAPHRAG\", \"LLM\" vs \"LARGE LANGUAGE MODEL\", or abbreviation variants that the LLM extracts inconsistently.\n",
    "\n",
    "We embed each entity using `nomic-embed-text` (same model used for retrieval in notebook 03) and **group** (not merge) entities whose cosine similarity exceeds a configurable threshold. Union-Find handles transitive grouping (if A~B and B~C, all three belong to one group).\n",
    "\n",
    "**Key design**: Original entities are preserved intact. The semantic groups are a **separate overlay** — like quarks inside protons. Notebook 02 uses these groups for compound node visualization (member entities nested inside their semantic group).\n",
    "\n",
    "**Output**: `semantic_entity_groups` list + `entity_to_semantic_group` lookup map, exported alongside the original entities in `extraction_results.json`.\n",
    "\n",
    "**Tuning:** The top-25 most similar pairs are printed before grouping so you can adjust `SIMILARITY_THRESHOLD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 3 entities with nomic-embed-text...\n",
      "  Dimension: 768\n",
      "\n",
      "Computing cosine similarity matrix (3x3)...\n",
      "\n",
      "Top 3 most similar entity pairs:\n",
      "   Sim  Entity A                            Entity B                            Group?\n",
      "-------------------------------------------------------------------------------------\n",
      "0.8082  HIPPOCAMPUS                         PREFRONTAL CORTEX                   \n",
      "0.7304  HIPPOCAMPUS                         NEUROSCIENCE                        \n",
      "0.7207  NEUROSCIENCE                        PREFRONTAL CORTEX                   \n",
      "\n",
      "============================================================\n",
      "SEMANTIC GROUPING RESULTS (threshold=0.85)\n",
      "============================================================\n",
      "  Total entities:     3 (unchanged)\n",
      "  Semantic groups:    0\n",
      "  Grouped entities:   0\n",
      "  Ungrouped entities: 3\n",
      "\n",
      "  No groups found at threshold 0.85.\n",
      "  Review the top similar pairs table above — lower the threshold if you see near-duplicates.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "SIMILARITY_THRESHOLD = 0.85  # Cosine similarity; lower = more aggressive grouping\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def get_embeddings_batch(texts: list[str], batch_size: int = 50) -> list[list[float]]:\n",
    "    \"\"\"Get embeddings for multiple texts in batches via Ollama.\"\"\"\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        response = httpx.post(\n",
    "            f\"{OLLAMA_BASE_URL}/api/embed\",\n",
    "            json={\"model\": EMBED_MODEL, \"input\": batch},\n",
    "            timeout=120.0,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        all_embeddings.extend(response.json().get(\"embeddings\", []))\n",
    "        if len(texts) > batch_size:\n",
    "            print(f\"  Embedded {min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"Disjoint set for transitive entity grouping.\"\"\"\n",
    "    def __init__(self, n: int):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "\n",
    "    def find(self, x: int) -> int:\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "\n",
    "    def union(self, x: int, y: int):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "\n",
    "\n",
    "# --- Embed all entities ---\n",
    "n = len(global_entities)\n",
    "print(f\"Embedding {n} entities with {EMBED_MODEL}...\")\n",
    "entity_texts = [f\"{e.name} ({e.type}): {e.description[:200]}\" for e in global_entities]\n",
    "embeddings_raw = get_embeddings_batch(entity_texts)\n",
    "print(f\"  Dimension: {len(embeddings_raw[0])}\")\n",
    "\n",
    "# --- Cosine similarity matrix (numpy for speed) ---\n",
    "print(f\"\\nComputing cosine similarity matrix ({n}x{n})...\")\n",
    "emb_matrix = np.array(embeddings_raw, dtype=np.float32)\n",
    "norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True)\n",
    "norms[norms == 0] = 1.0\n",
    "emb_norm = emb_matrix / norms\n",
    "sim_matrix = emb_norm @ emb_norm.T\n",
    "\n",
    "# --- Preview: top similar pairs (for threshold tuning) ---\n",
    "upper_i, upper_j = np.triu_indices(n, k=1)\n",
    "upper_sims = sim_matrix[upper_i, upper_j]\n",
    "top_k = min(25, len(upper_sims))\n",
    "\n",
    "if top_k > 0:\n",
    "    top_idx = np.argsort(-upper_sims)[:top_k]\n",
    "\n",
    "    print(f\"\\nTop {top_k} most similar entity pairs:\")\n",
    "    print(f\"{'Sim':>6}  {'Entity A':<35} {'Entity B':<35} {'Group?'}\")\n",
    "    print(\"-\" * 85)\n",
    "    for idx in top_idx:\n",
    "        i, j = upper_i[idx], upper_j[idx]\n",
    "        sim = upper_sims[idx]\n",
    "        will_group = \"YES\" if sim >= SIMILARITY_THRESHOLD else \"\"\n",
    "        print(f\"{sim:.4f}  {global_entities[i].name[:35]:<35} {global_entities[j].name[:35]:<35} {will_group}\")\n",
    "else:\n",
    "    print(\"\\n  Only 1 entity — no pairs to compare.\")\n",
    "\n",
    "# --- Build groups via Union-Find ---\n",
    "merge_i, merge_j = np.where(np.triu(sim_matrix >= SIMILARITY_THRESHOLD, k=1))\n",
    "uf = UnionFind(n)\n",
    "for i, j in zip(merge_i, merge_j):\n",
    "    uf.union(int(i), int(j))\n",
    "\n",
    "raw_groups: dict[int, list[int]] = {}\n",
    "for i in range(n):\n",
    "    raw_groups.setdefault(uf.find(i), []).append(i)\n",
    "\n",
    "# Only groups with 2+ members (actual semantic clusters)\n",
    "multi_groups = {k: v for k, v in raw_groups.items() if len(v) > 1}\n",
    "\n",
    "# --- Build semantic_entity_groups (read-only overlay, does NOT modify globals) ---\n",
    "semantic_entity_groups: list[dict] = []\n",
    "entity_to_semantic_group: dict[str, int] = {}  # entity_name -> group_id\n",
    "\n",
    "for gid, group_indices in enumerate(\n",
    "    sorted(multi_groups.values(), key=lambda g: -len(g))\n",
    "):\n",
    "    group_ents = [global_entities[i] for i in group_indices]\n",
    "    # Canonical = entity with longest description\n",
    "    canonical = max(group_ents, key=lambda e: len(e.description))\n",
    "\n",
    "    members = [e.name for e in group_ents]\n",
    "    member_similarities = {}\n",
    "    for e in group_ents:\n",
    "        if e.name != canonical.name:\n",
    "            i_idx = next(i for i in group_indices if global_entities[i].name == e.name)\n",
    "            c_idx = next(i for i in group_indices if global_entities[i].name == canonical.name)\n",
    "            member_similarities[e.name] = round(float(sim_matrix[i_idx, c_idx]), 4)\n",
    "\n",
    "    semantic_entity_groups.append({\n",
    "        \"group_id\": gid,\n",
    "        \"canonical\": canonical.name,\n",
    "        \"members\": members,\n",
    "        \"member_similarities\": member_similarities,\n",
    "    })\n",
    "\n",
    "    for name in members:\n",
    "        entity_to_semantic_group[name] = gid\n",
    "\n",
    "# --- Results ---\n",
    "ungrouped = n - len(entity_to_semantic_group)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SEMANTIC GROUPING RESULTS (threshold={SIMILARITY_THRESHOLD})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Total entities:     {n} (unchanged)\")\n",
    "print(f\"  Semantic groups:    {len(semantic_entity_groups)}\")\n",
    "print(f\"  Grouped entities:   {len(entity_to_semantic_group)}\")\n",
    "print(f\"  Ungrouped entities: {ungrouped}\")\n",
    "\n",
    "if semantic_entity_groups:\n",
    "    print(f\"\\n--- Semantic Groups ---\")\n",
    "    for g in semantic_entity_groups:\n",
    "        print(f\"\\n  Group {g['group_id']}: {g['canonical']} ({len(g['members'])} members)\")\n",
    "        for m in g[\"members\"]:\n",
    "            if m == g[\"canonical\"]:\n",
    "                print(f\"    * {m} (canonical)\")\n",
    "            else:\n",
    "                print(f\"      {m} (sim={g['member_similarities'][m]:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n  No groups found at threshold {SIMILARITY_THRESHOLD}.\")\n",
    "    print(\"  Review the top similar pairs table above — lower the threshold if you see near-duplicates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Extraction Results\n",
    "\n",
    "Consolidate all extracted knowledge elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPHRAG MULTI-SOURCE EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Sources: 1\n",
      "Total content: 1,481 characters\n",
      "Total chunks: 4\n",
      "\n",
      "Global entities: 3\n",
      "Global relationships: 0\n",
      "Global claims: 0\n",
      "\n",
      "--- Entity Types ---\n",
      "  ORGANIZATION: 1\n",
      "  CONCEPT: 1\n",
      "  EVENT: 1\n",
      "\n",
      "--- Claim Types ---\n",
      "\n",
      "--- Cross-Source Entities (0 entities in 2+ sources) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GRAPHRAG MULTI-SOURCE EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSources: {len(all_documents)}\")\n",
    "print(f\"Total content: {total_chars:,} characters\")\n",
    "print(f\"Total chunks: {total_chunks}\")\n",
    "print(f\"\\nGlobal entities: {len(global_entities)}\")\n",
    "print(f\"Global relationships: {len(global_relationships)}\")\n",
    "print(f\"Global claims: {len(global_claims)}\")\n",
    "\n",
    "# Entity type breakdown\n",
    "print(\"\\n--- Entity Types ---\")\n",
    "type_counts: dict[str, int] = {}\n",
    "for e in global_entities:\n",
    "    type_counts[e.type] = type_counts.get(e.type, 0) + 1\n",
    "for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")\n",
    "\n",
    "# Claim type breakdown\n",
    "print(\"\\n--- Claim Types ---\")\n",
    "claim_type_counts: dict[str, int] = {}\n",
    "for c in global_claims:\n",
    "    claim_type_counts[c.claim_type] = claim_type_counts.get(c.claim_type, 0) + 1\n",
    "for t, count in sorted(claim_type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")\n",
    "\n",
    "# Cross-source entities\n",
    "multi_source = {k: v for k, v in entity_source_map.items() if len(v) > 1}\n",
    "print(f\"\\n--- Cross-Source Entities ({len(multi_source)} entities in 2+ sources) ---\")\n",
    "for name, sources in sorted(multi_source.items(), key=lambda x: -len(x[1])):\n",
    "    print(f\"  {name}: {sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook we will:\n",
    "1. **Build the knowledge graph** - Store entities and relationships from all 7 sources in a graph structure\n",
    "2. **Apply community detection** - Use Louvain algorithm to find cross-domain topic clusters\n",
    "3. **Generate community summaries** - Create hierarchical summaries for each cluster\n",
    "4. **Interactive visualization** - Explore the graph with ipycytoscape\n",
    "5. **Store in SQLite** - Persist the graph for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export extracted data in multi-document format\n",
    "extraction_results = {\n",
    "    \"extraction_mode\": EXTRACTION_MODE,\n",
    "    \"doc_languages\": doc_languages,\n",
    "    \"sources\": [],\n",
    "    \"merged\": {\n",
    "        \"entities\": [{\"name\": e.name, \"type\": e.type, \"description\": e.description} for e in global_entities],\n",
    "        \"relationships\": [{\"source\": r.source, \"target\": r.target, \"description\": r.description, \"strength\": r.strength} for r in global_relationships],\n",
    "        \"claims\": [{\"subject\": c.subject, \"claim_type\": c.claim_type, \"description\": c.description, \"date\": c.date} for c in global_claims],\n",
    "        \"entity_source_map\": {k: v for k, v in entity_source_map.items()},\n",
    "        \"entity_chunk_map\": entity_chunk_map,\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"total_sources\": len(all_documents),\n",
    "    },\n",
    "    \"semantic_entity_groups\": semantic_entity_groups,\n",
    "    \"entity_to_semantic_group\": entity_to_semantic_group,\n",
    "}\n",
    "\n",
    "# Add per-source data\n",
    "for doc in all_documents:\n",
    "    result = source_results[doc.source_id]\n",
    "    extraction_results[\"sources\"].append({\n",
    "        \"source_id\": doc.source_id,\n",
    "        \"source_type\": doc.source_type,\n",
    "        \"title\": doc.title,\n",
    "        \"url\": doc.url,\n",
    "        \"content_type\": doc.content_type,\n",
    "        \"content_length\": len(doc.content),\n",
    "        \"fetched_at\": doc.fetched_at,\n",
    "        \"language\": doc_languages.get(doc.source_id, \"en\"),\n",
    "        \"chunks\": result[\"chunks\"],\n",
    "        \"entities\": [{\"name\": e.name, \"type\": e.type, \"description\": e.description} for e in result.get(\"unique_entities\", result[\"entities\"])],\n",
    "        \"relationships\": [{\"source\": r.source, \"target\": r.target, \"description\": r.description, \"strength\": r.strength} for r in result[\"relationships\"]],\n",
    "        \"claims\": [{\"subject\": c.subject, \"claim_type\": c.claim_type, \"description\": c.description, \"date\": c.date} for c in result[\"claims\"]],\n",
    "    })\n",
    "\n",
    "with open(\"extraction_results.json\", \"w\") as f:\n",
    "    json.dump(extraction_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to extraction_results.json\")\n",
    "print(f\"  Extraction mode: {EXTRACTION_MODE}\")\n",
    "print(f\"  {len(extraction_results['sources'])} sources\")\n",
    "print(f\"  {len(extraction_results['merged']['entities'])} entities (unchanged)\")\n",
    "print(f\"  {len(extraction_results['merged']['relationships'])} relationships\")\n",
    "print(f\"  {len(extraction_results['merged']['claims'])} claims\")\n",
    "print(f\"  {len(extraction_results['merged']['entity_chunk_map'])} entities with chunk provenance\")\n",
    "print(f\"  {len(extraction_results['semantic_entity_groups'])} semantic groups\")\n",
    "print(f\"  {len(extraction_results['entity_to_semantic_group'])} entities in semantic groups\")\n",
    "if doc_languages:\n",
    "    print(f\"  Languages detected: {dict(doc_languages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CROSS-SOURCE ENTITY OVERLAP: 0 entities in 2+ sources\n",
      "============================================================\n",
      "\n",
      "  No cross-source entities found.\n",
      "  This is expected with diverse topics — entities are domain-specific.\n",
      "\n",
      "============================================================\n",
      "SOURCE PAIR OVERLAP (shared entity count)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cross-source entity overlap analysis\n",
    "multi_source = {k: v for k, v in entity_source_map.items() if len(v) > 1}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"CROSS-SOURCE ENTITY OVERLAP: {len(multi_source)} entities in 2+ sources\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if multi_source:\n",
    "    for name, sources in sorted(multi_source.items(), key=lambda x: -len(x[1])):\n",
    "        entity = next((e for e in global_entities if e.name == name), None)\n",
    "        etype = entity.type if entity else \"?\"\n",
    "        print(f\"\\n  [{etype}] {name} (in {len(sources)} sources)\")\n",
    "        for sid in sources:\n",
    "            doc = next((d for d in all_documents if d.source_id == sid), None)\n",
    "            if doc:\n",
    "                print(f\"    - {sid}: {doc.title[:50]}\")\n",
    "else:\n",
    "    print(\"\\n  No cross-source entities found.\")\n",
    "    print(\"  This is expected with diverse topics — entities are domain-specific.\")\n",
    "\n",
    "# Source overlap matrix\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SOURCE PAIR OVERLAP (shared entity count)\")\n",
    "print(\"=\"*60)\n",
    "source_ids = [doc.source_id for doc in all_documents]\n",
    "for i, s1 in enumerate(source_ids):\n",
    "    for s2 in source_ids[i+1:]:\n",
    "        shared = [name for name, srcs in entity_source_map.items()\n",
    "                  if s1 in srcs and s2 in srcs]\n",
    "        if shared:\n",
    "            print(f\"  {s1} <-> {s2}: {len(shared)} ({', '.join(shared[:5])}{'...' if len(shared) > 5 else ''})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKIA GraphRAG",
   "language": "python",
   "name": "dkia-graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
