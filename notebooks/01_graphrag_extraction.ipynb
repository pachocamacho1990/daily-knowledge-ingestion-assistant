{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Source Document Processing & Knowledge Extraction\n",
    "\n",
    "This notebook implements the core GraphRAG pipeline from Microsoft's paper [\"From Local to Global: A Graph RAG Approach\"](https://arxiv.org/abs/2404.16130).\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Define and fetch sources** - 7 documents from arXiv + web (with offline fallbacks)\n",
    "2. **Chunk all documents** - Split into 600-token chunks with 100-token overlap\n",
    "3. **Entity extraction** - Use LLM to identify named entities per document\n",
    "4. **Relationship extraction** - Extract connections between entities\n",
    "5. **Claims extraction** - Extract factual statements about entities\n",
    "6. **Cross-document merge** - Deduplicate and merge entities across all sources\n",
    "\n",
    "## Model\n",
    "Using locally deployed Ollama with `qwen2.5:3b`\n",
    "\n",
    "## Sources\n",
    "- 4 arXiv papers (AI, biology, climate, astrophysics)\n",
    "- 3 web articles (neuroscience, economics, space exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import time\n",
    "import tempfile\n",
    "from typing import Any\n",
    "from datetime import datetime, timezone\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import arxiv\n",
    "import pymupdf\n",
    "import trafilatura\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "MODEL = \"qwen2.5:3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['nomic-embed-text:latest', 'qwen2.5:3b']\n"
     ]
    }
   ],
   "source": [
    "# Verify Ollama is running\n",
    "response = httpx.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Available models: {models}\")\n",
    "assert MODEL in models, f\"Model {MODEL} not found. Please run: ollama pull {MODEL}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define and Fetch Sources\n",
    "\n",
    "We fetch 7 documents from diverse domains to stress-test the extraction pipeline. Each source has hardcoded fallback text so the notebook runs offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source limits: ARXIV_LIMIT=1, WEB_LIMIT=0\n",
      "  Using 1/4 arXiv sources, 0/3 web sources\n",
      "\n",
      "Fetching arXiv sources (downloading full PDFs)...\n",
      "  [OK] arxiv:2404.16130: fetched full PDF from arXiv (89608 chars)\n",
      "\n",
      "Fetching web sources...\n",
      "\n",
      "============================================================\n",
      "SOURCES LOADED: 1\n",
      "============================================================\n",
      "  [arxiv] arxiv:2404.16130: From Local to Global: A Graph RAG Approach to Query-Focused \n",
      "         89608 chars | research_paper\n",
      "\n",
      "Total content: 89,608 characters across 1 sources\n"
     ]
    }
   ],
   "source": [
    "# --- Source limits (for debugging/testing) ---\n",
    "# Set to None to use all sources, or a number to limit.\n",
    "# Example: ARXIV_LIMIT=1, WEB_LIMIT=0 → only the first arXiv paper\n",
    "ARXIV_LIMIT = 1   # None = all 4, or 1/2/3\n",
    "WEB_LIMIT = 0     # None = all 3, or 0/1/2\n",
    "\n",
    "@dataclass\n",
    "class SourceDocument:\n",
    "    source_id: str       # e.g. \"arxiv:2404.16130\"\n",
    "    source_type: str     # \"arxiv\" or \"web\"\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "    content_type: str    # \"research_paper\", \"news\", \"reference\"\n",
    "    fetched_at: str = \"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.fetched_at:\n",
    "            self.fetched_at = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "# --- Source configurations ---\n",
    "\n",
    "ARXIV_SOURCES = [\n",
    "    {\"id\": \"2404.16130\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"},\n",
    "    {\"id\": \"2404.18021\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments\"},\n",
    "    {\"id\": \"2312.14090\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"Climate Economics and Finance: A Review\"},\n",
    "    {\"id\": \"2304.04869\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"The JWST Mission: Astrophysics Enabled\"},\n",
    "]\n",
    "\n",
    "WEB_SOURCES = [\n",
    "    {\"url\": \"https://www.quantamagazine.org/memory-and-perception-are-intertwined-20240416/\",\n",
    "     \"source_id\": \"web:quanta-memory\", \"content_type\": \"news\",\n",
    "     \"title\": \"Memory and Perception Are Intertwined in the Brain\"},\n",
    "    {\"url\": \"https://www.imf.org/en/Blogs/Articles/2024/01/14/ai-will-transform-the-global-economy-lets-make-sure-it-benefits-humanity\",\n",
    "     \"source_id\": \"web:imf-ai-economy\", \"content_type\": \"news\",\n",
    "     \"title\": \"AI Will Transform the Global Economy\"},\n",
    "    {\"url\": \"https://www.planetary.org/space-missions/voyager\",\n",
    "     \"source_id\": \"web:planetary-voyager\", \"content_type\": \"reference\",\n",
    "     \"title\": \"Voyager: The Grand Tour of the Solar System\"},\n",
    "]\n",
    "\n",
    "# Apply source limits\n",
    "_arxiv_configs = ARXIV_SOURCES[:ARXIV_LIMIT] if ARXIV_LIMIT is not None else ARXIV_SOURCES\n",
    "_web_configs = WEB_SOURCES[:WEB_LIMIT] if WEB_LIMIT is not None else WEB_SOURCES\n",
    "\n",
    "print(f\"Source limits: ARXIV_LIMIT={ARXIV_LIMIT}, WEB_LIMIT={WEB_LIMIT}\")\n",
    "print(f\"  Using {len(_arxiv_configs)}/{len(ARXIV_SOURCES)} arXiv sources, {len(_web_configs)}/{len(WEB_SOURCES)} web sources\")\n",
    "\n",
    "# --- Hardcoded fallback content (offline mode) ---\n",
    "\n",
    "FALLBACK_CONTENT = {\n",
    "    \"arxiv:2404.16130\": \"\"\"From Local to Global: A Graph RAG Approach to Query-Focused Summarization.\n",
    "The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as \"What are the main themes in the dataset?\", since this is inherently a query-focused summarization (QFS) task. Prior QFS methods fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pre-generate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are summarized into a final response. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a naive RAG baseline for both the comprehensiveness and diversity of generated answers.\"\"\",\n",
    "\n",
    "    \"arxiv:2404.18021\": \"\"\"CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.\n",
    "The integration of large language models (LLMs) with biological research holds significant promise for accelerating scientific discovery. CRISPR-GPT is a novel LLM agent that automates the design of CRISPR gene-editing experiments. It combines domain-specific knowledge of CRISPR biology with the reasoning capabilities of GPT-4 to guide researchers through the entire experimental workflow—from target gene selection and guide RNA design to delivery method optimization and off-target analysis. CRISPR-GPT integrates multiple bioinformatics tools, including sequence alignment algorithms, off-target prediction models, and primer design software, enabling end-to-end experimental planning. Evaluation across diverse gene-editing tasks demonstrates that CRISPR-GPT matches the performance of expert human researchers while significantly reducing experiment design time. The system handles multiple CRISPR platforms including Cas9, Cas12a, and base editors, adapting its recommendations based on the specific biological context. This work demonstrates the potential of AI-assisted scientific research and raises important questions about the responsible development of autonomous biological research agents.\"\"\",\n",
    "\n",
    "    \"arxiv:2312.14090\": \"\"\"Climate Economics and Finance: A Comprehensive Review.\n",
    "Climate change poses fundamental challenges to economic systems worldwide. This review synthesizes research at the intersection of climate science, economics, and finance, covering carbon pricing mechanisms, green bond markets, stranded asset risks, and the economic modeling of climate damages. Integrated assessment models (IAMs) such as DICE and FUND translate climate projections into economic impacts, estimating the social cost of carbon (SCC) at $50-200 per ton. Carbon markets like the EU Emissions Trading System (EU ETS) and California's cap-and-trade program demonstrate market-based approaches to emissions reduction. The green bond market has grown from $3 billion in 2012 to over $500 billion in 2023, financing renewable energy, energy efficiency, and sustainable infrastructure. Central banks, including the European Central Bank (ECB) and Bank of England, are conducting climate stress tests on financial institutions to assess systemic risk. Physical climate risks—extreme weather, sea-level rise, and agricultural disruption—threaten $43 trillion in global assets by 2100. Transition risks from policy changes and technological shifts could render fossil fuel reserves worth $1-4 trillion as stranded assets. This review identifies carbon border adjustment mechanisms, climate-related financial disclosure frameworks such as TCFD, and natural capital accounting as critical frontiers in climate economics.\"\"\",\n",
    "\n",
    "    \"arxiv:2304.04869\": \"\"\"The James Webb Space Telescope Mission: Scientific Capabilities and Early Results.\n",
    "The James Webb Space Telescope (JWST), launched in December 2021 on an Ariane 5 rocket, represents the most ambitious space observatory ever built. With its 6.5-meter primary mirror composed of 18 gold-coated beryllium segments, JWST operates at the second Lagrange point (L2), 1.5 million kilometers from Earth. The telescope carries four science instruments: NIRCam (Near-Infrared Camera), NIRSpec (Near-Infrared Spectrograph), MIRI (Mid-Infrared Instrument), and FGS/NIRISS (Fine Guidance Sensor/Near Infrared Imager and Slitless Spectrograph). Early science results have been transformative across multiple astrophysics domains. In exoplanet science, JWST detected carbon dioxide in the atmosphere of WASP-39b, a gas giant 700 light-years away. Deep field observations revealed galaxies forming just 300 million years after the Big Bang, challenging models of early galaxy formation. JWST has captured unprecedented images of the Carina Nebula, revealing star-forming regions previously hidden by dust. The telescope's sensitivity enables detailed spectroscopy of stellar nurseries, providing insights into the chemical composition of protoplanetary disks. The mission, led by NASA in partnership with ESA and CSA, has an expected operational lifetime of 10-20 years, far exceeding the original 5-year design goal.\"\"\",\n",
    "\n",
    "    \"web:quanta-memory\": \"\"\"Memory and Perception Are Deeply Intertwined in the Brain.\n",
    "Neuroscience research reveals that the boundary between memory and perception is far more blurred than previously believed. The hippocampus, long considered the brain's memory center, plays an active role in perception. Studies show that the hippocampus predicts what we are about to see based on past experience, creating an internal model that shapes conscious perception in real time. Researchers at University College London used fMRI to demonstrate that hippocampal activity precedes visual cortex activation during familiar scene recognition, suggesting the brain uses memory templates to \"pre-render\" expected visual information. This predictive coding framework has implications for understanding Alzheimer's disease, where the breakdown of hippocampal prediction circuits may explain why patients experience perceptual disturbances before overt memory loss. The prefrontal cortex mediates between memory-based predictions and incoming sensory data, resolving conflicts when reality differs from expectation. Similar predictive mechanisms have been found in the auditory system, where the hippocampus anticipates sounds in familiar sequences. These findings challenge the traditional cognitive architecture that separates perception, memory, and prediction into distinct modules, instead suggesting a unified system where the brain constantly generates and tests predictions about the world using stored knowledge.\"\"\",\n",
    "\n",
    "    \"web:imf-ai-economy\": \"\"\"AI Will Transform the Global Economy — Let's Make Sure It Benefits Humanity.\n",
    "The International Monetary Fund analysis indicates that artificial intelligence will affect approximately 40 percent of all jobs worldwide. In advanced economies, the exposure rises to 60 percent of jobs, where AI could either complement human workers—boosting productivity by 15-25 percent in affected sectors—or replace them entirely. The IMF proposes a comprehensive AI Preparedness Index measuring countries across digital infrastructure, human capital, labor market policies, and regulatory frameworks. Nordic countries, Singapore, and the United States score highest on AI readiness. Emerging markets and developing economies face a different challenge: while less immediately exposed to AI displacement (26 percent of jobs affected), they lack the infrastructure and skilled workforce to capture AI's productivity benefits, potentially widening the global inequality gap. The IMF recommends establishing social safety nets calibrated to the pace of AI adoption, investing in education programs that emphasize skills complementary to AI—such as critical thinking, creativity, and emotional intelligence—and creating international frameworks for AI governance. The analysis warns that without proactive policies, AI could increase income inequality within countries by up to 15 percentage points over the next decade, as high-skilled workers who effectively leverage AI tools see disproportionate wage gains.\"\"\",\n",
    "\n",
    "    \"web:planetary-voyager\": \"\"\"Voyager: The Grand Tour of the Solar System.\n",
    "The Voyager program, launched by NASA in 1977, represents one of humanity's greatest exploration achievements. Voyager 1 and Voyager 2 were designed to take advantage of a rare planetary alignment occurring once every 176 years, enabling gravity-assist flybys of the outer planets. Voyager 1 visited Jupiter in 1979 and Saturn in 1980, discovering active volcanoes on Jupiter's moon Io—the first found beyond Earth—and the complex structure of Saturn's rings. Voyager 2 is the only spacecraft to have visited Uranus (1986) and Neptune (1989), revealing Uranus's extreme axial tilt and Neptune's Great Dark Spot. The spacecraft carry the Golden Record, a 12-inch gold-plated copper disc containing sounds and images representing life and culture on Earth, curated by a team led by Carl Sagan. Voyager 1 entered interstellar space in August 2012, becoming the first human-made object to leave the heliosphere, at a distance of 121 astronomical units from the Sun. Voyager 2 followed in November 2018. Both spacecraft continue to transmit scientific data using their 23-watt radio transmitters—roughly the power of a refrigerator light bulb—communicating with NASA's Deep Space Network. As of 2024, Voyager 1 is over 24 billion kilometers from Earth, traveling at 17 kilometers per second. The mission, originally designed for 5 years, has operated for over 46 years, with power from their radioisotope thermoelectric generators expected to sustain limited operations until approximately 2030.\"\"\",\n",
    "}\n",
    "\n",
    "# --- PDF text extraction ---\n",
    "\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file using pymupdf.\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    text_parts = []\n",
    "    for page in doc:\n",
    "        text_parts.append(page.get_text())\n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "# --- Fetch functions ---\n",
    "\n",
    "def fetch_arxiv_sources(configs: list[dict]) -> list[SourceDocument]:\n",
    "    \"\"\"Fetch full paper content from arXiv PDFs. Falls back to abstract, then hardcoded content.\"\"\"\n",
    "    documents = []\n",
    "    for cfg in configs:\n",
    "        paper_id = cfg[\"id\"]\n",
    "        source_id = f\"arxiv:{paper_id}\"\n",
    "        fallback = FALLBACK_CONTENT.get(source_id, \"\")\n",
    "\n",
    "        try:\n",
    "            client = arxiv.Client()\n",
    "            search = arxiv.Search(id_list=[paper_id])\n",
    "            results = list(client.results(search))\n",
    "            if results:\n",
    "                paper = results[0]\n",
    "                title = paper.title\n",
    "                url = paper.entry_id\n",
    "\n",
    "                # Download and extract full PDF text\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    pdf_path = paper.download_pdf(dirpath=tmpdir)\n",
    "                    content = extract_pdf_text(pdf_path)\n",
    "\n",
    "                if len(content) < 500:\n",
    "                    raise ValueError(f\"PDF text too short ({len(content)} chars), falling back to abstract\")\n",
    "\n",
    "                print(f\"  [OK] {source_id}: fetched full PDF from arXiv ({len(content)} chars)\")\n",
    "            else:\n",
    "                raise ValueError(\"No results\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [FALLBACK] {source_id}: {e}\")\n",
    "            content = fallback\n",
    "            title = cfg.get(\"title\", paper_id)\n",
    "            url = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "\n",
    "        documents.append(SourceDocument(\n",
    "            source_id=source_id,\n",
    "            source_type=\"arxiv\",\n",
    "            title=title,\n",
    "            url=url,\n",
    "            content=content,\n",
    "            content_type=cfg[\"content_type\"],\n",
    "        ))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def fetch_web_sources(configs: list[dict]) -> list[SourceDocument]:\n",
    "    \"\"\"Fetch article text from web pages. Falls back to hardcoded content.\"\"\"\n",
    "    documents = []\n",
    "    for cfg in configs:\n",
    "        source_id = cfg[\"source_id\"]\n",
    "        fallback = FALLBACK_CONTENT.get(source_id, \"\")\n",
    "\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(cfg[\"url\"])\n",
    "            if downloaded:\n",
    "                content = trafilatura.extract(downloaded)\n",
    "                if content and len(content) > 200:\n",
    "                    print(f\"  [OK] {source_id}: fetched from web ({len(content)} chars)\")\n",
    "                else:\n",
    "                    raise ValueError(\"Extracted content too short\")\n",
    "            else:\n",
    "                raise ValueError(\"Download failed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [FALLBACK] {source_id}: {e}\")\n",
    "            content = fallback\n",
    "\n",
    "        documents.append(SourceDocument(\n",
    "            source_id=source_id,\n",
    "            source_type=\"web\",\n",
    "            title=cfg[\"title\"],\n",
    "            url=cfg[\"url\"],\n",
    "            content=content,\n",
    "            content_type=cfg[\"content_type\"],\n",
    "        ))\n",
    "    return documents\n",
    "\n",
    "\n",
    "# --- Fetch all sources ---\n",
    "print(f\"\\nFetching arXiv sources (downloading full PDFs)...\")\n",
    "arxiv_docs = fetch_arxiv_sources(_arxiv_configs)\n",
    "\n",
    "print(f\"\\nFetching web sources...\")\n",
    "web_docs = fetch_web_sources(_web_configs)\n",
    "\n",
    "all_documents = arxiv_docs + web_docs\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SOURCES LOADED: {len(all_documents)}\")\n",
    "print(f\"{'='*60}\")\n",
    "total_chars = 0\n",
    "for doc in all_documents:\n",
    "    print(f\"  [{doc.source_type}] {doc.source_id}: {doc.title[:60]}\")\n",
    "    print(f\"         {len(doc.content)} chars | {doc.content_type}\")\n",
    "    total_chars += len(doc.content)\n",
    "print(f\"\\nTotal content: {total_chars:,} characters across {len(all_documents)} sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Chunk All Documents\n",
    "\n",
    "Following GraphRAG methodology: ~600 tokens per chunk with 100 token overlap.\n",
    "Using character-based approximation (1 token ≈ 4 characters).\n",
    "Each document is chunked independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  arxiv:2404.16130: 193 chunks (89608 chars)\n",
      "\n",
      "========================================\n",
      "Total: 193 chunks across 1 sources\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG uses 600 tokens with 100 token overlap\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Chunk each document independently\n",
    "source_chunks: dict[str, list[str]] = {}\n",
    "total_chunks = 0\n",
    "\n",
    "for doc in all_documents:\n",
    "    doc_chunks = text_splitter.split_text(doc.content)\n",
    "    source_chunks[doc.source_id] = doc_chunks\n",
    "    total_chunks += len(doc_chunks)\n",
    "    print(f\"  {doc.source_id}: {len(doc_chunks)} chunks ({len(doc.content)} chars)\")\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Total: {total_chunks} chunks across {len(all_documents)} sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Ollama Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama test: Hello GraphRAG!\n",
      "Config: timeout=180.0s, max_retries=2\n"
     ]
    }
   ],
   "source": [
    "MAX_RETRIES = 2\n",
    "OLLAMA_TIMEOUT = 180.0  # seconds per request\n",
    "\n",
    "def chat_ollama(prompt: str, system: str = \"\", temperature: float = 0.0) -> str:\n",
    "    \"\"\"Send a chat request to Ollama with retry logic.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            response = httpx.post(\n",
    "                f\"{OLLAMA_BASE_URL}/api/chat\",\n",
    "                json={\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": temperature}\n",
    "                },\n",
    "                timeout=OLLAMA_TIMEOUT,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"message\"][\"content\"]\n",
    "        except (httpx.TimeoutException, httpx.HTTPStatusError) as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                wait = 2 * attempt\n",
    "                print(f\"[retry {attempt}/{MAX_RETRIES}: {type(e).__name__}, waiting {wait}s] \", end=\"\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Test the connection\n",
    "test_response = chat_ollama(\"Say 'Hello GraphRAG!' and nothing else.\")\n",
    "print(f\"Ollama test: {test_response}\")\n",
    "print(f\"Config: timeout={OLLAMA_TIMEOUT}s, max_retries={MAX_RETRIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Entity, Relationship & Claims Extraction\n",
    "\n",
    "Extract named entities, relationships, and claims from each document.\n",
    "\n",
    "**Entity types:** PERSON, ORGANIZATION, LOCATION, EVENT, PRODUCT, DATE, MONEY, CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction functions defined: extract_entities, extract_relationships, extract_claims\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    name: str\n",
    "    type: str\n",
    "    description: str\n",
    "    source_chunk: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    source: str\n",
    "    target: str\n",
    "    description: str\n",
    "    strength: float = 1.0\n",
    "    source_chunk: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Claim:\n",
    "    subject: str\n",
    "    claim_type: str\n",
    "    description: str\n",
    "    date: str = \"\"\n",
    "    source_chunk: int = 0\n",
    "\n",
    "\n",
    "# --- Extraction prompts ---\n",
    "\n",
    "ENTITY_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting named entities from text.\n",
    "\n",
    "Extract all named entities from the following text. For each entity provide:\n",
    "1. name: The entity name (use UPPERCASE for consistency)\n",
    "2. type: One of [PERSON, ORGANIZATION, LOCATION, EVENT, PRODUCT, DATE, MONEY, CONCEPT]\n",
    "3. description: A brief description of the entity based on the text\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"name\": \"JOHN SMITH\", \"type\": \"PERSON\", \"description\": \"CEO of Example Corp who announced the merger\"}},\n",
    "  {{\"name\": \"EXAMPLE CORP\", \"type\": \"ORGANIZATION\", \"description\": \"Technology company acquiring StartupXYZ\"}}\n",
    "]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "RELATIONSHIP_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting relationships between entities.\n",
    "\n",
    "Given the following text and list of entities, extract all relationships between them.\n",
    "For each relationship provide:\n",
    "1. source: The source entity name (UPPERCASE)\n",
    "2. target: The target entity name (UPPERCASE)\n",
    "3. description: A description of how these entities are related\n",
    "4. strength: A score from 1-10 indicating relationship strength (10 = very strong)\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"source\": \"JOHN SMITH\", \"target\": \"EXAMPLE CORP\", \"description\": \"John Smith is the CEO of Example Corp\", \"strength\": 9}},\n",
    "  {{\"source\": \"EXAMPLE CORP\", \"target\": \"STARTUPXYZ\", \"description\": \"Example Corp is acquiring StartupXYZ\", \"strength\": 8}}\n",
    "]\n",
    "\n",
    "ENTITIES:\n",
    "{entities}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "CLAIMS_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting factual claims from text.\n",
    "\n",
    "Extract all specific factual claims from the following text. For each claim provide:\n",
    "1. subject: The entity the claim is about (UPPERCASE)\n",
    "2. claim_type: One of [FACT, EVENT, STATEMENT, METRIC, PREDICTION]\n",
    "3. description: The specific claim or fact\n",
    "4. date: Associated date/timeframe if mentioned (otherwise empty string)\n",
    "\n",
    "Focus on:\n",
    "- Numerical facts (prices, percentages, amounts)\n",
    "- Events (announcements, launches, decisions)\n",
    "- Quotes and statements by people\n",
    "- Predictions and forecasts\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"subject\": \"EXAMPLE CORP\", \"claim_type\": \"METRIC\", \"description\": \"Stock rose 15% in after-hours trading\", \"date\": \"2026-02-10\"}},\n",
    "  {{\"subject\": \"JOHN SMITH\", \"claim_type\": \"STATEMENT\", \"description\": \"Stated that the merger will create 1000 new jobs\", \"date\": \"\"}}\n",
    "]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- Extraction functions ---\n",
    "\n",
    "def _parse_llm_json(response: str) -> list:\n",
    "    \"\"\"Parse JSON from LLM response, handling markdown code blocks.\"\"\"\n",
    "    json_str = response.strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str.split(\"```\")[1]\n",
    "        if json_str.startswith(\"json\"):\n",
    "            json_str = json_str[4:]\n",
    "    json_str = json_str.strip()\n",
    "    return json.loads(json_str)\n",
    "\n",
    "\n",
    "def extract_entities(text: str, chunk_id: int = 0) -> list[Entity]:\n",
    "    \"\"\"Extract entities from a text chunk using the LLM.\"\"\"\n",
    "    prompt = ENTITY_EXTRACTION_PROMPT.format(text=text)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        entities_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Entity(\n",
    "                name=e.get(\"name\", \"\").upper(),\n",
    "                type=e.get(\"type\", \"UNKNOWN\"),\n",
    "                description=e.get(\"description\", \"\"),\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for e in entities_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[E parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_relationships(text: str, entities: list[Entity], chunk_id: int = 0) -> list[Relationship]:\n",
    "    \"\"\"Extract relationships between entities from a text chunk.\"\"\"\n",
    "    entity_list = \", \".join([e.name for e in entities])\n",
    "    prompt = RELATIONSHIP_EXTRACTION_PROMPT.format(text=text, entities=entity_list)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        rels_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Relationship(\n",
    "                source=r.get(\"source\", \"\").upper(),\n",
    "                target=r.get(\"target\", \"\").upper(),\n",
    "                description=r.get(\"description\", \"\"),\n",
    "                strength=float(r.get(\"strength\", 5)) / 10.0,\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for r in rels_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[R parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_claims(text: str, chunk_id: int = 0) -> list[Claim]:\n",
    "    \"\"\"Extract factual claims from a text chunk.\"\"\"\n",
    "    prompt = CLAIMS_EXTRACTION_PROMPT.format(text=text)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        claims_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Claim(\n",
    "                subject=c.get(\"subject\", \"\").upper(),\n",
    "                claim_type=c.get(\"claim_type\", \"FACT\"),\n",
    "                description=c.get(\"description\", \"\"),\n",
    "                date=c.get(\"date\", \"\"),\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for c in claims_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[C parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"Extraction functions defined: extract_entities, extract_relationships, extract_claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[1/1] arxiv:2404.16130: From Local to Global: A Graph RAG Approach to Quer\n",
      "  193 chunks to process\n",
      "  Chunk 1/193: 3E 2R 0C\n",
      "  Chunk 2/193: 9E 5R 0C\n",
      "  Chunk 3/193: 4E 0R 0C\n",
      "  Chunk 4/193: 2E 0R 0C\n",
      "  Chunk 5/193: 3E 0R 0C\n",
      "  Chunk 6/193: 0E 0R 0C\n",
      "  Chunk 7/193: 4E 0R 0C\n",
      "  Chunk 8/193: 0E 0R 0C\n",
      "  Chunk 9/193: 2E 1R 0C\n",
      "  Chunk 10/193: 3E 2R 0C\n",
      "  Chunk 11/193: 1E 0R 0C\n",
      "  Chunk 12/193: 3E 0R 0C\n",
      "  Chunk 13/193: 5E 3R 0C\n",
      "  Chunk 14/193: 4E 11R 0C\n",
      "  Chunk 15/193: 2E 0R 0C\n",
      "  Chunk 16/193: 1E 0R 0C\n",
      "  Chunk 17/193: 9E 0R 0C\n",
      "  Chunk 18/193: 5E 2R 0C\n",
      "  Chunk 19/193: 12E 6R 0C\n",
      "  Chunk 20/193: 7E 0R 0C\n",
      "  Chunk 21/193: 5E 0R 0C\n",
      "  Chunk 22/193: 3E 0R 0C\n",
      "  Chunk 23/193: 2E 0R 0C\n",
      "  Chunk 24/193: 3E 2R 0C\n",
      "  Chunk 25/193: 7E 2R 0C\n",
      "  Chunk 26/193: 3E 0R 0C\n",
      "  Chunk 27/193: 13E 9R 0C\n",
      "  Chunk 28/193: 3E 0R 0C\n",
      "  Chunk 29/193: 3E 1R 0C\n",
      "  Chunk 30/193: 3E 2R 0C\n",
      "  Chunk 31/193: 3E 2R 0C\n",
      "  Chunk 32/193: 0E 0R 0C\n",
      "  Chunk 33/193: 1E 0R 1C\n",
      "  Chunk 34/193: 2E 1R 0C\n",
      "  Chunk 35/193: 4E 3R 0C\n",
      "  Chunk 36/193: 3E 2R 2C\n",
      "  Chunk 37/193: 3E 0R 0C\n",
      "  Chunk 38/193: 0E 0R 0C\n",
      "  Chunk 39/193: 3E 3R 0C\n",
      "  Chunk 40/193: 3E 2R 0C\n",
      "  Chunk 41/193: 0E 0R 0C\n",
      "  Chunk 42/193: 3E 0R 0C\n",
      "  Chunk 43/193: 4E 0R 0C\n",
      "  Chunk 44/193: 2E 0R 0C\n",
      "  Chunk 45/193: 0E 0R 0C\n",
      "  Chunk 46/193: 4E 3R 0C\n",
      "  Chunk 47/193: 0E 0R 0C\n",
      "  Chunk 48/193: 0E 0R 0C\n",
      "  Chunk 49/193: 1E 0R 0C\n",
      "  Chunk 50/193: 0E 0R 0C\n",
      "  Chunk 51/193: 2E 0R 0C\n",
      "  Chunk 52/193: 0E 0R 0C\n",
      "  Chunk 53/193: 0E 0R 0C\n",
      "  Chunk 54/193: 2E 0R 0C\n",
      "  Chunk 55/193: 2E 0R 0C\n",
      "  Chunk 56/193: 0E 0R 0C\n",
      "  Chunk 57/193: 0E 0R 0C\n",
      "  Chunk 58/193: 0E 0R 0C\n",
      "  Chunk 59/193: 1E 0R 0C\n",
      "  Chunk 60/193: 1E 0R 0C\n",
      "  Chunk 61/193: 2E 0R 0C\n",
      "  Chunk 62/193: 2E 0R 0C\n",
      "  Chunk 63/193: 3E 6R 0C\n",
      "  Chunk 64/193: 0E 0R 0C\n",
      "  Chunk 65/193: 1E 0R 0C\n",
      "  Chunk 66/193: 0E 0R 0C\n",
      "  Chunk 67/193: 2E 0R 0C\n",
      "  Chunk 68/193: 8E 8R 0C\n",
      "  Chunk 69/193: 3E 2R 0C\n",
      "  Chunk 70/193: 1E 0R 0C\n",
      "  Chunk 71/193: 0E 0R 0C\n",
      "  Chunk 72/193: 5E 0R 0C\n",
      "  Chunk 73/193: 3E 0R 0C\n",
      "  Chunk 74/193: 6E 2R 0C\n",
      "  Chunk 75/193: 0E 0R 0C\n",
      "  Chunk 76/193: 0E 0R 0C\n",
      "  Chunk 77/193: 3E 0R 0C\n",
      "  Chunk 78/193: 5E 0R 0C\n",
      "  Chunk 79/193: 0E 0R 0C\n",
      "  Chunk 80/193: 0E 0R 0C\n",
      "  Chunk 81/193: 1E 0R 0C\n",
      "  Chunk 82/193: 2E 0R 0C\n",
      "  Chunk 83/193: 9E 0R 0C\n",
      "  Chunk 84/193: 3E 4R 0C\n",
      "  Chunk 85/193: 0E 0R 0C\n",
      "  Chunk 86/193: 0E 0R 0C\n",
      "  Chunk 87/193: 3E 0R 0C\n",
      "  Chunk 88/193: 1E 0R 0C\n",
      "  Chunk 89/193: 1E 0R 0C\n",
      "  Chunk 90/193: 0E 0R 0C\n",
      "  Chunk 91/193: 1E 0R 0C\n",
      "  Chunk 92/193: 3E 2R 0C\n",
      "  Chunk 93/193: 2E 0R 0C\n",
      "  Chunk 94/193: 2E 1R 0C\n",
      "  Chunk 95/193: 3E 1R 0C\n",
      "  Chunk 96/193: 13E 12R 0C\n",
      "  Chunk 97/193: 0E 0R 0C\n",
      "  Chunk 98/193: 2E 0R 0C\n",
      "  Chunk 99/193: 9E 0R 0C\n",
      "  Chunk 100/193: 3E 0R 0C\n",
      "  Chunk 101/193: 4E 0R 0C\n",
      "  Chunk 102/193: 4E 8R 0C\n",
      "  Chunk 103/193: 16E 2R 0C\n",
      "  Chunk 104/193: 19E 3R 0C\n",
      "  Chunk 105/193: 16E 0R 0C\n",
      "  Chunk 106/193: 11E 0R 0C\n",
      "  Chunk 107/193: 4E 1R 0C\n",
      "  Chunk 108/193: 4E 0R 0C\n",
      "  Chunk 109/193: 15E 7R 0C\n",
      "  Chunk 110/193: 10E [retry 1/2: ReadTimeout, waiting 2s] SKIPPED (ReadTimeout: timed out)\n",
      "  Chunk 111/193: 13E 0R 0C\n",
      "  Chunk 112/193: 2E 0R 0C\n",
      "  Chunk 113/193: 4E 2R 2C\n",
      "  Chunk 114/193: 4E 3R 0C\n",
      "  Chunk 115/193: 5E 3R 0C\n",
      "  Chunk 116/193: [E parse error: Extra data: line 19 column 1 (char 1402)] 0E 0R 0C\n",
      "  Chunk 117/193: 14E 4R 0C\n",
      "  Chunk 118/193: [E parse error: Invalid control character at: line 15 column 98 (char 2140)] 0E 0R 0C\n",
      "  Chunk 119/193: 10E 10R 0C\n",
      "  Chunk 120/193: 8E 0R 0C\n",
      "  Chunk 121/193: 1E 0R 0C\n",
      "  Chunk 122/193: 18E 3R 0C\n",
      "  Chunk 123/193: 3E 0R 0C\n",
      "  Chunk 124/193: 18E 0R 0C\n",
      "  Chunk 125/193: 8E 0R 0C\n",
      "  Chunk 126/193: 14E 3R 0C\n",
      "  Chunk 127/193: 3E 0R 0C\n",
      "  Chunk 128/193: 3E 0R 0C\n",
      "  Chunk 129/193: 10E 0R 0C\n",
      "  Chunk 130/193: 6E 2R 0C\n",
      "  Chunk 131/193: 3E 0R 0C\n",
      "  Chunk 132/193: 0E 0R 0C\n",
      "  Chunk 133/193: 2E 0R 0C\n",
      "  Chunk 134/193: 3E 1R 0C\n",
      "  Chunk 135/193: 2E 1R 0C\n",
      "  Chunk 136/193: 7E 2R 0C\n",
      "  Chunk 137/193: 2E 0R 0C\n",
      "  Chunk 138/193: 1E 0R 0C\n",
      "  Chunk 139/193: 0E 0R 0C\n",
      "  Chunk 140/193: 7E 0R 0C\n",
      "  Chunk 141/193: 9E 5R 0C\n",
      "  Chunk 142/193: 4E 0R 0C\n",
      "  Chunk 143/193: 4E 0R 0C\n",
      "  Chunk 144/193: 0E 0R 0C\n",
      "  Chunk 145/193: 1E 0R 0C\n",
      "  Chunk 146/193: 1E 0R 0C\n",
      "  Chunk 147/193: 3E 1R 0C\n",
      "  Chunk 148/193: 4E 0R 0C\n",
      "  Chunk 149/193: 0E 0R 0C\n",
      "  Chunk 150/193: 1E 0R 0C\n",
      "  Chunk 151/193: [E parse error: Extra data: line 5 column 1 (char 213)] 0E 0R 0C\n",
      "  Chunk 152/193: 2E 0R 0C\n",
      "  Chunk 153/193: [E parse error: Extra data: line 6 column 1 (char 318)] 0E 0R [C parse error: Extra data: line 5 column 1 (char 276)] 0C\n",
      "  Chunk 154/193: 2E 0R 0C\n",
      "  Chunk 155/193: 2E 1R 0C\n",
      "  Chunk 156/193: 1E 0R 0C\n",
      "  Chunk 157/193: 3E 2R 0C\n",
      "  Chunk 158/193: 0E 0R 0C\n",
      "  Chunk 159/193: 1E 0R 0C\n",
      "  Chunk 160/193: 1E 0R 0C\n",
      "  Chunk 161/193: 0E 0R 0C\n",
      "  Chunk 162/193: 3E 2R 0C\n",
      "  Chunk 163/193: 0E 0R 0C\n",
      "  Chunk 164/193: 5E 4R 0C\n",
      "  Chunk 165/193: 1E 0R 0C\n",
      "  Chunk 166/193: 2E 1R 0C\n",
      "  Chunk 167/193: 0E 0R 0C\n",
      "  Chunk 168/193: 0E 0R 0C\n",
      "  Chunk 169/193: 0E 0R 0C\n",
      "  Chunk 170/193: 0E 0R 0C\n",
      "  Chunk 171/193: 0E 0R 0C\n",
      "  Chunk 172/193: 0E 0R 0C\n",
      "  Chunk 173/193: 0E 0R 0C\n",
      "  Chunk 174/193: 0E 0R 0C\n",
      "  Chunk 175/193: 0E 0R 0C\n",
      "  Chunk 176/193: [E parse error: Expecting value: line 1 column 1 (char 0)] 0E 0R [C parse error: Extra data: line 5 column 1 (char 135)] 0C\n",
      "  Chunk 177/193: 0E 0R 0C\n",
      "  Chunk 178/193: 0E 0R 0C\n",
      "  Chunk 179/193: 0E 0R 0C\n",
      "  Chunk 180/193: 0E 0R 0C\n",
      "  Chunk 181/193: 0E 0R 0C\n",
      "  Chunk 182/193: 0E 0R 0C\n",
      "  Chunk 183/193: 1E 0R 0C\n",
      "  Chunk 184/193: 1E 0R 0C\n",
      "  Chunk 185/193: 1E 0R 0C\n",
      "  Chunk 186/193: 4E 2R 0C\n",
      "  Chunk 187/193: 2E 1R 0C\n",
      "  Chunk 188/193: 1E 0R 0C\n",
      "  Chunk 189/193: 4E 0R 0C\n",
      "  Chunk 190/193: 6E 15R 0C\n",
      "  Chunk 191/193: 3E 2R 0C\n",
      "  Chunk 192/193: 6E 5R 0C\n",
      "  Chunk 193/193: 0E 0R 0C\n",
      "  => 623E, 198R, 5C\n",
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE\n",
      "Total: 623 entities, 198 relationships, 5 claims\n",
      "Chunks: 192 succeeded, 1 skipped\n",
      "\n",
      "--- Skipped Chunks ---\n",
      "  [arxiv:2404.16130] chunk 109 (511 chars): ReadTimeout: timed out\n"
     ]
    }
   ],
   "source": [
    "# Extract entities, relationships, and claims from all documents\n",
    "source_results: dict[str, dict] = {}\n",
    "global_chunk_offset = 0\n",
    "skipped_chunks: list[dict] = []  # track failures for diagnostics\n",
    "\n",
    "for doc_idx, doc in enumerate(all_documents):\n",
    "    doc_chunks = source_chunks[doc.source_id]\n",
    "    doc_entities: list[Entity] = []\n",
    "    doc_relationships: list[Relationship] = []\n",
    "    doc_claims: list[Claim] = []\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{doc_idx+1}/{len(all_documents)}] {doc.source_id}: {doc.title[:50]}\")\n",
    "    print(f\"  {len(doc_chunks)} chunks to process\")\n",
    "\n",
    "    for i, chunk in enumerate(doc_chunks):\n",
    "        chunk_id = global_chunk_offset + i\n",
    "        print(f\"  Chunk {i+1}/{len(doc_chunks)}: \", end=\"\")\n",
    "\n",
    "        try:\n",
    "            # Entity extraction\n",
    "            entities = extract_entities(chunk, chunk_id=chunk_id)\n",
    "            doc_entities.extend(entities)\n",
    "            print(f\"{len(entities)}E \", end=\"\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            # Relationship extraction\n",
    "            chunk_entities = [e for e in doc_entities if e.source_chunk == chunk_id]\n",
    "            if len(chunk_entities) >= 2:\n",
    "                relationships = extract_relationships(chunk, chunk_entities, chunk_id=chunk_id)\n",
    "                doc_relationships.extend(relationships)\n",
    "                print(f\"{len(relationships)}R \", end=\"\")\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                print(f\"0R \", end=\"\")\n",
    "\n",
    "            # Claims extraction\n",
    "            claims = extract_claims(chunk, chunk_id=chunk_id)\n",
    "            doc_claims.extend(claims)\n",
    "            print(f\"{len(claims)}C\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            err_type = type(e).__name__\n",
    "            print(f\"SKIPPED ({err_type}: {e})\")\n",
    "            skipped_chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"source_id\": doc.source_id,\n",
    "                \"chunk_index\": i,\n",
    "                \"error\": f\"{err_type}: {e}\",\n",
    "                \"chunk_len\": len(chunk),\n",
    "            })\n",
    "\n",
    "    global_chunk_offset += len(doc_chunks)\n",
    "\n",
    "    source_results[doc.source_id] = {\n",
    "        \"entities\": doc_entities,\n",
    "        \"relationships\": doc_relationships,\n",
    "        \"claims\": doc_claims,\n",
    "        \"chunks\": doc_chunks,\n",
    "    }\n",
    "\n",
    "    print(f\"  => {len(doc_entities)}E, {len(doc_relationships)}R, {len(doc_claims)}C\")\n",
    "\n",
    "# Grand totals\n",
    "total_e = sum(len(r[\"entities\"]) for r in source_results.values())\n",
    "total_r = sum(len(r[\"relationships\"]) for r in source_results.values())\n",
    "total_c = sum(len(r[\"claims\"]) for r in source_results.values())\n",
    "total_processed = sum(len(r[\"chunks\"]) for r in source_results.values())\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EXTRACTION COMPLETE\")\n",
    "print(f\"Total: {total_e} entities, {total_r} relationships, {total_c} claims\")\n",
    "print(f\"Chunks: {total_processed - len(skipped_chunks)} succeeded, {len(skipped_chunks)} skipped\")\n",
    "\n",
    "if skipped_chunks:\n",
    "    print(f\"\\n--- Skipped Chunks ---\")\n",
    "    for sc in skipped_chunks:\n",
    "        print(f\"  [{sc['source_id']}] chunk {sc['chunk_index']} ({sc['chunk_len']} chars): {sc['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PER-SOURCE EXTRACTION RESULTS ===\n",
      "\n",
      "Source                      Entities  Relations     Claims\n",
      "------------------------------------------------------------\n",
      "arxiv:2404.16130                 623        198          5\n",
      "------------------------------------------------------------\n",
      "TOTAL                            623        198          5\n"
     ]
    }
   ],
   "source": [
    "# Display per-source extraction summary\n",
    "print(\"=== PER-SOURCE EXTRACTION RESULTS ===\\n\")\n",
    "print(f\"{'Source':<25} {'Entities':>10} {'Relations':>10} {'Claims':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for source_id, result in source_results.items():\n",
    "    print(f\"{source_id:<25} {len(result['entities']):>10} {len(result['relationships']):>10} {len(result['claims']):>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'TOTAL':<25} {total_e:>10} {total_r:>10} {total_c:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PER-SOURCE DEDUPLICATION ===\n",
      "\n",
      "  arxiv:2404.16130: 623 raw -> 466 unique\n",
      "\n",
      "=== GLOBAL MERGE ===\n",
      "  Global unique entities: 466\n",
      "  Global relationships: 198\n",
      "  Global claims: 5\n",
      "\n",
      "=== CHUNK PROVENANCE ===\n",
      "  Entities with chunk tracking: 466\n",
      "  Entities in 2+ chunks: 69\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate entities by name (merge descriptions)\n",
    "def deduplicate_entities(entities: list[Entity]) -> list[Entity]:\n",
    "    \"\"\"Merge duplicate entities, combining their descriptions.\"\"\"\n",
    "    entity_map: dict[str, Entity] = {}\n",
    "    \n",
    "    for entity in entities:\n",
    "        key = entity.name\n",
    "        if key in entity_map:\n",
    "            existing = entity_map[key]\n",
    "            if entity.description and entity.description not in existing.description:\n",
    "                existing.description = f\"{existing.description} | {entity.description}\"\n",
    "        else:\n",
    "            entity_map[key] = Entity(\n",
    "                name=entity.name,\n",
    "                type=entity.type,\n",
    "                description=entity.description,\n",
    "                source_chunk=entity.source_chunk\n",
    "            )\n",
    "    \n",
    "    return list(entity_map.values())\n",
    "\n",
    "\n",
    "def merge_entities_across_sources(\n",
    "    source_results: dict[str, dict]\n",
    ") -> tuple[list[Entity], list[Relationship], list[Claim]]:\n",
    "    \"\"\"Deduplicate within each source, then merge across all sources.\n",
    "    \n",
    "    Returns global_entities, global_relationships, global_claims.\n",
    "    Also builds entity_source_map: entity_name -> list of source_ids.\n",
    "    \"\"\"\n",
    "    # Phase 1: Deduplicate within each source\n",
    "    for source_id, result in source_results.items():\n",
    "        result[\"unique_entities\"] = deduplicate_entities(result[\"entities\"])\n",
    "    \n",
    "    # Phase 2: Merge across all sources\n",
    "    global_entity_map: dict[str, Entity] = {}\n",
    "    entity_source_map: dict[str, list[str]] = {}\n",
    "    \n",
    "    for source_id, result in source_results.items():\n",
    "        for entity in result[\"unique_entities\"]:\n",
    "            key = entity.name\n",
    "            if key in global_entity_map:\n",
    "                existing = global_entity_map[key]\n",
    "                if entity.description and entity.description not in existing.description:\n",
    "                    existing.description = f\"{existing.description} | {entity.description}\"\n",
    "                entity_source_map[key].append(source_id)\n",
    "            else:\n",
    "                global_entity_map[key] = Entity(\n",
    "                    name=entity.name,\n",
    "                    type=entity.type,\n",
    "                    description=entity.description,\n",
    "                    source_chunk=entity.source_chunk\n",
    "                )\n",
    "                entity_source_map[key] = [source_id]\n",
    "    \n",
    "    global_entities = list(global_entity_map.values())\n",
    "    \n",
    "    # Merge all relationships (keep all, some may reference same entities)\n",
    "    global_relationships = []\n",
    "    for result in source_results.values():\n",
    "        global_relationships.extend(result[\"relationships\"])\n",
    "    \n",
    "    # Merge all claims\n",
    "    global_claims = []\n",
    "    for result in source_results.values():\n",
    "        global_claims.extend(result[\"claims\"])\n",
    "    \n",
    "    return global_entities, global_relationships, global_claims, entity_source_map\n",
    "\n",
    "\n",
    "# Run cross-document merge\n",
    "global_entities, global_relationships, global_claims, entity_source_map = merge_entities_across_sources(source_results)\n",
    "\n",
    "# Build entity -> chunk provenance mapping from raw extraction data.\n",
    "# Each entity was extracted from a specific chunk; this records ALL chunks\n",
    "# that produced each entity (before deduplication collapsed them).\n",
    "entity_chunk_map: dict[str, list[dict]] = {}\n",
    "for doc in all_documents:\n",
    "    result = source_results[doc.source_id]\n",
    "    for entity in result[\"entities\"]:\n",
    "        key = entity.name\n",
    "        if key not in entity_chunk_map:\n",
    "            entity_chunk_map[key] = []\n",
    "        entry = {\"chunk_index\": entity.source_chunk, \"source_id\": doc.source_id}\n",
    "        if entry not in entity_chunk_map[key]:\n",
    "            entity_chunk_map[key].append(entry)\n",
    "\n",
    "# Per-source dedup stats\n",
    "print(\"=== PER-SOURCE DEDUPLICATION ===\\n\")\n",
    "for source_id, result in source_results.items():\n",
    "    raw = len(result[\"entities\"])\n",
    "    unique = len(result[\"unique_entities\"])\n",
    "    print(f\"  {source_id}: {raw} raw -> {unique} unique\")\n",
    "\n",
    "print(f\"\\n=== GLOBAL MERGE ===\")\n",
    "print(f\"  Global unique entities: {len(global_entities)}\")\n",
    "print(f\"  Global relationships: {len(global_relationships)}\")\n",
    "print(f\"  Global claims: {len(global_claims)}\")\n",
    "\n",
    "# Chunk provenance stats\n",
    "entities_with_chunks = sum(1 for v in entity_chunk_map.values() if v)\n",
    "multi_chunk = sum(1 for v in entity_chunk_map.values() if len(v) > 1)\n",
    "print(f\"\\n=== CHUNK PROVENANCE ===\")\n",
    "print(f\"  Entities with chunk tracking: {entities_with_chunks}\")\n",
    "print(f\"  Entities in 2+ chunks: {multi_chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Extraction Results\n",
    "\n",
    "Consolidate all extracted knowledge elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPHRAG MULTI-SOURCE EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Sources: 1\n",
      "Total content: 89,608 characters\n",
      "Total chunks: 193\n",
      "\n",
      "Global entities: 466\n",
      "Global relationships: 198\n",
      "Global claims: 5\n",
      "\n",
      "--- Entity Types ---\n",
      "  PERSON: 258\n",
      "  CONCEPT: 81\n",
      "  ORGANIZATION: 32\n",
      "  EVENT: 30\n",
      "  PRODUCT: 30\n",
      "  LOCATION: 14\n",
      "  DATE: 9\n",
      "  MONEY: 5\n",
      "  JOURNAL: 3\n",
      "  PROJECT: 1\n",
      "  SOFTWARE: 1\n",
      "  ELEMENT_INSTANCE: 1\n",
      "  : 1\n",
      "\n",
      "--- Claim Types ---\n",
      "  EVENT: 3\n",
      "  FACT: 2\n",
      "\n",
      "--- Cross-Source Entities (0 entities in 2+ sources) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GRAPHRAG MULTI-SOURCE EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSources: {len(all_documents)}\")\n",
    "print(f\"Total content: {total_chars:,} characters\")\n",
    "print(f\"Total chunks: {total_chunks}\")\n",
    "print(f\"\\nGlobal entities: {len(global_entities)}\")\n",
    "print(f\"Global relationships: {len(global_relationships)}\")\n",
    "print(f\"Global claims: {len(global_claims)}\")\n",
    "\n",
    "# Entity type breakdown\n",
    "print(\"\\n--- Entity Types ---\")\n",
    "type_counts: dict[str, int] = {}\n",
    "for e in global_entities:\n",
    "    type_counts[e.type] = type_counts.get(e.type, 0) + 1\n",
    "for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")\n",
    "\n",
    "# Claim type breakdown\n",
    "print(\"\\n--- Claim Types ---\")\n",
    "claim_type_counts: dict[str, int] = {}\n",
    "for c in global_claims:\n",
    "    claim_type_counts[c.claim_type] = claim_type_counts.get(c.claim_type, 0) + 1\n",
    "for t, count in sorted(claim_type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")\n",
    "\n",
    "# Cross-source entities\n",
    "multi_source = {k: v for k, v in entity_source_map.items() if len(v) > 1}\n",
    "print(f\"\\n--- Cross-Source Entities ({len(multi_source)} entities in 2+ sources) ---\")\n",
    "for name, sources in sorted(multi_source.items(), key=lambda x: -len(x[1])):\n",
    "    print(f\"  {name}: {sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook we will:\n",
    "1. **Build the knowledge graph** - Store entities and relationships from all 7 sources in a graph structure\n",
    "2. **Apply community detection** - Use Louvain algorithm to find cross-domain topic clusters\n",
    "3. **Generate community summaries** - Create hierarchical summaries for each cluster\n",
    "4. **Interactive visualization** - Explore the graph with ipycytoscape\n",
    "5. **Store in SQLite** - Persist the graph for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to extraction_results.json\n",
      "  1 sources\n",
      "  466 merged entities\n",
      "  198 relationships\n",
      "  5 claims\n",
      "  466 entities with chunk provenance\n"
     ]
    }
   ],
   "source": [
    "# Export extracted data in multi-document format\n",
    "extraction_results = {\n",
    "    \"sources\": [],\n",
    "    \"merged\": {\n",
    "        \"entities\": [{\"name\": e.name, \"type\": e.type, \"description\": e.description} for e in global_entities],\n",
    "        \"relationships\": [{\"source\": r.source, \"target\": r.target, \"description\": r.description, \"strength\": r.strength} for r in global_relationships],\n",
    "        \"claims\": [{\"subject\": c.subject, \"claim_type\": c.claim_type, \"description\": c.description, \"date\": c.date} for c in global_claims],\n",
    "        \"entity_source_map\": {k: v for k, v in entity_source_map.items()},\n",
    "        \"entity_chunk_map\": entity_chunk_map,\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"total_sources\": len(all_documents),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add per-source data\n",
    "for doc in all_documents:\n",
    "    result = source_results[doc.source_id]\n",
    "    extraction_results[\"sources\"].append({\n",
    "        \"source_id\": doc.source_id,\n",
    "        \"source_type\": doc.source_type,\n",
    "        \"title\": doc.title,\n",
    "        \"url\": doc.url,\n",
    "        \"content_type\": doc.content_type,\n",
    "        \"content_length\": len(doc.content),\n",
    "        \"fetched_at\": doc.fetched_at,\n",
    "        \"chunks\": result[\"chunks\"],\n",
    "        \"entities\": [{\"name\": e.name, \"type\": e.type, \"description\": e.description} for e in result.get(\"unique_entities\", result[\"entities\"])],\n",
    "        \"relationships\": [{\"source\": r.source, \"target\": r.target, \"description\": r.description, \"strength\": r.strength} for r in result[\"relationships\"]],\n",
    "        \"claims\": [{\"subject\": c.subject, \"claim_type\": c.claim_type, \"description\": c.description, \"date\": c.date} for c in result[\"claims\"]],\n",
    "    })\n",
    "\n",
    "with open(\"extraction_results.json\", \"w\") as f:\n",
    "    json.dump(extraction_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to extraction_results.json\")\n",
    "print(f\"  {len(extraction_results['sources'])} sources\")\n",
    "print(f\"  {len(extraction_results['merged']['entities'])} merged entities\")\n",
    "print(f\"  {len(extraction_results['merged']['relationships'])} relationships\")\n",
    "print(f\"  {len(extraction_results['merged']['claims'])} claims\")\n",
    "print(f\"  {len(extraction_results['merged']['entity_chunk_map'])} entities with chunk provenance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CROSS-SOURCE ENTITY OVERLAP: 0 entities in 2+ sources\n",
      "============================================================\n",
      "\n",
      "  No cross-source entities found.\n",
      "  This is expected with diverse topics — entities are domain-specific.\n",
      "\n",
      "============================================================\n",
      "SOURCE PAIR OVERLAP (shared entity count)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cross-source entity overlap analysis\n",
    "multi_source = {k: v for k, v in entity_source_map.items() if len(v) > 1}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"CROSS-SOURCE ENTITY OVERLAP: {len(multi_source)} entities in 2+ sources\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if multi_source:\n",
    "    for name, sources in sorted(multi_source.items(), key=lambda x: -len(x[1])):\n",
    "        entity = next((e for e in global_entities if e.name == name), None)\n",
    "        etype = entity.type if entity else \"?\"\n",
    "        print(f\"\\n  [{etype}] {name} (in {len(sources)} sources)\")\n",
    "        for sid in sources:\n",
    "            doc = next((d for d in all_documents if d.source_id == sid), None)\n",
    "            if doc:\n",
    "                print(f\"    - {sid}: {doc.title[:50]}\")\n",
    "else:\n",
    "    print(\"\\n  No cross-source entities found.\")\n",
    "    print(\"  This is expected with diverse topics — entities are domain-specific.\")\n",
    "\n",
    "# Source overlap matrix\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SOURCE PAIR OVERLAP (shared entity count)\")\n",
    "print(\"=\"*60)\n",
    "source_ids = [doc.source_id for doc in all_documents]\n",
    "for i, s1 in enumerate(source_ids):\n",
    "    for s2 in source_ids[i+1:]:\n",
    "        shared = [name for name, srcs in entity_source_map.items()\n",
    "                  if s1 in srcs and s2 in srcs]\n",
    "        if shared:\n",
    "            print(f\"  {s1} <-> {s2}: {len(shared)} ({', '.join(shared[:5])}{'...' if len(shared) > 5 else ''})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKIA GraphRAG",
   "language": "python",
   "name": "dkia-graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
