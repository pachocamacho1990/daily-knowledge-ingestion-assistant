{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Step 1: Document Processing & Knowledge Extraction\n",
    "\n",
    "This notebook implements the core GraphRAG pipeline from Microsoft's paper [\"From Local to Global: A Graph RAG Approach\"](https://arxiv.org/abs/2404.16130).\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Load document** - Sample text for processing\n",
    "2. **Chunk document** - Split into 600-token chunks with 100-token overlap\n",
    "3. **Entity extraction** - Use LLM to identify named entities\n",
    "4. **Relationship extraction** - Extract connections between entities\n",
    "5. **Claims extraction** - Extract factual statements about entities\n",
    "\n",
    "## Model\n",
    "Using locally deployed Ollama with `qwen2.5:3b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pacho-home-server/daily-knowledge-ingestion-assistant/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "from typing import Any\n",
    "from dataclasses import dataclass, field\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "MODEL = \"qwen2.5:3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['qwen2.5:3b']\n"
     ]
    }
   ],
   "source": [
    "# Verify Ollama is running\n",
    "response = httpx.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Available models: {models}\")\n",
    "assert MODEL in models, f\"Model {MODEL} not found. Please run: ollama pull {MODEL}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Sample Document\n",
    "\n",
    "Using a sample tech news article for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 2281 characters\n"
     ]
    }
   ],
   "source": [
    "# Sample document - a tech news article about AI companies\n",
    "SAMPLE_DOCUMENT = \"\"\"\n",
    "OpenAI Announces GPT-5 Partnership with Microsoft\n",
    "\n",
    "San Francisco, February 2026 - OpenAI, the artificial intelligence research company led by CEO Sam Altman, \n",
    "announced today a major expansion of its partnership with Microsoft. The deal, reportedly worth $10 billion, \n",
    "will see Microsoft integrate GPT-5, OpenAI's latest large language model, across its entire product suite \n",
    "including Azure, Office 365, and GitHub Copilot.\n",
    "\n",
    "Sam Altman stated in the press conference: \"This partnership represents the next chapter in our mission to \n",
    "ensure artificial general intelligence benefits all of humanity. Microsoft's enterprise reach combined with \n",
    "our research capabilities creates unprecedented opportunities.\"\n",
    "\n",
    "Microsoft CEO Satya Nadella emphasized the strategic importance of the deal: \"AI is the defining technology \n",
    "of our era. By deepening our collaboration with OpenAI, we're positioning Microsoft at the forefront of this \n",
    "transformation.\"\n",
    "\n",
    "The announcement sent Microsoft shares up 4.2% in after-hours trading. Industry analysts at Goldman Sachs \n",
    "raised their price target for Microsoft stock to $450, citing the competitive advantage in enterprise AI.\n",
    "\n",
    "Google, Microsoft's primary competitor in cloud services, responded by announcing accelerated deployment of \n",
    "its Gemini Ultra model. Google CEO Sundar Pichai said the company would invest an additional $5 billion in \n",
    "AI infrastructure through 2027.\n",
    "\n",
    "The deal comes amid increased regulatory scrutiny of AI companies. The Federal Trade Commission (FTC) has \n",
    "opened an inquiry into whether the Microsoft-OpenAI relationship constitutes an anti-competitive arrangement. \n",
    "FTC Chair Lina Khan stated that regulators are \"closely monitoring the concentration of AI capabilities \n",
    "among a small number of tech giants.\"\n",
    "\n",
    "OpenAI board member and former Treasury Secretary Larry Summers defended the partnership, arguing that \n",
    "collaboration between research organizations and large technology companies accelerates beneficial AI \n",
    "development while maintaining necessary safety guardrails.\n",
    "\n",
    "The GPT-5 model, which OpenAI claims achieves human-level performance on complex reasoning tasks, will \n",
    "begin rolling out to Microsoft enterprise customers in Q3 2026. Consumer products powered by GPT-5 are \n",
    "expected to launch by early 2027.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(SAMPLE_DOCUMENT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Chunk Document\n",
    "\n",
    "Following GraphRAG methodology: ~600 tokens per chunk with 100 token overlap.\n",
    "Using character-based approximation (1 token ≈ 4 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 chunks\n",
      "\n",
      "--- Chunk 1 (424 chars) ---\n",
      "OpenAI Announces GPT-5 Partnership with Microsoft\n",
      "\n",
      "San Francisco, February 2026 - OpenAI, the artificial intelligence research company led by CEO Sam Altman, \n",
      "announced today a major expansion of its ...\n",
      "\n",
      "--- Chunk 2 (517 chars) ---\n",
      "Sam Altman stated in the press conference: \"This partnership represents the next chapter in our mission to \n",
      "ensure artificial general intelligence benefits all of humanity. Microsoft's enterprise reac...\n",
      "\n",
      "--- Chunk 3 (462 chars) ---\n",
      "The announcement sent Microsoft shares up 4.2% in after-hours trading. Industry analysts at Goldman Sachs \n",
      "raised their price target for Microsoft stock to $450, citing the competitive advantage in en...\n",
      "\n",
      "--- Chunk 4 (360 chars) ---\n",
      "The deal comes amid increased regulatory scrutiny of AI companies. The Federal Trade Commission (FTC) has \n",
      "opened an inquiry into whether the Microsoft-OpenAI relationship constitutes an anti-competit...\n",
      "\n",
      "--- Chunk 5 (508 chars) ---\n",
      "OpenAI board member and former Treasury Secretary Larry Summers defended the partnership, arguing that \n",
      "collaboration between research organizations and large technology companies accelerates benefici...\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG uses 600 tokens with 100 token overlap\n",
    "# Approximation: 1 token ≈ 4 characters\n",
    "CHUNK_SIZE = 600 # ~600 tokens\n",
    "CHUNK_OVERLAP = 100 # ~100 tokens\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(SAMPLE_DOCUMENT)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Ollama Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama test: Hello GraphRAG!\n"
     ]
    }
   ],
   "source": [
    "def chat_ollama(prompt: str, system: str = \"\", temperature: float = 0.0) -> str:\n",
    "    \"\"\"Send a chat request to Ollama and return the response.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = httpx.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/chat\",\n",
    "        json={\n",
    "            \"model\": MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        },\n",
    "        timeout=120.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "# Test the connection\n",
    "test_response = chat_ollama(\"Say 'Hello GraphRAG!' and nothing else.\")\n",
    "print(f\"Ollama test: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Entity Extraction\n",
    "\n",
    "Extract named entities with their types and descriptions.\n",
    "Entity types: PERSON, ORGANIZATION, LOCATION, EVENT, PRODUCT, DATE, MONEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    name: str\n",
    "    type: str\n",
    "    description: str\n",
    "    source_chunk: int = 0\n",
    "\n",
    "ENTITY_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting named entities from text.\n",
    "\n",
    "Extract all named entities from the following text. For each entity provide:\n",
    "1. name: The entity name (use UPPERCASE for consistency)\n",
    "2. type: One of [PERSON, ORGANIZATION, LOCATION, EVENT, PRODUCT, DATE, MONEY, CONCEPT]\n",
    "3. description: A brief description of the entity based on the text\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"name\": \"JOHN SMITH\", \"type\": \"PERSON\", \"description\": \"CEO of Example Corp who announced the merger\"}},\n",
    "  {{\"name\": \"EXAMPLE CORP\", \"type\": \"ORGANIZATION\", \"description\": \"Technology company acquiring StartupXYZ\"}}\n",
    "]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(text: str, chunk_id: int = 0) -> list[Entity]:\n",
    "    \"\"\"Extract entities from a text chunk using the LLM.\"\"\"\n",
    "    prompt = ENTITY_EXTRACTION_PROMPT.format(text=text)\n",
    "    response = chat_ollama(prompt)\n",
    "    \n",
    "    # Parse JSON from response (handle potential markdown code blocks)\n",
    "    json_str = response.strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str.split(\"```\")[1]\n",
    "        if json_str.startswith(\"json\"):\n",
    "            json_str = json_str[4:]\n",
    "    json_str = json_str.strip()\n",
    "    \n",
    "    try:\n",
    "        entities_data = json.loads(json_str)\n",
    "        return [\n",
    "            Entity(\n",
    "                name=e.get(\"name\", \"\").upper(),\n",
    "                type=e.get(\"type\", \"UNKNOWN\"),\n",
    "                description=e.get(\"description\", \"\"),\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for e in entities_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"Failed to parse JSON: {ex}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/5...\n",
      "  Found 4 entities\n",
      "Processing chunk 2/5...\n",
      "  Found 2 entities\n",
      "Processing chunk 3/5...\n",
      "  Found 5 entities\n",
      "Processing chunk 4/5...\n",
      "  Found 4 entities\n",
      "Processing chunk 5/5...\n",
      "  Found 5 entities\n",
      "\n",
      "Total entities extracted: 20\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from all chunks\n",
    "all_entities: list[Entity] = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "    entities = extract_entities(chunk, chunk_id=i)\n",
    "    all_entities.extend(entities)\n",
    "    print(f\"  Found {len(entities)} entities\")\n",
    "\n",
    "print(f\"\\nTotal entities extracted: {len(all_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXTRACTED ENTITIES ===\n",
      "\n",
      "[ORGANIZATION] OPENAI\n",
      "  Description: Artificial intelligence research company\n",
      "  Source: Chunk 1\n",
      "\n",
      "[ORGANIZATION] MICROSOFT\n",
      "  Description: Technology company\n",
      "  Source: Chunk 1\n",
      "\n",
      "[PERSON] SAM ALTMAN\n",
      "  Description: CEO of OpenAI\n",
      "  Source: Chunk 1\n",
      "\n",
      "[PRODUCT] GPT-5\n",
      "  Description: Large language model developed by OpenAI\n",
      "  Source: Chunk 1\n",
      "\n",
      "[PERSON] SATYA NADELLA\n",
      "  Description: CEO of Microsoft\n",
      "  Source: Chunk 2\n",
      "\n",
      "[ORGANIZATION] MICROSOFT\n",
      "  Description: Technology company\n",
      "  Source: Chunk 2\n",
      "\n",
      "[ORGANIZATION] MICROSOFT\n",
      "  Description: Company whose shares increased after announcement\n",
      "  Source: Chunk 3\n",
      "\n",
      "[ORGANIZATION] GOLDMAN SACHS\n",
      "  Description: Financial institution providing price target for Microsoft stock\n",
      "  Source: Chunk 3\n",
      "\n",
      "[ORGANIZATION] GOOGLE\n",
      "  Description: Competitor in cloud services with CEO Sundar Pichai\n",
      "  Source: Chunk 3\n",
      "\n",
      "[ORGANIZATION] MICROSOFT\n",
      "  Description: Company whose shares increased after announcement\n",
      "  Source: Chunk 3\n",
      "\n",
      "[PERSON] SUNDAR PICHAI\n",
      "  Description: CEO of Google\n",
      "  Source: Chunk 3\n",
      "\n",
      "[ORGANIZATION] FEDERAL TRADE COMMISSION\n",
      "  Description: Regulatory body investigating Microsoft-OpenAI relationship\n",
      "  Source: Chunk 4\n",
      "\n",
      "[ORGANIZATION] FTC\n",
      "  Description: Regulatory body investigating Microsoft-OpenAI relationship\n",
      "  Source: Chunk 4\n",
      "\n",
      "[PERSON] LINA KAHN\n",
      "  Description: Chair of the Federal Trade Commission\n",
      "  Source: Chunk 4\n",
      "\n",
      "[ORGANIZATION] FEDERAL TRADE COMMISSION\n",
      "  Description: Regulatory body monitoring AI capabilities among tech giants\n",
      "  Source: Chunk 4\n",
      "\n",
      "[ORGANIZATION] OPENAI\n",
      "  Description: Company\n",
      "  Source: Chunk 5\n",
      "\n",
      "[PERSON] TREASURY SECRETARY\n",
      "  Description: \n",
      "  Source: Chunk 5\n",
      "\n",
      "[ORGANIZATION] EXAMPLE CORP\n",
      "  Description: Technology company\n",
      "  Source: Chunk 5\n",
      "\n",
      "[ORGANIZATION] STARTUPXYZ\n",
      "  Description: \n",
      "  Source: Chunk 5\n",
      "\n",
      "[PRODUCT] GPT-5\n",
      "  Description: \n",
      "  Source: Chunk 5\n"
     ]
    }
   ],
   "source": [
    "# Display extracted entities\n",
    "print(\"\\n=== EXTRACTED ENTITIES ===\")\n",
    "for entity in all_entities:\n",
    "    print(f\"\\n[{entity.type}] {entity.name}\")\n",
    "    print(f\"  Description: {entity.description}\")\n",
    "    print(f\"  Source: Chunk {entity.source_chunk + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entities after deduplication: 14\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate entities by name (merge descriptions)\n",
    "def deduplicate_entities(entities: list[Entity]) -> list[Entity]:\n",
    "    \"\"\"Merge duplicate entities, combining their descriptions.\"\"\"\n",
    "    entity_map: dict[str, Entity] = {}\n",
    "    \n",
    "    for entity in entities:\n",
    "        key = entity.name\n",
    "        if key in entity_map:\n",
    "            # Merge descriptions if different\n",
    "            existing = entity_map[key]\n",
    "            if entity.description not in existing.description:\n",
    "                existing.description = f\"{existing.description} | {entity.description}\"\n",
    "        else:\n",
    "            entity_map[key] = Entity(\n",
    "                name=entity.name,\n",
    "                type=entity.type,\n",
    "                description=entity.description,\n",
    "                source_chunk=entity.source_chunk\n",
    "            )\n",
    "    \n",
    "    return list(entity_map.values())\n",
    "\n",
    "unique_entities = deduplicate_entities(all_entities)\n",
    "print(f\"Unique entities after deduplication: {len(unique_entities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Relationship Extraction\n",
    "\n",
    "Extract relationships between entities with descriptions and strength scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Relationship:\n",
    "    source: str\n",
    "    target: str\n",
    "    description: str\n",
    "    strength: float = 1.0\n",
    "    source_chunk: int = 0\n",
    "\n",
    "RELATIONSHIP_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting relationships between entities.\n",
    "\n",
    "Given the following text and list of entities, extract all relationships between them.\n",
    "For each relationship provide:\n",
    "1. source: The source entity name (UPPERCASE)\n",
    "2. target: The target entity name (UPPERCASE)\n",
    "3. description: A description of how these entities are related\n",
    "4. strength: A score from 1-10 indicating relationship strength (10 = very strong)\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"source\": \"JOHN SMITH\", \"target\": \"EXAMPLE CORP\", \"description\": \"John Smith is the CEO of Example Corp\", \"strength\": 9}},\n",
    "  {{\"source\": \"EXAMPLE CORP\", \"target\": \"STARTUPXYZ\", \"description\": \"Example Corp is acquiring StartupXYZ\", \"strength\": 8}}\n",
    "]\n",
    "\n",
    "ENTITIES:\n",
    "{entities}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "def extract_relationships(text: str, entities: list[Entity], chunk_id: int = 0) -> list[Relationship]:\n",
    "    \"\"\"Extract relationships between entities from a text chunk.\"\"\"\n",
    "    entity_list = \", \".join([e.name for e in entities])\n",
    "    prompt = RELATIONSHIP_EXTRACTION_PROMPT.format(text=text, entities=entity_list)\n",
    "    response = chat_ollama(prompt)\n",
    "    \n",
    "    # Parse JSON from response\n",
    "    json_str = response.strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str.split(\"```\")[1]\n",
    "        if json_str.startswith(\"json\"):\n",
    "            json_str = json_str[4:]\n",
    "    json_str = json_str.strip()\n",
    "    \n",
    "    try:\n",
    "        rels_data = json.loads(json_str)\n",
    "        return [\n",
    "            Relationship(\n",
    "                source=r.get(\"source\", \"\").upper(),\n",
    "                target=r.get(\"target\", \"\").upper(),\n",
    "                description=r.get(\"description\", \"\"),\n",
    "                strength=float(r.get(\"strength\", 5)) / 10.0,  # Normalize to 0-1\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for r in rels_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"Failed to parse JSON: {ex}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/5 for relationships...\n",
      "  Found 2 relationships\n",
      "Processing chunk 2/5 for relationships...\n",
      "  Found 0 relationships\n",
      "Processing chunk 3/5 for relationships...\n",
      "  Found 3 relationships\n",
      "Processing chunk 4/5 for relationships...\n",
      "  Found 2 relationships\n",
      "Processing chunk 5/5 for relationships...\n",
      "  Found 0 relationships\n",
      "\n",
      "Total relationships extracted: 7\n"
     ]
    }
   ],
   "source": [
    "# Extract relationships from all chunks\n",
    "all_relationships: list[Relationship] = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)} for relationships...\")\n",
    "    # Get entities relevant to this chunk\n",
    "    chunk_entities = [e for e in all_entities if e.source_chunk == i]\n",
    "    if len(chunk_entities) < 2:\n",
    "        print(f\"  Skipping - need at least 2 entities\")\n",
    "        continue\n",
    "    \n",
    "    relationships = extract_relationships(chunk, chunk_entities, chunk_id=i)\n",
    "    all_relationships.extend(relationships)\n",
    "    print(f\"  Found {len(relationships)} relationships\")\n",
    "\n",
    "print(f\"\\nTotal relationships extracted: {len(all_relationships)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXTRACTED RELATIONSHIPS ===\n",
      "\n",
      "OPENAI --> GPT-5\n",
      "  Description: OpenAI develops GPT-5\n",
      "  Strength: 0.8\n",
      "\n",
      "MICROSOFT --> GPT-5\n",
      "  Description: Microsoft integrates GPT-5 into its product suite\n",
      "  Strength: 0.9\n",
      "\n",
      "MICROSOFT --> GOLDMAN SACHS\n",
      "  Description: Goldman Sachs raised their price target for Microsoft stock\n",
      "  Strength: 0.8\n",
      "\n",
      "MICROSOFT --> GOOGLE\n",
      "  Description: Microsoft's primary competitor in cloud services is Google\n",
      "  Strength: 0.9\n",
      "\n",
      "GOOGLE --> SUNDAR PICHAI\n",
      "  Description: Sundar Pichai is the CEO of Google\n",
      "  Strength: 1.0\n",
      "\n",
      "FEDERAL TRADE COMMISSION --> LINA KAHN\n",
      "  Description: FEDERAL TRADE COMMISSION appoints Lina Kahn as Chair\n",
      "  Strength: 0.8\n",
      "\n",
      "FEDERAL TRADE COMMISSION --> MICROSOFT-OPENAI RELATIONSHIP\n",
      "  Description: FEDERAL TRADE COMMISSION is investigating the Microsoft-OpenAI relationship\n",
      "  Strength: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Display extracted relationships\n",
    "print(\"\\n=== EXTRACTED RELATIONSHIPS ===\")\n",
    "for rel in all_relationships:\n",
    "    print(f\"\\n{rel.source} --> {rel.target}\")\n",
    "    print(f\"  Description: {rel.description}\")\n",
    "    print(f\"  Strength: {rel.strength:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Claims Extraction\n",
    "\n",
    "Extract factual claims/statements about entities, including dates, events, and specific facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Claim:\n",
    "    subject: str  # Entity the claim is about\n",
    "    claim_type: str  # Type: FACT, EVENT, STATEMENT, METRIC\n",
    "    description: str  # The actual claim\n",
    "    date: str = \"\"  # Associated date if any\n",
    "    source_chunk: int = 0\n",
    "\n",
    "CLAIMS_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting factual claims from text.\n",
    "\n",
    "Extract all specific factual claims from the following text. For each claim provide:\n",
    "1. subject: The entity the claim is about (UPPERCASE)\n",
    "2. claim_type: One of [FACT, EVENT, STATEMENT, METRIC, PREDICTION]\n",
    "3. description: The specific claim or fact\n",
    "4. date: Associated date/timeframe if mentioned (otherwise empty string)\n",
    "\n",
    "Focus on:\n",
    "- Numerical facts (prices, percentages, amounts)\n",
    "- Events (announcements, launches, decisions)\n",
    "- Quotes and statements by people\n",
    "- Predictions and forecasts\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"subject\": \"EXAMPLE CORP\", \"claim_type\": \"METRIC\", \"description\": \"Stock rose 15% in after-hours trading\", \"date\": \"2026-02-10\"}},\n",
    "  {{\"subject\": \"JOHN SMITH\", \"claim_type\": \"STATEMENT\", \"description\": \"Stated that the merger will create 1000 new jobs\", \"date\": \"\"}}\n",
    "]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "def extract_claims(text: str, chunk_id: int = 0) -> list[Claim]:\n",
    "    \"\"\"Extract factual claims from a text chunk.\"\"\"\n",
    "    prompt = CLAIMS_EXTRACTION_PROMPT.format(text=text)\n",
    "    response = chat_ollama(prompt)\n",
    "    \n",
    "    # Parse JSON from response\n",
    "    json_str = response.strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str.split(\"```\")[1]\n",
    "        if json_str.startswith(\"json\"):\n",
    "            json_str = json_str[4:]\n",
    "    json_str = json_str.strip()\n",
    "    \n",
    "    try:\n",
    "        claims_data = json.loads(json_str)\n",
    "        return [\n",
    "            Claim(\n",
    "                subject=c.get(\"subject\", \"\").upper(),\n",
    "                claim_type=c.get(\"claim_type\", \"FACT\"),\n",
    "                description=c.get(\"description\", \"\"),\n",
    "                date=c.get(\"date\", \"\"),\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for c in claims_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"Failed to parse JSON: {ex}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/5 for claims...\n",
      "  Found 0 claims\n",
      "Processing chunk 2/5 for claims...\n",
      "  Found 0 claims\n",
      "Processing chunk 3/5 for claims...\n",
      "  Found 4 claims\n",
      "Processing chunk 4/5 for claims...\n",
      "  Found 1 claims\n",
      "Processing chunk 5/5 for claims...\n",
      "  Found 0 claims\n",
      "\n",
      "Total claims extracted: 5\n"
     ]
    }
   ],
   "source": [
    "# Extract claims from all chunks\n",
    "all_claims: list[Claim] = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)} for claims...\")\n",
    "    claims = extract_claims(chunk, chunk_id=i)\n",
    "    all_claims.extend(claims)\n",
    "    print(f\"  Found {len(claims)} claims\")\n",
    "\n",
    "print(f\"\\nTotal claims extracted: {len(all_claims)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXTRACTED CLAIMS ===\n",
      "\n",
      "[METRIC] MICROSOFT [2023-10-05]\n",
      "  Microsoft shares rose 4.2% in after-hours trading\n",
      "\n",
      "[STATEMENT] GOLDMAN SCHACS INDUSTRY ANALYSTS\n",
      "  Cited competitive advantage in enterprise AI\n",
      "\n",
      "[EVENT] GOOGLE\n",
      "  Announced accelerated deployment of its Gemini Ultra model\n",
      "\n",
      "[STATEMENT] GOOGLE CEO SUNDAR PICHAI\n",
      "  Stated the company would invest an additional $5 billion in AI infrastructure through 2027\n",
      "\n",
      "[STATEMENT] LINA KAHN\n",
      "  Stated that regulators are 'closely monitoring the concentration of AI capabilities among a small number of tech giants'\n"
     ]
    }
   ],
   "source": [
    "# Display extracted claims\n",
    "print(\"\\n=== EXTRACTED CLAIMS ===\")\n",
    "for claim in all_claims:\n",
    "    date_str = f\" [{claim.date}]\" if claim.date else \"\"\n",
    "    print(f\"\\n[{claim.claim_type}] {claim.subject}{date_str}\")\n",
    "    print(f\"  {claim.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Extraction Results\n",
    "\n",
    "Consolidate all extracted knowledge elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPHRAG EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Document: 2281 characters\n",
      "Chunks: 5\n",
      "\n",
      "Entities: 20 total, 14 unique\n",
      "Relationships: 7\n",
      "Claims: 5\n",
      "\n",
      "--- Entity Types ---\n",
      "  ORGANIZATION: 8\n",
      "  PERSON: 5\n",
      "  PRODUCT: 1\n",
      "\n",
      "--- Claim Types ---\n",
      "  STATEMENT: 3\n",
      "  METRIC: 1\n",
      "  EVENT: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GRAPHRAG EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDocument: {len(SAMPLE_DOCUMENT)} characters\")\n",
    "print(f\"Chunks: {len(chunks)}\")\n",
    "print(f\"\\nEntities: {len(all_entities)} total, {len(unique_entities)} unique\")\n",
    "print(f\"Relationships: {len(all_relationships)}\")\n",
    "print(f\"Claims: {len(all_claims)}\")\n",
    "\n",
    "# Entity type breakdown\n",
    "print(\"\\n--- Entity Types ---\")\n",
    "type_counts: dict[str, int] = {}\n",
    "for e in unique_entities:\n",
    "    type_counts[e.type] = type_counts.get(e.type, 0) + 1\n",
    "for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")\n",
    "\n",
    "# Claim type breakdown\n",
    "print(\"\\n--- Claim Types ---\")\n",
    "claim_type_counts: dict[str, int] = {}\n",
    "for c in all_claims:\n",
    "    claim_type_counts[c.claim_type] = claim_type_counts.get(c.claim_type, 0) + 1\n",
    "for t, count in sorted(claim_type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook we will:\n",
    "1. **Build the knowledge graph** - Store entities and relationships in a graph structure\n",
    "2. **Apply community detection** - Use Leiden algorithm to find topic clusters\n",
    "3. **Generate community summaries** - Create hierarchical summaries for each cluster\n",
    "4. **Store in SQLite** - Persist the graph for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to extraction_results.json\n"
     ]
    }
   ],
   "source": [
    "# Export extracted data for next notebook\n",
    "extraction_results = {\n",
    "    \"document_length\": len(SAMPLE_DOCUMENT),\n",
    "    \"chunks\": chunks,\n",
    "    \"entities\": [{\"name\": e.name, \"type\": e.type, \"description\": e.description} for e in unique_entities],\n",
    "    \"relationships\": [{\"source\": r.source, \"target\": r.target, \"description\": r.description, \"strength\": r.strength} for r in all_relationships],\n",
    "    \"claims\": [{\"subject\": c.subject, \"claim_type\": c.claim_type, \"description\": c.description, \"date\": c.date} for c in all_claims]\n",
    "}\n",
    "\n",
    "with open(\"extraction_results.json\", \"w\") as f:\n",
    "    json.dump(extraction_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to extraction_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKIA GraphRAG",
   "language": "python",
   "name": "dkia-graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
