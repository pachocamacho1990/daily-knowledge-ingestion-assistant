{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Source Document Processing & Knowledge Extraction\n",
    "\n",
    "This notebook implements the core GraphRAG pipeline from Microsoft's paper [\"From Local to Global: A Graph RAG Approach\"](https://arxiv.org/abs/2404.16130).\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Define and fetch sources** - 7 documents from arXiv + web (with offline fallbacks)\n",
    "2. **Chunk all documents** - Split into 600-token chunks with 100-token overlap\n",
    "3. **Entity extraction** - Use LLM to identify named entities per document\n",
    "4. **Relationship extraction** - Extract connections between entities\n",
    "5. **Claims extraction** - Extract factual statements about entities\n",
    "6. **Cross-document merge** - Deduplicate and merge entities across all sources\n",
    "\n",
    "## Model\n",
    "Using locally deployed Ollama with `qwen2.5:3b`\n",
    "\n",
    "## Sources\n",
    "- 4 arXiv papers (AI, biology, climate, astrophysics)\n",
    "- 3 web articles (neuroscience, economics, space exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import time\n",
    "import tempfile\n",
    "from typing import Any\n",
    "from datetime import datetime, timezone\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import arxiv\n",
    "import pymupdf\n",
    "import trafilatura\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "MODEL = \"qwen2.5:3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['nomic-embed-text:latest', 'qwen2.5:3b']\n"
     ]
    }
   ],
   "source": [
    "# Verify Ollama is running\n",
    "response = httpx.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "models = [m[\"name\"] for m in response.json().get(\"models\", [])]\n",
    "print(f\"Available models: {models}\")\n",
    "assert MODEL in models, f\"Model {MODEL} not found. Please run: ollama pull {MODEL}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define and Fetch Sources\n",
    "\n",
    "We fetch 7 documents from diverse domains to stress-test the extraction pipeline. Each source has hardcoded fallback text so the notebook runs offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source limits: ARXIV_LIMIT=1, WEB_LIMIT=0\n",
      "  Using 1/4 arXiv sources, 0/3 web sources\n",
      "\n",
      "Fetching arXiv sources (downloading full PDFs)...\n",
      "  [OK] arxiv:2404.16130: fetched full PDF from arXiv (89608 chars)\n",
      "\n",
      "Fetching web sources...\n",
      "\n",
      "============================================================\n",
      "SOURCES LOADED: 1\n",
      "============================================================\n",
      "  [arxiv] arxiv:2404.16130: From Local to Global: A Graph RAG Approach to Query-Focused \n",
      "         89608 chars | research_paper\n",
      "\n",
      "Total content: 89,608 characters across 1 sources\n"
     ]
    }
   ],
   "source": [
    "# --- Source limits (for debugging/testing) ---\n",
    "# Set to None to use all sources, or a number to limit.\n",
    "# Example: ARXIV_LIMIT=1, WEB_LIMIT=0 → only the first arXiv paper\n",
    "ARXIV_LIMIT = 1   # None = all 4, or 1/2/3\n",
    "WEB_LIMIT = 0     # None = all 3, or 0/1/2\n",
    "\n",
    "@dataclass\n",
    "class SourceDocument:\n",
    "    source_id: str       # e.g. \"arxiv:2404.16130\"\n",
    "    source_type: str     # \"arxiv\" or \"web\"\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "    content_type: str    # \"research_paper\", \"news\", \"reference\"\n",
    "    fetched_at: str = \"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not self.fetched_at:\n",
    "            self.fetched_at = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "# --- Source configurations ---\n",
    "\n",
    "ARXIV_SOURCES = [\n",
    "    {\"id\": \"2404.16130\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"From Local to Global: A Graph RAG Approach to Query-Focused Summarization\"},\n",
    "    {\"id\": \"2404.18021\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments\"},\n",
    "    {\"id\": \"2312.14090\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"Climate Economics and Finance: A Review\"},\n",
    "    {\"id\": \"2304.04869\", \"content_type\": \"research_paper\",\n",
    "     \"title\": \"The JWST Mission: Astrophysics Enabled\"},\n",
    "]\n",
    "\n",
    "WEB_SOURCES = [\n",
    "    {\"url\": \"https://www.quantamagazine.org/memory-and-perception-are-intertwined-20240416/\",\n",
    "     \"source_id\": \"web:quanta-memory\", \"content_type\": \"news\",\n",
    "     \"title\": \"Memory and Perception Are Intertwined in the Brain\"},\n",
    "    {\"url\": \"https://www.imf.org/en/Blogs/Articles/2024/01/14/ai-will-transform-the-global-economy-lets-make-sure-it-benefits-humanity\",\n",
    "     \"source_id\": \"web:imf-ai-economy\", \"content_type\": \"news\",\n",
    "     \"title\": \"AI Will Transform the Global Economy\"},\n",
    "    {\"url\": \"https://www.planetary.org/space-missions/voyager\",\n",
    "     \"source_id\": \"web:planetary-voyager\", \"content_type\": \"reference\",\n",
    "     \"title\": \"Voyager: The Grand Tour of the Solar System\"},\n",
    "]\n",
    "\n",
    "# Apply source limits\n",
    "_arxiv_configs = ARXIV_SOURCES[:ARXIV_LIMIT] if ARXIV_LIMIT is not None else ARXIV_SOURCES\n",
    "_web_configs = WEB_SOURCES[:WEB_LIMIT] if WEB_LIMIT is not None else WEB_SOURCES\n",
    "\n",
    "print(f\"Source limits: ARXIV_LIMIT={ARXIV_LIMIT}, WEB_LIMIT={WEB_LIMIT}\")\n",
    "print(f\"  Using {len(_arxiv_configs)}/{len(ARXIV_SOURCES)} arXiv sources, {len(_web_configs)}/{len(WEB_SOURCES)} web sources\")\n",
    "\n",
    "# --- Hardcoded fallback content (offline mode) ---\n",
    "\n",
    "FALLBACK_CONTENT = {\n",
    "    \"arxiv:2404.16130\": \"\"\"From Local to Global: A Graph RAG Approach to Query-Focused Summarization.\n",
    "The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as \"What are the main themes in the dataset?\", since this is inherently a query-focused summarization (QFS) task. Prior QFS methods fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pre-generate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are summarized into a final response. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a naive RAG baseline for both the comprehensiveness and diversity of generated answers.\"\"\",\n",
    "\n",
    "    \"arxiv:2404.18021\": \"\"\"CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.\n",
    "The integration of large language models (LLMs) with biological research holds significant promise for accelerating scientific discovery. CRISPR-GPT is a novel LLM agent that automates the design of CRISPR gene-editing experiments. It combines domain-specific knowledge of CRISPR biology with the reasoning capabilities of GPT-4 to guide researchers through the entire experimental workflow—from target gene selection and guide RNA design to delivery method optimization and off-target analysis. CRISPR-GPT integrates multiple bioinformatics tools, including sequence alignment algorithms, off-target prediction models, and primer design software, enabling end-to-end experimental planning. Evaluation across diverse gene-editing tasks demonstrates that CRISPR-GPT matches the performance of expert human researchers while significantly reducing experiment design time. The system handles multiple CRISPR platforms including Cas9, Cas12a, and base editors, adapting its recommendations based on the specific biological context. This work demonstrates the potential of AI-assisted scientific research and raises important questions about the responsible development of autonomous biological research agents.\"\"\",\n",
    "\n",
    "    \"arxiv:2312.14090\": \"\"\"Climate Economics and Finance: A Comprehensive Review.\n",
    "Climate change poses fundamental challenges to economic systems worldwide. This review synthesizes research at the intersection of climate science, economics, and finance, covering carbon pricing mechanisms, green bond markets, stranded asset risks, and the economic modeling of climate damages. Integrated assessment models (IAMs) such as DICE and FUND translate climate projections into economic impacts, estimating the social cost of carbon (SCC) at $50-200 per ton. Carbon markets like the EU Emissions Trading System (EU ETS) and California's cap-and-trade program demonstrate market-based approaches to emissions reduction. The green bond market has grown from $3 billion in 2012 to over $500 billion in 2023, financing renewable energy, energy efficiency, and sustainable infrastructure. Central banks, including the European Central Bank (ECB) and Bank of England, are conducting climate stress tests on financial institutions to assess systemic risk. Physical climate risks—extreme weather, sea-level rise, and agricultural disruption—threaten $43 trillion in global assets by 2100. Transition risks from policy changes and technological shifts could render fossil fuel reserves worth $1-4 trillion as stranded assets. This review identifies carbon border adjustment mechanisms, climate-related financial disclosure frameworks such as TCFD, and natural capital accounting as critical frontiers in climate economics.\"\"\",\n",
    "\n",
    "    \"arxiv:2304.04869\": \"\"\"The James Webb Space Telescope Mission: Scientific Capabilities and Early Results.\n",
    "The James Webb Space Telescope (JWST), launched in December 2021 on an Ariane 5 rocket, represents the most ambitious space observatory ever built. With its 6.5-meter primary mirror composed of 18 gold-coated beryllium segments, JWST operates at the second Lagrange point (L2), 1.5 million kilometers from Earth. The telescope carries four science instruments: NIRCam (Near-Infrared Camera), NIRSpec (Near-Infrared Spectrograph), MIRI (Mid-Infrared Instrument), and FGS/NIRISS (Fine Guidance Sensor/Near Infrared Imager and Slitless Spectrograph). Early science results have been transformative across multiple astrophysics domains. In exoplanet science, JWST detected carbon dioxide in the atmosphere of WASP-39b, a gas giant 700 light-years away. Deep field observations revealed galaxies forming just 300 million years after the Big Bang, challenging models of early galaxy formation. JWST has captured unprecedented images of the Carina Nebula, revealing star-forming regions previously hidden by dust. The telescope's sensitivity enables detailed spectroscopy of stellar nurseries, providing insights into the chemical composition of protoplanetary disks. The mission, led by NASA in partnership with ESA and CSA, has an expected operational lifetime of 10-20 years, far exceeding the original 5-year design goal.\"\"\",\n",
    "\n",
    "    \"web:quanta-memory\": \"\"\"Memory and Perception Are Deeply Intertwined in the Brain.\n",
    "Neuroscience research reveals that the boundary between memory and perception is far more blurred than previously believed. The hippocampus, long considered the brain's memory center, plays an active role in perception. Studies show that the hippocampus predicts what we are about to see based on past experience, creating an internal model that shapes conscious perception in real time. Researchers at University College London used fMRI to demonstrate that hippocampal activity precedes visual cortex activation during familiar scene recognition, suggesting the brain uses memory templates to \"pre-render\" expected visual information. This predictive coding framework has implications for understanding Alzheimer's disease, where the breakdown of hippocampal prediction circuits may explain why patients experience perceptual disturbances before overt memory loss. The prefrontal cortex mediates between memory-based predictions and incoming sensory data, resolving conflicts when reality differs from expectation. Similar predictive mechanisms have been found in the auditory system, where the hippocampus anticipates sounds in familiar sequences. These findings challenge the traditional cognitive architecture that separates perception, memory, and prediction into distinct modules, instead suggesting a unified system where the brain constantly generates and tests predictions about the world using stored knowledge.\"\"\",\n",
    "\n",
    "    \"web:imf-ai-economy\": \"\"\"AI Will Transform the Global Economy — Let's Make Sure It Benefits Humanity.\n",
    "The International Monetary Fund analysis indicates that artificial intelligence will affect approximately 40 percent of all jobs worldwide. In advanced economies, the exposure rises to 60 percent of jobs, where AI could either complement human workers—boosting productivity by 15-25 percent in affected sectors—or replace them entirely. The IMF proposes a comprehensive AI Preparedness Index measuring countries across digital infrastructure, human capital, labor market policies, and regulatory frameworks. Nordic countries, Singapore, and the United States score highest on AI readiness. Emerging markets and developing economies face a different challenge: while less immediately exposed to AI displacement (26 percent of jobs affected), they lack the infrastructure and skilled workforce to capture AI's productivity benefits, potentially widening the global inequality gap. The IMF recommends establishing social safety nets calibrated to the pace of AI adoption, investing in education programs that emphasize skills complementary to AI—such as critical thinking, creativity, and emotional intelligence—and creating international frameworks for AI governance. The analysis warns that without proactive policies, AI could increase income inequality within countries by up to 15 percentage points over the next decade, as high-skilled workers who effectively leverage AI tools see disproportionate wage gains.\"\"\",\n",
    "\n",
    "    \"web:planetary-voyager\": \"\"\"Voyager: The Grand Tour of the Solar System.\n",
    "The Voyager program, launched by NASA in 1977, represents one of humanity's greatest exploration achievements. Voyager 1 and Voyager 2 were designed to take advantage of a rare planetary alignment occurring once every 176 years, enabling gravity-assist flybys of the outer planets. Voyager 1 visited Jupiter in 1979 and Saturn in 1980, discovering active volcanoes on Jupiter's moon Io—the first found beyond Earth—and the complex structure of Saturn's rings. Voyager 2 is the only spacecraft to have visited Uranus (1986) and Neptune (1989), revealing Uranus's extreme axial tilt and Neptune's Great Dark Spot. The spacecraft carry the Golden Record, a 12-inch gold-plated copper disc containing sounds and images representing life and culture on Earth, curated by a team led by Carl Sagan. Voyager 1 entered interstellar space in August 2012, becoming the first human-made object to leave the heliosphere, at a distance of 121 astronomical units from the Sun. Voyager 2 followed in November 2018. Both spacecraft continue to transmit scientific data using their 23-watt radio transmitters—roughly the power of a refrigerator light bulb—communicating with NASA's Deep Space Network. As of 2024, Voyager 1 is over 24 billion kilometers from Earth, traveling at 17 kilometers per second. The mission, originally designed for 5 years, has operated for over 46 years, with power from their radioisotope thermoelectric generators expected to sustain limited operations until approximately 2030.\"\"\",\n",
    "}\n",
    "\n",
    "# --- PDF text extraction ---\n",
    "\n",
    "def extract_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file using pymupdf.\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    text_parts = []\n",
    "    for page in doc:\n",
    "        text_parts.append(page.get_text())\n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "# --- Fetch functions ---\n",
    "\n",
    "def fetch_arxiv_sources(configs: list[dict]) -> list[SourceDocument]:\n",
    "    \"\"\"Fetch full paper content from arXiv PDFs. Falls back to abstract, then hardcoded content.\"\"\"\n",
    "    documents = []\n",
    "    for cfg in configs:\n",
    "        paper_id = cfg[\"id\"]\n",
    "        source_id = f\"arxiv:{paper_id}\"\n",
    "        fallback = FALLBACK_CONTENT.get(source_id, \"\")\n",
    "\n",
    "        try:\n",
    "            client = arxiv.Client()\n",
    "            search = arxiv.Search(id_list=[paper_id])\n",
    "            results = list(client.results(search))\n",
    "            if results:\n",
    "                paper = results[0]\n",
    "                title = paper.title\n",
    "                url = paper.entry_id\n",
    "\n",
    "                # Download and extract full PDF text\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    pdf_path = paper.download_pdf(dirpath=tmpdir)\n",
    "                    content = extract_pdf_text(pdf_path)\n",
    "\n",
    "                if len(content) < 500:\n",
    "                    raise ValueError(f\"PDF text too short ({len(content)} chars), falling back to abstract\")\n",
    "\n",
    "                print(f\"  [OK] {source_id}: fetched full PDF from arXiv ({len(content)} chars)\")\n",
    "            else:\n",
    "                raise ValueError(\"No results\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [FALLBACK] {source_id}: {e}\")\n",
    "            content = fallback\n",
    "            title = cfg.get(\"title\", paper_id)\n",
    "            url = f\"https://arxiv.org/abs/{paper_id}\"\n",
    "\n",
    "        documents.append(SourceDocument(\n",
    "            source_id=source_id,\n",
    "            source_type=\"arxiv\",\n",
    "            title=title,\n",
    "            url=url,\n",
    "            content=content,\n",
    "            content_type=cfg[\"content_type\"],\n",
    "        ))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def fetch_web_sources(configs: list[dict]) -> list[SourceDocument]:\n",
    "    \"\"\"Fetch article text from web pages. Falls back to hardcoded content.\"\"\"\n",
    "    documents = []\n",
    "    for cfg in configs:\n",
    "        source_id = cfg[\"source_id\"]\n",
    "        fallback = FALLBACK_CONTENT.get(source_id, \"\")\n",
    "\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(cfg[\"url\"])\n",
    "            if downloaded:\n",
    "                content = trafilatura.extract(downloaded)\n",
    "                if content and len(content) > 200:\n",
    "                    print(f\"  [OK] {source_id}: fetched from web ({len(content)} chars)\")\n",
    "                else:\n",
    "                    raise ValueError(\"Extracted content too short\")\n",
    "            else:\n",
    "                raise ValueError(\"Download failed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [FALLBACK] {source_id}: {e}\")\n",
    "            content = fallback\n",
    "\n",
    "        documents.append(SourceDocument(\n",
    "            source_id=source_id,\n",
    "            source_type=\"web\",\n",
    "            title=cfg[\"title\"],\n",
    "            url=cfg[\"url\"],\n",
    "            content=content,\n",
    "            content_type=cfg[\"content_type\"],\n",
    "        ))\n",
    "    return documents\n",
    "\n",
    "\n",
    "# --- Fetch all sources ---\n",
    "print(f\"\\nFetching arXiv sources (downloading full PDFs)...\")\n",
    "arxiv_docs = fetch_arxiv_sources(_arxiv_configs)\n",
    "\n",
    "print(f\"\\nFetching web sources...\")\n",
    "web_docs = fetch_web_sources(_web_configs)\n",
    "\n",
    "all_documents = arxiv_docs + web_docs\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SOURCES LOADED: {len(all_documents)}\")\n",
    "print(f\"{'='*60}\")\n",
    "total_chars = 0\n",
    "for doc in all_documents:\n",
    "    print(f\"  [{doc.source_type}] {doc.source_id}: {doc.title[:60]}\")\n",
    "    print(f\"         {len(doc.content)} chars | {doc.content_type}\")\n",
    "    total_chars += len(doc.content)\n",
    "print(f\"\\nTotal content: {total_chars:,} characters across {len(all_documents)} sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Chunk All Documents\n",
    "\n",
    "Following GraphRAG methodology: ~600 tokens per chunk with 100 token overlap.\n",
    "Using character-based approximation (1 token ≈ 4 characters).\n",
    "Each document is chunked independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  arxiv:2404.16130: 193 chunks (89608 chars)\n",
      "\n",
      "========================================\n",
      "Total: 193 chunks across 1 sources\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG uses 600 tokens with 100 token overlap\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Chunk each document independently\n",
    "source_chunks: dict[str, list[str]] = {}\n",
    "total_chunks = 0\n",
    "\n",
    "for doc in all_documents:\n",
    "    doc_chunks = text_splitter.split_text(doc.content)\n",
    "    source_chunks[doc.source_id] = doc_chunks\n",
    "    total_chunks += len(doc_chunks)\n",
    "    print(f\"  {doc.source_id}: {len(doc_chunks)} chunks ({len(doc.content)} chars)\")\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Total: {total_chunks} chunks across {len(all_documents)} sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Ollama Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MAX_RETRIES = 2\nOLLAMA_TIMEOUT = 180.0  # seconds per request\n\ndef chat_ollama(prompt: str, system: str = \"\", temperature: float = 0.0) -> str:\n    \"\"\"Send a chat request to Ollama with retry logic.\"\"\"\n    messages = []\n    if system:\n        messages.append({\"role\": \"system\", \"content\": system})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    for attempt in range(1, MAX_RETRIES + 1):\n        try:\n            response = httpx.post(\n                f\"{OLLAMA_BASE_URL}/api/chat\",\n                json={\n                    \"model\": MODEL,\n                    \"messages\": messages,\n                    \"stream\": False,\n                    \"options\": {\"temperature\": temperature}\n                },\n                timeout=OLLAMA_TIMEOUT,\n            )\n            response.raise_for_status()\n            return response.json()[\"message\"][\"content\"]\n        except (httpx.TimeoutException, httpx.HTTPStatusError) as e:\n            if attempt < MAX_RETRIES:\n                wait = 2 * attempt\n                print(f\"[retry {attempt}/{MAX_RETRIES}: {type(e).__name__}, waiting {wait}s] \", end=\"\")\n                time.sleep(wait)\n            else:\n                raise\n\n# Test the connection\ntest_response = chat_ollama(\"Say 'Hello GraphRAG!' and nothing else.\")\nprint(f\"Ollama test: {test_response}\")\nprint(f\"Config: timeout={OLLAMA_TIMEOUT}s, max_retries={MAX_RETRIES}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Entity, Relationship & Claims Extraction\n",
    "\n",
    "Extract named entities, relationships, and claims from each document.\n",
    "\n",
    "**Entity types:** PERSON, ORGANIZATION, LOCATION, EVENT, PRODUCT, DATE, MONEY, CONCEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction functions defined: extract_entities, extract_relationships, extract_claims\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    name: str\n",
    "    type: str\n",
    "    description: str\n",
    "    source_chunk: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    source: str\n",
    "    target: str\n",
    "    description: str\n",
    "    strength: float = 1.0\n",
    "    source_chunk: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Claim:\n",
    "    subject: str\n",
    "    claim_type: str\n",
    "    description: str\n",
    "    date: str = \"\"\n",
    "    source_chunk: int = 0\n",
    "\n",
    "\n",
    "# --- Extraction prompts ---\n",
    "\n",
    "ENTITY_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting named entities from text.\n",
    "\n",
    "Extract all named entities from the following text. For each entity provide:\n",
    "1. name: The entity name (use UPPERCASE for consistency)\n",
    "2. type: One of [PERSON, ORGANIZATION, LOCATION, EVENT, PRODUCT, DATE, MONEY, CONCEPT]\n",
    "3. description: A brief description of the entity based on the text\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"name\": \"JOHN SMITH\", \"type\": \"PERSON\", \"description\": \"CEO of Example Corp who announced the merger\"}},\n",
    "  {{\"name\": \"EXAMPLE CORP\", \"type\": \"ORGANIZATION\", \"description\": \"Technology company acquiring StartupXYZ\"}}\n",
    "]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "RELATIONSHIP_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting relationships between entities.\n",
    "\n",
    "Given the following text and list of entities, extract all relationships between them.\n",
    "For each relationship provide:\n",
    "1. source: The source entity name (UPPERCASE)\n",
    "2. target: The target entity name (UPPERCASE)\n",
    "3. description: A description of how these entities are related\n",
    "4. strength: A score from 1-10 indicating relationship strength (10 = very strong)\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"source\": \"JOHN SMITH\", \"target\": \"EXAMPLE CORP\", \"description\": \"John Smith is the CEO of Example Corp\", \"strength\": 9}},\n",
    "  {{\"source\": \"EXAMPLE CORP\", \"target\": \"STARTUPXYZ\", \"description\": \"Example Corp is acquiring StartupXYZ\", \"strength\": 8}}\n",
    "]\n",
    "\n",
    "ENTITIES:\n",
    "{entities}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "CLAIMS_EXTRACTION_PROMPT = \"\"\"\n",
    "You are an expert at extracting factual claims from text.\n",
    "\n",
    "Extract all specific factual claims from the following text. For each claim provide:\n",
    "1. subject: The entity the claim is about (UPPERCASE)\n",
    "2. claim_type: One of [FACT, EVENT, STATEMENT, METRIC, PREDICTION]\n",
    "3. description: The specific claim or fact\n",
    "4. date: Associated date/timeframe if mentioned (otherwise empty string)\n",
    "\n",
    "Focus on:\n",
    "- Numerical facts (prices, percentages, amounts)\n",
    "- Events (announcements, launches, decisions)\n",
    "- Quotes and statements by people\n",
    "- Predictions and forecasts\n",
    "\n",
    "Return ONLY valid JSON array. Example format:\n",
    "[\n",
    "  {{\"subject\": \"EXAMPLE CORP\", \"claim_type\": \"METRIC\", \"description\": \"Stock rose 15% in after-hours trading\", \"date\": \"2026-02-10\"}},\n",
    "  {{\"subject\": \"JOHN SMITH\", \"claim_type\": \"STATEMENT\", \"description\": \"Stated that the merger will create 1000 new jobs\", \"date\": \"\"}}\n",
    "]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "JSON OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- Extraction functions ---\n",
    "\n",
    "def _parse_llm_json(response: str) -> list:\n",
    "    \"\"\"Parse JSON from LLM response, handling markdown code blocks.\"\"\"\n",
    "    json_str = response.strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str.split(\"```\")[1]\n",
    "        if json_str.startswith(\"json\"):\n",
    "            json_str = json_str[4:]\n",
    "    json_str = json_str.strip()\n",
    "    return json.loads(json_str)\n",
    "\n",
    "\n",
    "def extract_entities(text: str, chunk_id: int = 0) -> list[Entity]:\n",
    "    \"\"\"Extract entities from a text chunk using the LLM.\"\"\"\n",
    "    prompt = ENTITY_EXTRACTION_PROMPT.format(text=text)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        entities_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Entity(\n",
    "                name=e.get(\"name\", \"\").upper(),\n",
    "                type=e.get(\"type\", \"UNKNOWN\"),\n",
    "                description=e.get(\"description\", \"\"),\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for e in entities_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[E parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_relationships(text: str, entities: list[Entity], chunk_id: int = 0) -> list[Relationship]:\n",
    "    \"\"\"Extract relationships between entities from a text chunk.\"\"\"\n",
    "    entity_list = \", \".join([e.name for e in entities])\n",
    "    prompt = RELATIONSHIP_EXTRACTION_PROMPT.format(text=text, entities=entity_list)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        rels_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Relationship(\n",
    "                source=r.get(\"source\", \"\").upper(),\n",
    "                target=r.get(\"target\", \"\").upper(),\n",
    "                description=r.get(\"description\", \"\"),\n",
    "                strength=float(r.get(\"strength\", 5)) / 10.0,\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for r in rels_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[R parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_claims(text: str, chunk_id: int = 0) -> list[Claim]:\n",
    "    \"\"\"Extract factual claims from a text chunk.\"\"\"\n",
    "    prompt = CLAIMS_EXTRACTION_PROMPT.format(text=text)\n",
    "    response = chat_ollama(prompt)\n",
    "    try:\n",
    "        claims_data = _parse_llm_json(response)\n",
    "        return [\n",
    "            Claim(\n",
    "                subject=c.get(\"subject\", \"\").upper(),\n",
    "                claim_type=c.get(\"claim_type\", \"FACT\"),\n",
    "                description=c.get(\"description\", \"\"),\n",
    "                date=c.get(\"date\", \"\"),\n",
    "                source_chunk=chunk_id\n",
    "            )\n",
    "            for c in claims_data\n",
    "        ]\n",
    "    except json.JSONDecodeError as ex:\n",
    "        print(f\"[C parse error: {ex}]\", end=\" \")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"Extraction functions defined: extract_entities, extract_relationships, extract_claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract entities, relationships, and claims from all documents\nsource_results: dict[str, dict] = {}\nglobal_chunk_offset = 0\nskipped_chunks: list[dict] = []  # track failures for diagnostics\n\nfor doc_idx, doc in enumerate(all_documents):\n    doc_chunks = source_chunks[doc.source_id]\n    doc_entities: list[Entity] = []\n    doc_relationships: list[Relationship] = []\n    doc_claims: list[Claim] = []\n\n    print(f\"\\n{'='*60}\")\n    print(f\"[{doc_idx+1}/{len(all_documents)}] {doc.source_id}: {doc.title[:50]}\")\n    print(f\"  {len(doc_chunks)} chunks to process\")\n\n    for i, chunk in enumerate(doc_chunks):\n        chunk_id = global_chunk_offset + i\n        print(f\"  Chunk {i+1}/{len(doc_chunks)}: \", end=\"\")\n\n        try:\n            # Entity extraction\n            entities = extract_entities(chunk, chunk_id=chunk_id)\n            doc_entities.extend(entities)\n            print(f\"{len(entities)}E \", end=\"\")\n            time.sleep(0.5)\n\n            # Relationship extraction\n            chunk_entities = [e for e in doc_entities if e.source_chunk == chunk_id]\n            if len(chunk_entities) >= 2:\n                relationships = extract_relationships(chunk, chunk_entities, chunk_id=chunk_id)\n                doc_relationships.extend(relationships)\n                print(f\"{len(relationships)}R \", end=\"\")\n                time.sleep(0.5)\n            else:\n                print(f\"0R \", end=\"\")\n\n            # Claims extraction\n            claims = extract_claims(chunk, chunk_id=chunk_id)\n            doc_claims.extend(claims)\n            print(f\"{len(claims)}C\")\n            time.sleep(0.5)\n\n        except Exception as e:\n            err_type = type(e).__name__\n            print(f\"SKIPPED ({err_type}: {e})\")\n            skipped_chunks.append({\n                \"chunk_id\": chunk_id,\n                \"source_id\": doc.source_id,\n                \"chunk_index\": i,\n                \"error\": f\"{err_type}: {e}\",\n                \"chunk_len\": len(chunk),\n            })\n\n    global_chunk_offset += len(doc_chunks)\n\n    source_results[doc.source_id] = {\n        \"entities\": doc_entities,\n        \"relationships\": doc_relationships,\n        \"claims\": doc_claims,\n        \"chunks\": doc_chunks,\n    }\n\n    print(f\"  => {len(doc_entities)}E, {len(doc_relationships)}R, {len(doc_claims)}C\")\n\n# Grand totals\ntotal_e = sum(len(r[\"entities\"]) for r in source_results.values())\ntotal_r = sum(len(r[\"relationships\"]) for r in source_results.values())\ntotal_c = sum(len(r[\"claims\"]) for r in source_results.values())\ntotal_processed = sum(len(r[\"chunks\"]) for r in source_results.values())\nprint(f\"\\n{'='*60}\")\nprint(f\"EXTRACTION COMPLETE\")\nprint(f\"Total: {total_e} entities, {total_r} relationships, {total_c} claims\")\nprint(f\"Chunks: {total_processed - len(skipped_chunks)} succeeded, {len(skipped_chunks)} skipped\")\n\nif skipped_chunks:\n    print(f\"\\n--- Skipped Chunks ---\")\n    for sc in skipped_chunks:\n        print(f\"  [{sc['source_id']}] chunk {sc['chunk_index']} ({sc['chunk_len']} chars): {sc['error']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PER-SOURCE EXTRACTION RESULTS ===\n",
      "\n",
      "Source                      Entities  Relations     Claims\n",
      "------------------------------------------------------------\n",
      "arxiv:2404.16130                   8          4          0\n",
      "arxiv:2404.18021                  10          1          0\n",
      "arxiv:2312.14090                  14          7          0\n",
      "arxiv:2304.04869                  12          4          0\n",
      "web:quanta-memory                  4          0          0\n",
      "web:imf-ai-economy                42         19          1\n",
      "web:planetary-voyager             52         18          1\n",
      "------------------------------------------------------------\n",
      "TOTAL                            142         53          2\n"
     ]
    }
   ],
   "source": [
    "# Display per-source extraction summary\n",
    "print(\"=== PER-SOURCE EXTRACTION RESULTS ===\\n\")\n",
    "print(f\"{'Source':<25} {'Entities':>10} {'Relations':>10} {'Claims':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for source_id, result in source_results.items():\n",
    "    print(f\"{source_id:<25} {len(result['entities']):>10} {len(result['relationships']):>10} {len(result['claims']):>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'TOTAL':<25} {total_e:>10} {total_r:>10} {total_c:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PER-SOURCE DEDUPLICATION ===\n",
      "\n",
      "  arxiv:2404.16130: 8 raw -> 7 unique\n",
      "  arxiv:2404.18021: 10 raw -> 7 unique\n",
      "  arxiv:2312.14090: 14 raw -> 12 unique\n",
      "  arxiv:2304.04869: 12 raw -> 11 unique\n",
      "  web:quanta-memory: 4 raw -> 3 unique\n",
      "  web:imf-ai-economy: 42 raw -> 29 unique\n",
      "  web:planetary-voyager: 52 raw -> 35 unique\n",
      "\n",
      "=== GLOBAL MERGE ===\n",
      "  Global unique entities: 101\n",
      "  Global relationships: 53\n",
      "  Global claims: 2\n",
      "\n",
      "=== CHUNK PROVENANCE ===\n",
      "  Entities with chunk tracking: 101\n",
      "  Entities in 2+ chunks: 16\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate entities by name (merge descriptions)\n",
    "def deduplicate_entities(entities: list[Entity]) -> list[Entity]:\n",
    "    \"\"\"Merge duplicate entities, combining their descriptions.\"\"\"\n",
    "    entity_map: dict[str, Entity] = {}\n",
    "    \n",
    "    for entity in entities:\n",
    "        key = entity.name\n",
    "        if key in entity_map:\n",
    "            existing = entity_map[key]\n",
    "            if entity.description and entity.description not in existing.description:\n",
    "                existing.description = f\"{existing.description} | {entity.description}\"\n",
    "        else:\n",
    "            entity_map[key] = Entity(\n",
    "                name=entity.name,\n",
    "                type=entity.type,\n",
    "                description=entity.description,\n",
    "                source_chunk=entity.source_chunk\n",
    "            )\n",
    "    \n",
    "    return list(entity_map.values())\n",
    "\n",
    "\n",
    "def merge_entities_across_sources(\n",
    "    source_results: dict[str, dict]\n",
    ") -> tuple[list[Entity], list[Relationship], list[Claim]]:\n",
    "    \"\"\"Deduplicate within each source, then merge across all sources.\n",
    "    \n",
    "    Returns global_entities, global_relationships, global_claims.\n",
    "    Also builds entity_source_map: entity_name -> list of source_ids.\n",
    "    \"\"\"\n",
    "    # Phase 1: Deduplicate within each source\n",
    "    for source_id, result in source_results.items():\n",
    "        result[\"unique_entities\"] = deduplicate_entities(result[\"entities\"])\n",
    "    \n",
    "    # Phase 2: Merge across all sources\n",
    "    global_entity_map: dict[str, Entity] = {}\n",
    "    entity_source_map: dict[str, list[str]] = {}\n",
    "    \n",
    "    for source_id, result in source_results.items():\n",
    "        for entity in result[\"unique_entities\"]:\n",
    "            key = entity.name\n",
    "            if key in global_entity_map:\n",
    "                existing = global_entity_map[key]\n",
    "                if entity.description and entity.description not in existing.description:\n",
    "                    existing.description = f\"{existing.description} | {entity.description}\"\n",
    "                entity_source_map[key].append(source_id)\n",
    "            else:\n",
    "                global_entity_map[key] = Entity(\n",
    "                    name=entity.name,\n",
    "                    type=entity.type,\n",
    "                    description=entity.description,\n",
    "                    source_chunk=entity.source_chunk\n",
    "                )\n",
    "                entity_source_map[key] = [source_id]\n",
    "    \n",
    "    global_entities = list(global_entity_map.values())\n",
    "    \n",
    "    # Merge all relationships (keep all, some may reference same entities)\n",
    "    global_relationships = []\n",
    "    for result in source_results.values():\n",
    "        global_relationships.extend(result[\"relationships\"])\n",
    "    \n",
    "    # Merge all claims\n",
    "    global_claims = []\n",
    "    for result in source_results.values():\n",
    "        global_claims.extend(result[\"claims\"])\n",
    "    \n",
    "    return global_entities, global_relationships, global_claims, entity_source_map\n",
    "\n",
    "\n",
    "# Run cross-document merge\n",
    "global_entities, global_relationships, global_claims, entity_source_map = merge_entities_across_sources(source_results)\n",
    "\n",
    "# Build entity -> chunk provenance mapping from raw extraction data.\n",
    "# Each entity was extracted from a specific chunk; this records ALL chunks\n",
    "# that produced each entity (before deduplication collapsed them).\n",
    "entity_chunk_map: dict[str, list[dict]] = {}\n",
    "for doc in all_documents:\n",
    "    result = source_results[doc.source_id]\n",
    "    for entity in result[\"entities\"]:\n",
    "        key = entity.name\n",
    "        if key not in entity_chunk_map:\n",
    "            entity_chunk_map[key] = []\n",
    "        entry = {\"chunk_index\": entity.source_chunk, \"source_id\": doc.source_id}\n",
    "        if entry not in entity_chunk_map[key]:\n",
    "            entity_chunk_map[key].append(entry)\n",
    "\n",
    "# Per-source dedup stats\n",
    "print(\"=== PER-SOURCE DEDUPLICATION ===\\n\")\n",
    "for source_id, result in source_results.items():\n",
    "    raw = len(result[\"entities\"])\n",
    "    unique = len(result[\"unique_entities\"])\n",
    "    print(f\"  {source_id}: {raw} raw -> {unique} unique\")\n",
    "\n",
    "print(f\"\\n=== GLOBAL MERGE ===\")\n",
    "print(f\"  Global unique entities: {len(global_entities)}\")\n",
    "print(f\"  Global relationships: {len(global_relationships)}\")\n",
    "print(f\"  Global claims: {len(global_claims)}\")\n",
    "\n",
    "# Chunk provenance stats\n",
    "entities_with_chunks = sum(1 for v in entity_chunk_map.values() if v)\n",
    "multi_chunk = sum(1 for v in entity_chunk_map.values() if len(v) > 1)\n",
    "print(f\"\\n=== CHUNK PROVENANCE ===\")\n",
    "print(f\"  Entities with chunk tracking: {entities_with_chunks}\")\n",
    "print(f\"  Entities in 2+ chunks: {multi_chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Extraction Results\n",
    "\n",
    "Consolidate all extracted knowledge elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPHRAG MULTI-SOURCE EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Sources: 7\n",
      "Total content: 19,823 characters\n",
      "Total chunks: 52\n",
      "\n",
      "Global entities: 101\n",
      "Global relationships: 53\n",
      "Global claims: 2\n",
      "\n",
      "--- Entity Types ---\n",
      "  LOCATION: 29\n",
      "  CONCEPT: 20\n",
      "  PRODUCT: 15\n",
      "  ORGANIZATION: 11\n",
      "  EVENT: 11\n",
      "  MONEY: 7\n",
      "  PERSON: 6\n",
      "  UNKNOWN: 1\n",
      "  NUMBER: 1\n",
      "\n",
      "--- Claim Types ---\n",
      "  FACT: 1\n",
      "  STATEMENT: 1\n",
      "\n",
      "--- Cross-Source Entities (3 entities in 2+ sources) ---\n",
      "  LLM: ['arxiv:2404.16130', 'arxiv:2404.18021']\n",
      "  STARTUPXYZ: ['arxiv:2404.18021', 'arxiv:2304.04869']\n",
      "  AI: ['arxiv:2312.14090', 'web:imf-ai-economy']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GRAPHRAG MULTI-SOURCE EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSources: {len(all_documents)}\")\n",
    "print(f\"Total content: {total_chars:,} characters\")\n",
    "print(f\"Total chunks: {total_chunks}\")\n",
    "print(f\"\\nGlobal entities: {len(global_entities)}\")\n",
    "print(f\"Global relationships: {len(global_relationships)}\")\n",
    "print(f\"Global claims: {len(global_claims)}\")\n",
    "\n",
    "# Entity type breakdown\n",
    "print(\"\\n--- Entity Types ---\")\n",
    "type_counts: dict[str, int] = {}\n",
    "for e in global_entities:\n",
    "    type_counts[e.type] = type_counts.get(e.type, 0) + 1\n",
    "for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")\n",
    "\n",
    "# Claim type breakdown\n",
    "print(\"\\n--- Claim Types ---\")\n",
    "claim_type_counts: dict[str, int] = {}\n",
    "for c in global_claims:\n",
    "    claim_type_counts[c.claim_type] = claim_type_counts.get(c.claim_type, 0) + 1\n",
    "for t, count in sorted(claim_type_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {t}: {count}\")\n",
    "\n",
    "# Cross-source entities\n",
    "multi_source = {k: v for k, v in entity_source_map.items() if len(v) > 1}\n",
    "print(f\"\\n--- Cross-Source Entities ({len(multi_source)} entities in 2+ sources) ---\")\n",
    "for name, sources in sorted(multi_source.items(), key=lambda x: -len(x[1])):\n",
    "    print(f\"  {name}: {sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook we will:\n",
    "1. **Build the knowledge graph** - Store entities and relationships from all 7 sources in a graph structure\n",
    "2. **Apply community detection** - Use Louvain algorithm to find cross-domain topic clusters\n",
    "3. **Generate community summaries** - Create hierarchical summaries for each cluster\n",
    "4. **Interactive visualization** - Explore the graph with ipycytoscape\n",
    "5. **Store in SQLite** - Persist the graph for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to extraction_results.json\n",
      "  7 sources\n",
      "  101 merged entities\n",
      "  53 relationships\n",
      "  2 claims\n",
      "  101 entities with chunk provenance\n"
     ]
    }
   ],
   "source": [
    "# Export extracted data in multi-document format\n",
    "extraction_results = {\n",
    "    \"sources\": [],\n",
    "    \"merged\": {\n",
    "        \"entities\": [{\"name\": e.name, \"type\": e.type, \"description\": e.description} for e in global_entities],\n",
    "        \"relationships\": [{\"source\": r.source, \"target\": r.target, \"description\": r.description, \"strength\": r.strength} for r in global_relationships],\n",
    "        \"claims\": [{\"subject\": c.subject, \"claim_type\": c.claim_type, \"description\": c.description, \"date\": c.date} for c in global_claims],\n",
    "        \"entity_source_map\": {k: v for k, v in entity_source_map.items()},\n",
    "        \"entity_chunk_map\": entity_chunk_map,\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"total_sources\": len(all_documents),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add per-source data\n",
    "for doc in all_documents:\n",
    "    result = source_results[doc.source_id]\n",
    "    extraction_results[\"sources\"].append({\n",
    "        \"source_id\": doc.source_id,\n",
    "        \"source_type\": doc.source_type,\n",
    "        \"title\": doc.title,\n",
    "        \"url\": doc.url,\n",
    "        \"content_type\": doc.content_type,\n",
    "        \"content_length\": len(doc.content),\n",
    "        \"fetched_at\": doc.fetched_at,\n",
    "        \"chunks\": result[\"chunks\"],\n",
    "        \"entities\": [{\"name\": e.name, \"type\": e.type, \"description\": e.description} for e in result.get(\"unique_entities\", result[\"entities\"])],\n",
    "        \"relationships\": [{\"source\": r.source, \"target\": r.target, \"description\": r.description, \"strength\": r.strength} for r in result[\"relationships\"]],\n",
    "        \"claims\": [{\"subject\": c.subject, \"claim_type\": c.claim_type, \"description\": c.description, \"date\": c.date} for c in result[\"claims\"]],\n",
    "    })\n",
    "\n",
    "with open(\"extraction_results.json\", \"w\") as f:\n",
    "    json.dump(extraction_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to extraction_results.json\")\n",
    "print(f\"  {len(extraction_results['sources'])} sources\")\n",
    "print(f\"  {len(extraction_results['merged']['entities'])} merged entities\")\n",
    "print(f\"  {len(extraction_results['merged']['relationships'])} relationships\")\n",
    "print(f\"  {len(extraction_results['merged']['claims'])} claims\")\n",
    "print(f\"  {len(extraction_results['merged']['entity_chunk_map'])} entities with chunk provenance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CROSS-SOURCE ENTITY OVERLAP: 3 entities in 2+ sources\n",
      "============================================================\n",
      "\n",
      "  [ORGANIZATION] LLM (in 2 sources)\n",
      "    - arxiv:2404.16130: From Local to Global: A Graph RAG Approach to Quer\n",
      "    - arxiv:2404.18021: CRISPR-GPT for Agentic Automation of Gene-editing \n",
      "\n",
      "  [ORGANIZATION] STARTUPXYZ (in 2 sources)\n",
      "    - arxiv:2404.18021: CRISPR-GPT for Agentic Automation of Gene-editing \n",
      "    - arxiv:2304.04869: The James Webb Space Telescope Mission\n",
      "\n",
      "  [CONCEPT] AI (in 2 sources)\n",
      "    - arxiv:2312.14090: Designing Artificial Intelligence Equipped Social \n",
      "    - web:imf-ai-economy: AI Will Transform the Global Economy\n",
      "\n",
      "============================================================\n",
      "SOURCE PAIR OVERLAP (shared entity count)\n",
      "============================================================\n",
      "  arxiv:2404.16130 <-> arxiv:2404.18021: 1 (LLM)\n",
      "  arxiv:2404.18021 <-> arxiv:2304.04869: 1 (STARTUPXYZ)\n",
      "  arxiv:2312.14090 <-> web:imf-ai-economy: 1 (AI)\n"
     ]
    }
   ],
   "source": [
    "# Cross-source entity overlap analysis\n",
    "multi_source = {k: v for k, v in entity_source_map.items() if len(v) > 1}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"CROSS-SOURCE ENTITY OVERLAP: {len(multi_source)} entities in 2+ sources\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if multi_source:\n",
    "    for name, sources in sorted(multi_source.items(), key=lambda x: -len(x[1])):\n",
    "        entity = next((e for e in global_entities if e.name == name), None)\n",
    "        etype = entity.type if entity else \"?\"\n",
    "        print(f\"\\n  [{etype}] {name} (in {len(sources)} sources)\")\n",
    "        for sid in sources:\n",
    "            doc = next((d for d in all_documents if d.source_id == sid), None)\n",
    "            if doc:\n",
    "                print(f\"    - {sid}: {doc.title[:50]}\")\n",
    "else:\n",
    "    print(\"\\n  No cross-source entities found.\")\n",
    "    print(\"  This is expected with diverse topics — entities are domain-specific.\")\n",
    "\n",
    "# Source overlap matrix\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SOURCE PAIR OVERLAP (shared entity count)\")\n",
    "print(\"=\"*60)\n",
    "source_ids = [doc.source_id for doc in all_documents]\n",
    "for i, s1 in enumerate(source_ids):\n",
    "    for s2 in source_ids[i+1:]:\n",
    "        shared = [name for name, srcs in entity_source_map.items()\n",
    "                  if s1 in srcs and s2 in srcs]\n",
    "        if shared:\n",
    "            print(f\"  {s1} <-> {s2}: {len(shared)} ({', '.join(shared[:5])}{'...' if len(shared) > 5 else ''})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKIA GraphRAG",
   "language": "python",
   "name": "dkia-graphrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}